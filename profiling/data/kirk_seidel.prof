--------------------------------------------------------------------------------
Profile data file 'callgrind.out.266358' (creator: callgrind-3.18.1)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 81273741
Trigger: Program termination
Profiled target:  ./target/release/kirk_seidel (PID 266358, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
Ir                   
--------------------------------------------------------------------------------
362,683,902 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
Ir                   file:function
--------------------------------------------------------------------------------
33,160,480 ( 9.14%)  /rust/deps/hashbrown-0.14.3/src/raw/mod.rs:hashbrown::set::HashSet<T,S,A>::insert
32,650,895 ( 9.00%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc.so.6]
24,961,618 ( 6.88%)  /rust/deps/hashbrown-0.14.3/src/raw/mod.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
14,162,545 ( 3.90%)  src/bin/kirk_seidel.rs:kirk_seidel::bridge'2 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
11,640,303 ( 3.21%)  /rust/deps/hashbrown-0.14.3/src/raw/bitmask.rs:hashbrown::set::HashSet<T,S,A>::insert
10,719,643 ( 2.96%)  /rust/deps/hashbrown-0.14.3/src/set.rs:hashbrown::set::HashSet<T,S,A>::insert [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 7,928,270 ( 2.19%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/sse2.rs:hashbrown::set::HashSet<T,S,A>::insert
 7,329,653 ( 2.02%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/sort.rs:core::slice::sort::insertion_sort_shift_left [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 7,309,447 ( 2.02%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/f32.rs:kirk_seidel::bridge'2
 7,249,568 ( 2.00%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs:core::slice::sort::insertion_sort_shift_left
 6,597,074 ( 1.82%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:kirk_seidel::bridge'2
 6,021,696 ( 1.66%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:tikv_jemallocator::layout_to_flags [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 5,796,201 ( 1.60%)  /rust/deps/hashbrown-0.14.3/src/raw/bitmask.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 5,551,883 ( 1.53%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 5,506,175 ( 1.52%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 5,495,920 ( 1.52%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/hash/mod.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 5,346,746 ( 1.47%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/range.rs:core::slice::sort::insertion_sort_shift_left
 5,169,492 ( 1.43%)  src/bin/kirk_seidel.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 4,877,372 ( 1.34%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 4,872,565 ( 1.34%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs:hashbrown::set::HashSet<T,S,A>::insert
 4,761,497 ( 1.31%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::bridge'2
 4,677,905 ( 1.29%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs:core::slice::sort::insertion_sort_shift_left
 4,614,627 ( 1.27%)  src/bin/kirk_seidel.rs:kirk_seidel::bridge [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 4,583,726 ( 1.26%)  /rust/deps/hashbrown-0.14.3/src/raw/bitmask.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 4,202,532 ( 1.16%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/sort.rs:_ZN4core5slice4sort7recurse17h49f264b72b8509b8E.llvm.5035684801631549913 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 4,135,020 ( 1.14%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs:kirk_seidel::bridge'2
 3,937,920 ( 1.09%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
 3,906,191 ( 1.08%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:kirk_seidel::bridge'2
 3,882,903 ( 1.07%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 3,034,036 ( 0.84%)  src/bin/kirk_seidel.rs:hashbrown::set::HashSet<T,S,A>::insert
 3,020,298 ( 0.83%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 2,975,952 ( 0.82%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_sdallocx [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 2,928,406 ( 0.81%)  /rust/deps/hashbrown-0.14.3/src/raw/mod.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 2,912,646 ( 0.80%)  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc.so.6]
 2,800,114 ( 0.77%)  src/bin/kirk_seidel.rs:kirk_seidel::upper_hull [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 2,747,960 ( 0.76%)  src/bin/kirk_seidel.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 2,642,654 ( 0.73%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs:hashbrown::set::HashSet<T,S,A>::insert
 2,479,507 ( 0.68%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/cache_bin.h:_rjem_malloc
 2,473,340 ( 0.68%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/cache_bin.h:_rjem_sdallocx
 2,438,623 ( 0.67%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::bridge'2
 2,384,216 ( 0.66%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/f32.rs:kirk_seidel::bridge
 2,143,170 ( 0.59%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:kirk_seidel::bridge
 2,066,640 ( 0.57%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/sort.rs:_ZN4core5slice4sort7recurse17h72cfc30f3dbacb87E.llvm.5035684801631549913 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
 1,985,962 ( 0.55%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h:_rjem_malloc
 1,952,775 ( 0.54%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter.rs:kirk_seidel::bridge'2
 1,952,353 ( 0.54%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 1,949,026 ( 0.54%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:hashbrown::set::HashSet<T,S,A>::insert
 1,855,386 ( 0.51%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs:core::slice::sort::insertion_sort_shift_left
 1,571,531 ( 0.43%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/sse2.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 1,556,250 ( 0.43%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/avx2.rs:rand_chacha::guts::refill_wide::impl_avx2
 1,550,355 ( 0.43%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::bridge
 1,477,190 ( 0.41%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs:kirk_seidel::bridge'2
 1,475,024 ( 0.41%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 1,466,838 ( 0.40%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/std/src/collections/hash/set.rs:kirk_seidel::bridge'2
 1,446,912 ( 0.40%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
 1,446,912 ( 0.40%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/sort.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
 1,376,081 ( 0.38%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/nonzero.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 1,373,980 ( 0.38%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/bit.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
 1,351,224 ( 0.37%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs:kirk_seidel::bridge
 1,321,327 ( 0.36%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs:hashbrown::set::HashSet<T,S,A>::insert
 1,292,760 ( 0.36%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 1,292,373 ( 0.36%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/arith.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
 1,271,469 ( 0.35%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:kirk_seidel::bridge
 1,203,125 ( 0.33%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_core-0.6.4/src/block.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
 1,200,002 ( 0.33%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::upper_hull
 1,125,242 ( 0.31%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::connect
   999,986 ( 0.28%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:kirk_seidel::upper_hull
   991,124 ( 0.27%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_malloc
   990,024 ( 0.27%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_sdallocx
   980,421 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs:kirk_seidel::bridge'2
   975,746 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:kirk_seidel::bridge'2
   974,882 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
   974,513 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/convert/num.rs:hashbrown::set::HashSet<T,S,A>::insert
   974,513 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/bit.rs:hashbrown::set::HashSet<T,S,A>::insert
   974,513 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:hashbrown::set::HashSet<T,S,A>::insert
   974,484 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/arith.rs:kirk_seidel::bridge'2
   974,130 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/nonzero.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
   964,608 ( 0.27%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/traits/iterator.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   923,456 ( 0.25%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   800,096 ( 0.22%)  src/bin/kirk_seidel.rs:kirk_seidel::connect [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   794,871 ( 0.22%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::bridge
   744,308 ( 0.21%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_malloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   742,249 ( 0.20%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/adapters/map.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   726,138 ( 0.20%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   704,689 ( 0.19%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::connect'2
   700,000 ( 0.19%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.8.5/src/distributions/float.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   675,002 ( 0.19%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::connect
   636,003 ( 0.18%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter.rs:kirk_seidel::bridge
   583,199 ( 0.16%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/set_len_on_drop.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   552,485 ( 0.15%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs:hashbrown::set::HashSet<T,S,A>::insert
   500,000 ( 0.14%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.8.5/src/distributions/utils.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   499,064 ( 0.14%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:kirk_seidel::connect
   494,904 ( 0.14%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/thread_event.h:_rjem_malloc
   492,666 ( 0.14%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/into_iter.rs:kirk_seidel::bridge'2
   487,668 ( 0.13%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   482,371 ( 0.13%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/std/src/collections/hash/set.rs:kirk_seidel::bridge
   482,304 ( 0.13%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/alloc/layout.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   482,304 ( 0.13%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   477,769 ( 0.13%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs:kirk_seidel::bridge
   472,665 ( 0.13%)  src/bin/kirk_seidel.rs:kirk_seidel::connect'2 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   400,020 ( 0.11%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::upper_hull
   399,996 ( 0.11%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:kirk_seidel::upper_hull
   397,873 ( 0.11%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::connect'2
   320,640 ( 0.09%)  src/bin/kirk_seidel.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   318,245 ( 0.09%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs:kirk_seidel::bridge
   317,966 ( 0.09%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:kirk_seidel::bridge
   317,889 ( 0.09%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/arith.rs:kirk_seidel::bridge
   312,386 ( 0.09%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_ralloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   302,640 ( 0.08%)  ./nptl/./nptl/pthread_mutex_trylock.c:pthread_mutex_trylock@@GLIBC_2.34 [/usr/lib/x86_64-linux-gnu/libc.so.6]
   247,506 ( 0.07%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/thread_event.h:_rjem_sdallocx
   241,280 ( 0.07%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:do_rallocx [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   241,152 ( 0.07%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/index.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   218,298 ( 0.06%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/thread_event.c:_rjem_je_te_event_trigger [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   205,150 ( 0.06%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_recycle [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   199,996 ( 0.06%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs:kirk_seidel::upper_hull
   199,624 ( 0.06%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:kirk_seidel::connect
   195,816 ( 0.05%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/sse2.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
   175,008 ( 0.05%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:kirk_seidel::connect
   173,965 ( 0.05%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_ralloc_no_move [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   170,822 ( 0.05%)  ./nptl/./nptl/pthread_mutex_unlock.c:pthread_mutex_unlock@@GLIBC_2.2.5 [/usr/lib/x86_64-linux-gnu/libc.so.6]
   159,263 ( 0.04%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/into_iter.rs:kirk_seidel::bridge
   150,238 ( 0.04%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs:kirk_seidel::main
   150,033 ( 0.04%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs:kirk_seidel::main
   149,196 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_fit [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   140,815 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_update_edata_state
   135,783 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   133,977 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:emap_try_acquire_edata_neighbor_impl [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   132,001 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_cache_bin_fill_small [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   128,180 ( 0.04%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_realloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   113,177 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_small [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   111,344 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/thread_event.h:do_rallocx
   110,651 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/nonzero.rs:hashbrown::set::HashSet<T,S,A>::insert
   105,415 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_insert [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   104,755 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_stashed [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
   103,104 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs:kirk_seidel::connect'2
   100,006 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/range.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
   100,001 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
    99,806 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs:kirk_seidel::connect
    99,579 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_arena_ralloc_no_move
    97,908 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/const_ptr.rs:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter
    97,000 ( 0.03%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs:alloc::raw_vec::RawVec<T,A>::reserve_for_push [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    95,072 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_arena_ralloc
    95,051 ( 0.03%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_try_coalesce_impl [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    90,668 ( 0.02%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs:kirk_seidel::connect'2
    90,480 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/tsd.h:do_rallocx
    82,229 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_remove [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    80,637 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/cache_bin.h:_rjem_je_arena_ralloc
    80,604 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_remap [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    79,530 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c:_rjem_je_hook_invoke_alloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    79,530 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c:_rjem_je_hook_invoke_dalloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    79,387 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/edata.h:_rjem_je_emap_update_edata_state
    76,500 ( 0.02%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs:_ZN5alloc7raw_vec11finish_grow17h79d956b34747eb25E.llvm.3125636675663933307 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    75,022 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_split_interior.constprop.0 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    75,005 ( 0.02%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs:kirk_seidel::main
    74,673 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:emap_try_acquire_edata_neighbor_impl
    73,675 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/fb.h:_rjem_je_eset_fit
    69,867 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_split_impl.constprop.0 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    69,168 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_record [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    67,860 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:do_rallocx
    63,144 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_sdallocx_default [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    61,075 ( 0.02%)  ???:_rjem_je_tcache_gc_dalloc_event_handler [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    59,476 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/edata.h:_rjem_je_eset_insert
    58,389 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_sdallocx_default
    58,318 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_alloc [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    56,559 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_split_prepare
    56,550 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h:do_rallocx
    56,306 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_sz_psz_quantize_floor
    55,694 ( 0.02%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_avail_remove
    53,688 ( 0.01%)  ./stdio-common/./stdio-common/vfscanf-internal.c:__vfscanf_internal [/usr/lib/x86_64-linux-gnu/libc.so.6]
    53,644 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/edata.h:_rjem_je_eset_remove
    53,125 ( 0.01%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/sse2.rs:rand_chacha::guts::refill_wide::impl_avx2
    52,609 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/extent.h:emap_try_acquire_edata_neighbor_impl
    51,150 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_merge_prepare
    50,608 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:_rjem_je_arena_ralloc_no_move
    50,571 ( 0.01%)  ./elf/./elf/dl-lookup.c:do_lookup_x [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    50,478 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_malloc_default [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    50,248 ( 0.01%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/result.rs:_ZN5alloc7raw_vec11finish_grow17h79d956b34747eb25E.llvm.3125636675663933307
    50,224 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:pac_alloc_impl [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    50,082 ( 0.01%)  src/bin/kirk_seidel.rs:kirk_seidel::main [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    50,000 ( 0.01%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_chacha-0.3.1/src/guts.rs:rand_chacha::guts::refill_wide::impl_avx2 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    50,000 ( 0.01%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs:rand_chacha::guts::refill_wide::impl_avx2
    49,905 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_split_prepare [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    48,934 ( 0.01%)  /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:_ZN9hashbrown3raw21RawTable$LT$T$C$A$GT$14reserve_rehash17he9de1d769ed5a6f8E.llvm.15701349819212896973
    48,896 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_gc_event_handler [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    48,564 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_alloc_impl [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    47,670 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:do_rallocx
    46,875 ( 0.01%)  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/range.rs:rand_chacha::guts::refill_wide::impl_avx2
    46,872 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_merge_impl.constprop.0 [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    46,683 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_extent_alloc_large [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    46,578 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h:tcache_bin_flush_edatas_lookup.constprop.0
    46,255 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    44,875 ( 0.01%)  ./elf/./elf/dl-tunables.c:__GI___tunables_init [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    44,860 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_eset_insert
    44,088 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:pac_alloc_real [/home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/kirk_seidel]
    43,264 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_heap_insert
    42,977 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/arena_inlines_b.h:_rjem_je_tcache_bin_flush_small
    42,042 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/edata.h:emap_try_acquire_edata_neighbor_impl
    41,971 ( 0.01%)  target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h:_rjem_je_eset_remove

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c
--------------------------------------------------------------------------------
Ir              

-- line 27 ----------------------------------------
     .           	"auto",
     .           	"always"
     .           };
     .           
     .           /******************************************************************************/
     .           
     .           static inline bool
     .           metadata_thp_madvise(void) {
   320 ( 0.00%)  	return (metadata_thp_enabled() &&
     .           	    (init_system_thp_mode == thp_mode_default));
     .           }
     .           
     .           static void *
     .           base_map(tsdn_t *tsdn, ehooks_t *ehooks, unsigned ind, size_t size) {
     .           	void *addr;
     2 ( 0.00%)  	bool zero = true;
     2 ( 0.00%)  	bool commit = true;
     .           
     .           	/* Use huge page sizes and alignment regardless of opt_metadata_thp. */
     .           	assert(size == HUGEPAGE_CEILING(size));
     .           	size_t alignment = HUGEPAGE;
     6 ( 0.00%)  	if (ehooks_are_default(ehooks)) {
    14 ( 0.00%)  		addr = extent_alloc_mmap(NULL, size, alignment, &zero, &commit);
   238 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent_mmap.c:_rjem_je_extent_alloc_mmap (2x)
     4 ( 0.00%)  		if (have_madvise_huge && addr) {
     8 ( 0.00%)  			pages_set_thp_state(addr, size);
    10 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pages.c:_rjem_je_pages_set_thp_state (2x)
     .           		}
     .           	} else {
     .           		addr = ehooks_alloc(tsdn, ehooks, NULL, size, alignment, &zero,
     .           		    &commit);
     .           	}
     .           
     .           	return addr;
     .           }
-- line 59 ----------------------------------------
-- line 110 ----------------------------------------
     .           	}
     .           }
     .           
     .           static void
     .           base_edata_init(size_t *extent_sn_next, edata_t *edata, void *addr,
     .               size_t size) {
     .           	size_t sn;
     .           
     2 ( 0.00%)  	sn = *extent_sn_next;
     4 ( 0.00%)  	(*extent_sn_next)++;
     .           
     .           	edata_binit(edata, addr, size, sn);
     .           }
     .           
     .           static size_t
     .           base_get_num_blocks(base_t *base, bool with_new_block) {
     .           	base_block_t *b = base->blocks;
     .           	assert(b != NULL);
-- line 127 ----------------------------------------
-- line 174 ----------------------------------------
     .           static void *
     .           base_extent_bump_alloc_helper(edata_t *edata, size_t *gap_size, size_t size,
     .               size_t alignment) {
     .           	void *ret;
     .           
     .           	assert(alignment == ALIGNMENT_CEILING(alignment, QUANTUM));
     .           	assert(size == ALIGNMENT_CEILING(size, alignment));
     .           
   315 ( 0.00%)  	*gap_size = ALIGNMENT_CEILING((uintptr_t)edata_addr_get(edata),
   315 ( 0.00%)  	    alignment) - (uintptr_t)edata_addr_get(edata);
   157 ( 0.00%)  	ret = (void *)((uintptr_t)edata_addr_get(edata) + *gap_size);
     .           	assert(edata_bsize_get(edata) >= *gap_size + size);
   627 ( 0.00%)  	edata_binit(edata, (void *)((uintptr_t)edata_addr_get(edata) +
   314 ( 0.00%)  	    *gap_size + size), edata_bsize_get(edata) - *gap_size - size,
     .           	    edata_sn_get(edata));
     .           	return ret;
     .           }
     .           
     .           static void
     .           base_extent_bump_alloc_post(base_t *base, edata_t *edata, size_t gap_size,
 1,570 ( 0.00%)      void *addr, size_t size) {
   314 ( 0.00%)  	if (edata_bsize_get(edata) > 0) {
     .           		/*
     .           		 * Compute the index for the largest size class that does not
     .           		 * exceed extent's size.
     .           		 */
     .           		szind_t index_floor =
   157 ( 0.00%)  		    sz_size2index(edata_bsize_get(edata) + 1) - 1;
   314 ( 0.00%)  		edata_heap_insert(&base->avail[index_floor], edata);
 5,623 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_insert (157x)
     .           	}
     .           
     .           	if (config_stats) {
   785 ( 0.00%)  		base->allocated += size;
     .           		/*
     .           		 * Add one PAGE to base_resident for every page boundary that is
     .           		 * crossed by the new allocation. Adjust n_thp similarly when
     .           		 * metadata_thp is enabled.
     .           		 */
   785 ( 0.00%)  		base->resident += PAGE_CEILING((uintptr_t)addr + size) -
   471 ( 0.00%)  		    PAGE_CEILING((uintptr_t)addr - gap_size);
     .           		assert(base->allocated <= base->resident);
     .           		assert(base->resident <= base->mapped);
     .           		if (metadata_thp_madvise() && (opt_metadata_thp ==
     .           		    metadata_thp_always || base->auto_thp_switched)) {
     .           			base->n_thp += (HUGEPAGE_CEILING((uintptr_t)addr + size)
     .           			    - HUGEPAGE_CEILING((uintptr_t)addr - gap_size)) >>
     .           			    LG_HUGEPAGE;
     .           			assert(base->mapped >= base->n_thp << LG_HUGEPAGE);
     .           		}
     .           	}
   942 ( 0.00%)  }
     .           
     .           static void *
     .           base_extent_bump_alloc(base_t *base, edata_t *edata, size_t size,
     .               size_t alignment) {
     .           	void *ret;
     .           	size_t gap_size;
     .           
     .           	ret = base_extent_bump_alloc_helper(edata, &gap_size, size, alignment);
   936 ( 0.00%)  	base_extent_bump_alloc_post(base, edata, gap_size, ret, size);
15,112 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_extent_bump_alloc_post (156x)
     .           	return ret;
     .           }
     .           
     .           /*
     .            * Allocate a block of virtual memory that is large enough to start with a
     .            * base_block_t header, followed by an object of specified size and alignment.
     .            * On success a pointer to the initialized base_block_t header is returned.
     .            */
     .           static base_block_t *
    24 ( 0.00%)  base_block_alloc(tsdn_t *tsdn, base_t *base, ehooks_t *ehooks, unsigned ind,
     .               pszind_t *pind_last, size_t *extent_sn_next, size_t size,
     .               size_t alignment) {
    10 ( 0.00%)  	alignment = ALIGNMENT_CEILING(alignment, QUANTUM);
     8 ( 0.00%)  	size_t usize = ALIGNMENT_CEILING(size, alignment);
     .           	size_t header_size = sizeof(base_block_t);
     4 ( 0.00%)  	size_t gap_size = ALIGNMENT_CEILING(header_size, alignment) -
     .           	    header_size;
     .           	/*
     .           	 * Create increasingly larger blocks in order to limit the total number
     .           	 * of disjoint virtual memory ranges.  Choose the next size in the page
     .           	 * size class series (skipping size classes that are not a multiple of
     .           	 * HUGEPAGE), or a size large enough to satisfy the requested size and
     .           	 * alignment, whichever is larger.
     .           	 */
     6 ( 0.00%)  	size_t min_block_size = HUGEPAGE_CEILING(sz_psz2u(header_size + gap_size
     .           	    + usize));
     4 ( 0.00%)  	pszind_t pind_next = (*pind_last + 1 < sz_psz2ind(SC_LARGE_MAXCLASS)) ?
     4 ( 0.00%)  	    *pind_last + 1 : *pind_last;
     8 ( 0.00%)  	size_t next_block_size = HUGEPAGE_CEILING(sz_pind2sz(pind_next));
     4 ( 0.00%)  	size_t block_size = (min_block_size > next_block_size) ? min_block_size
     .           	    : next_block_size;
     .           	base_block_t *block = (base_block_t *)base_map(tsdn, ehooks, ind,
     .           	    block_size);
     4 ( 0.00%)  	if (block == NULL) {
     .           		return NULL;
     .           	}
     .           
     .           	if (metadata_thp_madvise()) {
     .           		void *addr = (void *)block;
     .           		assert(((uintptr_t)addr & HUGEPAGE_MASK) == 0 &&
     .           		    (block_size & HUGEPAGE_MASK) == 0);
     .           		if (opt_metadata_thp == metadata_thp_always) {
-- line 275 ----------------------------------------
-- line 281 ----------------------------------------
     .           			base_auto_thp_switch(tsdn, base);
     .           			if (base->auto_thp_switched) {
     .           				pages_huge(addr, block_size);
     .           			}
     .           			malloc_mutex_unlock(tsdn, &base->mtx);
     .           		}
     .           	}
     .           
     2 ( 0.00%)  	*pind_last = sz_psz2ind(block_size);
     2 ( 0.00%)  	block->size = block_size;
     2 ( 0.00%)  	block->next = NULL;
     .           	assert(block_size >= header_size);
     4 ( 0.00%)  	base_edata_init(extent_sn_next, &block->edata,
     4 ( 0.00%)  	    (void *)((uintptr_t)block + header_size), block_size - header_size);
     .           	return block;
    24 ( 0.00%)  }
     .           
     .           /*
     .            * Allocate an extent that is at least as large as specified size, with
     .            * specified alignment.
     .            */
     .           static edata_t *
     .           base_extent_alloc(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment) {
     .           	malloc_mutex_assert_owner(tsdn, &base->mtx);
     .           
     .           	ehooks_t *ehooks = base_ehooks_get_for_metadata(base);
     .           	/*
     .           	 * Drop mutex during base_block_alloc(), because an extent hook will be
     .           	 * called.
     .           	 */
     .           	malloc_mutex_unlock(tsdn, &base->mtx);
     9 ( 0.00%)  	base_block_t *block = base_block_alloc(tsdn, base, ehooks,
   204 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_block_alloc.isra.0 (1x)
     .           	    base_ind_get(base), &base->pind_last, &base->extent_sn_next, size,
     .           	    alignment);
     .           	malloc_mutex_lock(tsdn, &base->mtx);
     2 ( 0.00%)  	if (block == NULL) {
     .           		return NULL;
     .           	}
     2 ( 0.00%)  	block->next = base->blocks;
     1 ( 0.00%)  	base->blocks = block;
     .           	if (config_stats) {
     3 ( 0.00%)  		base->allocated += sizeof(base_block_t);
     .           		base->resident += PAGE_CEILING(sizeof(base_block_t));
     2 ( 0.00%)  		base->mapped += block->size;
     .           		if (metadata_thp_madvise() &&
     .           		    !(opt_metadata_thp == metadata_thp_auto
     .           		      && !base->auto_thp_switched)) {
     .           			assert(base->n_thp > 0);
     .           			base->n_thp += HUGEPAGE_CEILING(sizeof(base_block_t)) >>
     .           			    LG_HUGEPAGE;
     .           		}
     .           		assert(base->allocated <= base->resident);
     .           		assert(base->resident <= base->mapped);
     .           		assert(base->n_thp << LG_HUGEPAGE <= base->mapped);
     .           	}
     1 ( 0.00%)  	return &block->edata;
     .           }
     .           
     .           base_t *
     5 ( 0.00%)  b0get(void) {
     .           	return b0;
    10 ( 0.00%)  }
     .           
     .           base_t *
     .           base_new(tsdn_t *tsdn, unsigned ind, const extent_hooks_t *extent_hooks,
    16 ( 0.00%)      bool metadata_use_hooks) {
     1 ( 0.00%)  	pszind_t pind_last = 0;
     1 ( 0.00%)  	size_t extent_sn_next = 0;
     .           
     .           	/*
     .           	 * The base will contain the ehooks eventually, but it itself is
     .           	 * allocated using them.  So we use some stack ehooks to bootstrap its
     .           	 * memory, and then initialize the ehooks within the base_t.
     .           	 */
     .           	ehooks_t fake_ehooks;
     9 ( 0.00%)  	ehooks_init(&fake_ehooks, metadata_use_hooks ?
     4 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
     .           	    (extent_hooks_t *)extent_hooks :
     .           	    (extent_hooks_t *)&ehooks_default_extent_hooks, ind);
     .           
    10 ( 0.00%)  	base_block_t *block = base_block_alloc(tsdn, NULL, &fake_ehooks, ind,
   290 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_block_alloc.isra.0 (1x)
     .           	    &pind_last, &extent_sn_next, sizeof(base_t), QUANTUM);
     4 ( 0.00%)  	if (block == NULL) {
     .           		return NULL;
     .           	}
     .           
     .           	size_t gap_size;
     .           	size_t base_alignment = CACHELINE;
     .           	size_t base_size = ALIGNMENT_CEILING(sizeof(base_t), base_alignment);
     2 ( 0.00%)  	base_t *base = (base_t *)base_extent_bump_alloc_helper(&block->edata,
     .           	    &gap_size, base_size, base_alignment);
     4 ( 0.00%)  	ehooks_init(&base->ehooks, (extent_hooks_t *)extent_hooks, ind);
     4 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
     4 ( 0.00%)  	ehooks_init(&base->ehooks_base, metadata_use_hooks ?
     4 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
     .           	    (extent_hooks_t *)extent_hooks :
     .           	    (extent_hooks_t *)&ehooks_default_extent_hooks, ind);
     7 ( 0.00%)  	if (malloc_mutex_init(&base->mtx, "base", WITNESS_RANK_BASE,
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           	    malloc_mutex_rank_exclusive)) {
     .           		base_unmap(tsdn, &fake_ehooks, ind, block, block->size);
     .           		return NULL;
     .           	}
     2 ( 0.00%)  	base->pind_last = pind_last;
     2 ( 0.00%)  	base->extent_sn_next = extent_sn_next;
     2 ( 0.00%)  	base->blocks = block;
     1 ( 0.00%)  	base->auto_thp_switched = false;
   292 ( 0.00%)  	for (szind_t i = 0; i < SC_NSIZES; i++) {
   261 ( 0.00%)  		edata_heap_new(&base->avail[i]);
   928 ( 0.00%)  => ???:_rjem_je_edata_heap_new (232x)
     .           	}
     .           	if (config_stats) {
     2 ( 0.00%)  		base->allocated = sizeof(base_block_t);
     .           		base->resident = PAGE_CEILING(sizeof(base_block_t));
     2 ( 0.00%)  		base->mapped = block->size;
     1 ( 0.00%)  		base->n_thp = (opt_metadata_thp == metadata_thp_always) &&
     .           		    metadata_thp_madvise() ? HUGEPAGE_CEILING(sizeof(base_block_t))
     3 ( 0.00%)  		    >> LG_HUGEPAGE : 0;
     .           		assert(base->allocated <= base->resident);
     .           		assert(base->resident <= base->mapped);
     .           		assert(base->n_thp << LG_HUGEPAGE <= base->mapped);
     .           	}
     6 ( 0.00%)  	base_extent_bump_alloc_post(base, &block->edata, gap_size, base,
    88 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_extent_bump_alloc_post (1x)
     .           	    base_size);
     .           
     .           	return base;
    12 ( 0.00%)  }
     .           
     .           void
     .           base_delete(tsdn_t *tsdn, base_t *base) {
     .           	ehooks_t *ehooks = base_ehooks_get_for_metadata(base);
     .           	base_block_t *next = base->blocks;
     .           	do {
     .           		base_block_t *block = next;
     .           		next = block->next;
     .           		base_unmap(tsdn, ehooks, base_ind_get(base), block,
     .           		    block->size);
     .           	} while (next != NULL);
     .           }
     .           
     .           ehooks_t *
 7,730 ( 0.00%)  base_ehooks_get(base_t *base) {
     .           	return &base->ehooks;
 3,865 ( 0.00%)  }
     .           
     .           ehooks_t *
     .           base_ehooks_get_for_metadata(base_t *base) {
     1 ( 0.00%)  	return &base->ehooks_base;
     .           }
     .           
     .           extent_hooks_t *
     .           base_extent_hooks_set(base_t *base, extent_hooks_t *extent_hooks) {
     .           	extent_hooks_t *old_extent_hooks =
     .           	    ehooks_get_extent_hooks_ptr(&base->ehooks);
     .           	ehooks_init(&base->ehooks, extent_hooks, ehooks_ind_get(&base->ehooks));
     .           	return old_extent_hooks;
     .           }
     .           
     .           static void *
     .           base_alloc_impl(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment,
 1,716 ( 0.00%)      size_t *esn) {
   312 ( 0.00%)  	alignment = QUANTUM_CEILING(alignment);
   780 ( 0.00%)  	size_t usize = ALIGNMENT_CEILING(size, alignment);
   468 ( 0.00%)  	size_t asize = usize + alignment - QUANTUM;
     .           
     .           	edata_t *edata = NULL;
   312 ( 0.00%)  	malloc_mutex_lock(tsdn, &base->mtx);
13,988 ( 0.00%)  	for (szind_t i = sz_size2index(asize); i < SC_NSIZES; i++) {
 9,761 ( 0.00%)  		edata = edata_heap_remove_first(&base->avail[i]);
53,941 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_remove_first (8,188x)
16,376 ( 0.00%)  		if (edata != NULL) {
     .           			/* Use existing space. */
     .           			break;
     .           		}
     .           	}
     .           	if (edata == NULL) {
     .           		/* Try to allocate more space. */
     .           		edata = base_extent_alloc(tsdn, base, usize, alignment);
     .           	}
     .           	void *ret;
     .           	if (edata == NULL) {
     .           		ret = NULL;
     .           		goto label_return;
     .           	}
     .           
     .           	ret = base_extent_bump_alloc(base, edata, usize, alignment);
   468 ( 0.00%)  	if (esn != NULL) {
   304 ( 0.00%)  		*esn = (size_t)edata_sn_get(edata);
     .           	}
     .           label_return:
     .           	malloc_mutex_unlock(tsdn, &base->mtx);
     .           	return ret;
 1,404 ( 0.00%)  }
     .           
     .           /*
     .            * base_alloc() returns zeroed memory, which is always demand-zeroed for the
     .            * auto arenas, in order to make multi-page sparse data structures such as radix
     .            * tree nodes efficient with respect to physical memory usage.  Upon success a
     .            * pointer to at least size bytes with specified alignment is returned.  Note
     .            * that size is rounded up to the nearest multiple of alignment to avoid false
     .            * sharing.
     .            */
     .           void *
     4 ( 0.00%)  base_alloc(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment) {
     8 ( 0.00%)  	return base_alloc_impl(tsdn, base, size, alignment, NULL);
 4,417 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_alloc_impl (4x)
     .           }
     .           
     .           edata_t *
 1,064 ( 0.00%)  base_alloc_edata(tsdn_t *tsdn, base_t *base) {
     .           	size_t esn;
   608 ( 0.00%)  	edata_t *edata = base_alloc_impl(tsdn, base, sizeof(edata_t),
127,085 ( 0.04%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:base_alloc_impl (152x)
     .           	    EDATA_ALIGNMENT, &esn);
   304 ( 0.00%)  	if (edata == NULL) {
     .           		return NULL;
     .           	}
     .           	edata_esn_set(edata, esn);
     .           	return edata;
   760 ( 0.00%)  }
     .           
     .           void
     .           base_stats_get(tsdn_t *tsdn, base_t *base, size_t *allocated, size_t *resident,
     .               size_t *mapped, size_t *n_thp) {
     .           	cassert(config_stats);
     .           
     .           	malloc_mutex_lock(tsdn, &base->mtx);
     .           	assert(base->allocated <= base->resident);
-- line 500 ----------------------------------------
-- line 517 ----------------------------------------
     .           }
     .           
     .           void
     .           base_postfork_child(tsdn_t *tsdn, base_t *base) {
     .           	malloc_mutex_postfork_child(tsdn, &base->mtx);
     .           }
     .           
     .           bool
     3 ( 0.00%)  base_boot(tsdn_t *tsdn) {
     5 ( 0.00%)  	b0 = base_new(tsdn, 0, (extent_hooks_t *)&ehooks_default_extent_hooks,
 2,111 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_base_new (1x)
     .           	    /* metadata_use_hooks */ true);
     2 ( 0.00%)  	return (b0 == NULL);
     2 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/edata.h
--------------------------------------------------------------------------------
Ir              

-- line 245 ----------------------------------------
     .           		/* Small region slab metadata. */
     .           		slab_data_t	e_slab_data;
     .           
     .           		/* Profiling data, used for large objects. */
     .           		e_prof_info_t	e_prof_info;
     .           	};
     .           };
     .           
    37 ( 0.00%)  TYPED_LIST(edata_list_active, edata_t, ql_link_active)
66,293 ( 0.02%)  TYPED_LIST(edata_list_inactive, edata_t, ql_link_inactive)
     .           
     .           static inline unsigned
     .           edata_arena_ind_get(const edata_t *edata) {
12,182 ( 0.00%)  	unsigned arena_ind = (unsigned)((edata->e_bits &
     .           	    EDATA_BITS_ARENA_MASK) >> EDATA_BITS_ARENA_SHIFT);
     .           	assert(arena_ind < MALLOCX_ARENA_LIMIT);
     .           
     .           	return arena_ind;
     .           }
     .           
     .           static inline szind_t
     .           edata_szind_get_maybe_invalid(const edata_t *edata) {
 7,948 ( 0.00%)  	szind_t szind = (szind_t)((edata->e_bits & EDATA_BITS_SZIND_MASK) >>
     .           	    EDATA_BITS_SZIND_SHIFT);
     .           	assert(szind <= SC_NSIZES);
     .           	return szind;
     .           }
     .           
     .           static inline szind_t
     .           edata_szind_get(const edata_t *edata) {
     .           	szind_t szind = edata_szind_get_maybe_invalid(edata);
-- line 275 ----------------------------------------
-- line 279 ----------------------------------------
     .           
     .           static inline size_t
     .           edata_usize_get(const edata_t *edata) {
     .           	return sz_index2size(edata_szind_get(edata));
     .           }
     .           
     .           static inline unsigned
     .           edata_binshard_get(const edata_t *edata) {
 4,109 ( 0.00%)  	unsigned binshard = (unsigned)((edata->e_bits &
 4,109 ( 0.00%)  	    EDATA_BITS_BINSHARD_MASK) >> EDATA_BITS_BINSHARD_SHIFT);
     .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
     .           	return binshard;
     .           }
     .           
     .           static inline uint64_t
     .           edata_sn_get(const edata_t *edata) {
 7,922 ( 0.00%)  	return edata->e_sn;
     .           }
     .           
     .           static inline extent_state_t
     .           edata_state_get(const edata_t *edata) {
 3,975 ( 0.00%)  	return (extent_state_t)((edata->e_bits & EDATA_BITS_STATE_MASK) >>
     .           	    EDATA_BITS_STATE_SHIFT);
     .           }
     .           
     .           static inline bool
     .           edata_guarded_get(const edata_t *edata) {
 1,523 ( 0.00%)  	return (bool)((edata->e_bits & EDATA_BITS_GUARDED_MASK) >>
     .           	    EDATA_BITS_GUARDED_SHIFT);
     .           }
     .           
     .           static inline bool
     .           edata_zeroed_get(const edata_t *edata) {
     .           	return (bool)((edata->e_bits & EDATA_BITS_ZEROED_MASK) >>
     .           	    EDATA_BITS_ZEROED_SHIFT);
     .           }
     .           
     .           static inline bool
     .           edata_committed_get(const edata_t *edata) {
 9,695 ( 0.00%)  	return (bool)((edata->e_bits & EDATA_BITS_COMMITTED_MASK) >>
     .           	    EDATA_BITS_COMMITTED_SHIFT);
     .           }
     .           
     .           static inline extent_pai_t
     .           edata_pai_get(const edata_t *edata) {
 3,693 ( 0.00%)  	return (extent_pai_t)((edata->e_bits & EDATA_BITS_PAI_MASK) >>
     .           	    EDATA_BITS_PAI_SHIFT);
     .           }
     .           
     .           static inline bool
     .           edata_slab_get(const edata_t *edata) {
 1,110 ( 0.00%)  	return (bool)((edata->e_bits & EDATA_BITS_SLAB_MASK) >>
     .           	    EDATA_BITS_SLAB_SHIFT);
     .           }
     .           
     .           static inline unsigned
     .           edata_nfree_get(const edata_t *edata) {
     .           	assert(edata_slab_get(edata));
 8,794 ( 0.00%)  	return (unsigned)((edata->e_bits & EDATA_BITS_NFREE_MASK) >>
     .           	    EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void *
     .           edata_base_get(const edata_t *edata) {
     .           	assert(edata->e_addr == PAGE_ADDR2BASE(edata->e_addr) ||
     .           	    !edata_slab_get(edata));
76,946 ( 0.02%)  	return PAGE_ADDR2BASE(edata->e_addr);
     .           }
     .           
     .           static inline void *
     .           edata_addr_get(const edata_t *edata) {
     .           	assert(edata->e_addr == PAGE_ADDR2BASE(edata->e_addr) ||
     .           	    !edata_slab_get(edata));
10,286 ( 0.00%)  	return edata->e_addr;
     .           }
     .           
     .           static inline size_t
     .           edata_size_get(const edata_t *edata) {
51,359 ( 0.01%)  	return (edata->e_size_esn & EDATA_SIZE_MASK);
     .           }
     .           
     .           static inline size_t
     .           edata_esn_get(const edata_t *edata) {
11,294 ( 0.00%)  	return (edata->e_size_esn & EDATA_ESN_MASK);
     .           }
     .           
     .           static inline size_t
     .           edata_bsize_get(const edata_t *edata) {
   157 ( 0.00%)  	return edata->e_bsize;
     .           }
     .           
     .           static inline hpdata_t *
     .           edata_ps_get(const edata_t *edata) {
     .           	assert(edata_pai_get(edata) == EXTENT_PAI_HPA);
     .           	return edata->e_ps;
     .           }
     .           
-- line 375 ----------------------------------------
-- line 376 ----------------------------------------
     .           static inline void *
     .           edata_before_get(const edata_t *edata) {
     .           	return (void *)((uintptr_t)edata_base_get(edata) - PAGE);
     .           }
     .           
     .           static inline void *
     .           edata_last_get(const edata_t *edata) {
     .           	return (void *)((uintptr_t)edata_base_get(edata) +
 8,940 ( 0.00%)  	    edata_size_get(edata) - PAGE);
     .           }
     .           
     .           static inline void *
     .           edata_past_get(const edata_t *edata) {
 2,761 ( 0.00%)  	return (void *)((uintptr_t)edata_base_get(edata) +
     .           	    edata_size_get(edata));
     .           }
     .           
     .           static inline slab_data_t *
     .           edata_slab_data_get(edata_t *edata) {
     .           	assert(edata_slab_get(edata));
     .           	return &edata->e_slab_data;
     .           }
-- line 397 ----------------------------------------
-- line 421 ----------------------------------------
     .           static inline prof_recent_t *
     .           edata_prof_recent_alloc_get_dont_call_directly(const edata_t *edata) {
     .           	return (prof_recent_t *)atomic_load_p(
     .           	    &edata->e_prof_info.e_prof_recent_alloc, ATOMIC_RELAXED);
     .           }
     .           
     .           static inline void
     .           edata_arena_ind_set(edata_t *edata, unsigned arena_ind) {
    18 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_ARENA_MASK) |
     6 ( 0.00%)  	    ((uint64_t)arena_ind << EDATA_BITS_ARENA_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_binshard_set(edata_t *edata, unsigned binshard) {
     .           	/* The assertion assumes szind is set already. */
     .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
     .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_BINSHARD_MASK) |
     .           	    ((uint64_t)binshard << EDATA_BITS_BINSHARD_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_addr_set(edata_t *edata, void *addr) {
     6 ( 0.00%)  	edata->e_addr = addr;
     .           }
     .           
     .           static inline void
     .           edata_size_set(edata_t *edata, size_t size) {
     .           	assert((size & ~EDATA_SIZE_MASK) == 0);
13,074 ( 0.00%)  	edata->e_size_esn = size | (edata->e_size_esn & ~EDATA_SIZE_MASK);
     .           }
     .           
     .           static inline void
     .           edata_esn_set(edata_t *edata, size_t esn) {
   912 ( 0.00%)  	edata->e_size_esn = (edata->e_size_esn & ~EDATA_ESN_MASK) | (esn &
     .           	    EDATA_ESN_MASK);
     .           }
     .           
     .           static inline void
     .           edata_bsize_set(edata_t *edata, size_t bsize) {
   157 ( 0.00%)  	edata->e_bsize = bsize;
     .           }
     .           
     .           static inline void
     .           edata_ps_set(edata_t *edata, hpdata_t *ps) {
     .           	assert(edata_pai_get(edata) == EXTENT_PAI_HPA);
     .           	edata->e_ps = ps;
     .           }
     .           
     .           static inline void
     .           edata_szind_set(edata_t *edata, szind_t szind) {
     .           	assert(szind <= SC_NSIZES); /* SC_NSIZES means "invalid". */
 8,058 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_SZIND_MASK) |
 2,801 ( 0.00%)  	    ((uint64_t)szind << EDATA_BITS_SZIND_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_nfree_set(edata_t *edata, unsigned nfree) {
     .           	assert(edata_slab_get(edata));
     .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_NFREE_MASK) |
     .           	    ((uint64_t)nfree << EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_nfree_binshard_set(edata_t *edata, unsigned nfree, unsigned binshard) {
     .           	/* The assertion assumes szind is set already. */
     .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
   698 ( 0.00%)  	edata->e_bits = (edata->e_bits &
     .           	    (~EDATA_BITS_NFREE_MASK & ~EDATA_BITS_BINSHARD_MASK)) |
 1,396 ( 0.00%)  	    ((uint64_t)binshard << EDATA_BITS_BINSHARD_SHIFT) |
   698 ( 0.00%)  	    ((uint64_t)nfree << EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_nfree_inc(edata_t *edata) {
     .           	assert(edata_slab_get(edata));
 8,595 ( 0.00%)  	edata->e_bits += ((uint64_t)1U << EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_nfree_dec(edata_t *edata) {
     .           	assert(edata_slab_get(edata));
     .           	edata->e_bits -= ((uint64_t)1U << EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_nfree_sub(edata_t *edata, uint64_t n) {
     .           	assert(edata_slab_get(edata));
 3,300 ( 0.00%)  	edata->e_bits -= (n << EDATA_BITS_NFREE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_sn_set(edata_t *edata, uint64_t sn) {
 6,232 ( 0.00%)  	edata->e_sn = sn;
     .           }
     .           
     .           static inline void
     .           edata_state_set(edata_t *edata, extent_state_t state) {
39,295 ( 0.01%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_STATE_MASK) |
 5,029 ( 0.00%)  	    ((uint64_t)state << EDATA_BITS_STATE_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_guarded_set(edata_t *edata, bool guarded) {
     .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_GUARDED_MASK) |
     .           	    ((uint64_t)guarded << EDATA_BITS_GUARDED_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_zeroed_set(edata_t *edata, bool zeroed) {
 4,188 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_ZEROED_MASK) |
 2,248 ( 0.00%)  	    ((uint64_t)zeroed << EDATA_BITS_ZEROED_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_committed_set(edata_t *edata, bool committed) {
 5,554 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_COMMITTED_MASK) |
 1,115 ( 0.00%)  	    ((uint64_t)committed << EDATA_BITS_COMMITTED_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_pai_set(edata_t *edata, extent_pai_t pai) {
   636 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_PAI_MASK) |
     .           	    ((uint64_t)pai << EDATA_BITS_PAI_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_slab_set(edata_t *edata, bool slab) {
 3,504 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_SLAB_MASK) |
 2,336 ( 0.00%)  	    ((uint64_t)slab << EDATA_BITS_SLAB_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_prof_tctx_set(edata_t *edata, prof_tctx_t *tctx) {
     .           	atomic_store_p(&edata->e_prof_info.e_prof_tctx, tctx, ATOMIC_RELEASE);
     .           }
     .           
     .           static inline void
-- line 557 ----------------------------------------
-- line 568 ----------------------------------------
     .           edata_prof_recent_alloc_set_dont_call_directly(edata_t *edata,
     .               prof_recent_t *recent_alloc) {
     .           	atomic_store_p(&edata->e_prof_info.e_prof_recent_alloc, recent_alloc,
     .           	    ATOMIC_RELAXED);
     .           }
     .           
     .           static inline bool
     .           edata_is_head_get(edata_t *edata) {
12,426 ( 0.00%)  	return (bool)((edata->e_bits & EDATA_BITS_IS_HEAD_MASK) >>
     .           	    EDATA_BITS_IS_HEAD_SHIFT);
     .           }
     .           
     .           static inline void
     .           edata_is_head_set(edata_t *edata, bool is_head) {
 3,375 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_IS_HEAD_MASK) |
     .           	    ((uint64_t)is_head << EDATA_BITS_IS_HEAD_SHIFT);
     .           }
     .           
     .           static inline bool
     .           edata_state_in_transition(extent_state_t state) {
     .           	return state >= extent_state_transition;
     .           }
     .           
-- line 590 ----------------------------------------
-- line 638 ----------------------------------------
     .           	edata_pai_set(edata, EXTENT_PAI_PAC);
     .           }
     .           
     .           static inline int
     .           edata_esn_comp(const edata_t *a, const edata_t *b) {
     .           	size_t a_esn = edata_esn_get(a);
     .           	size_t b_esn = edata_esn_get(b);
     .           
11,309 ( 0.00%)  	return (a_esn > b_esn) - (a_esn < b_esn);
     .           }
     .           
     .           static inline int
     .           edata_ead_comp(const edata_t *a, const edata_t *b) {
     .           	uintptr_t a_eaddr = (uintptr_t)a;
     .           	uintptr_t b_eaddr = (uintptr_t)b;
     .           
12,468 ( 0.00%)  	return (a_eaddr > b_eaddr) - (a_eaddr < b_eaddr);
     .           }
     .           
     .           static inline edata_cmp_summary_t
     .           edata_cmp_summary_get(const edata_t *edata) {
 1,236 ( 0.00%)  	return (edata_cmp_summary_t){edata_sn_get(edata),
 2,243 ( 0.00%)  		(uintptr_t)edata_addr_get(edata)};
     .           }
     .           
     .           static inline int
     .           edata_cmp_summary_comp(edata_cmp_summary_t a, edata_cmp_summary_t b) {
     .           	int ret;
35,474 ( 0.01%)  	ret = (a.sn > b.sn) - (a.sn < b.sn);
10,386 ( 0.00%)  	if (ret != 0) {
     .           		return ret;
     .           	}
15,458 ( 0.00%)  	ret = (a.addr > b.addr) - (a.addr < b.addr);
     .           	return ret;
     .           }
     .           
     .           static inline int
     .           edata_snad_comp(const edata_t *a, const edata_t *b) {
     .           	edata_cmp_summary_t a_cmp = edata_cmp_summary_get(a);
     .           	edata_cmp_summary_t b_cmp = edata_cmp_summary_get(b);
     .           
-- line 678 ----------------------------------------
-- line 679 ----------------------------------------
     .           	return edata_cmp_summary_comp(a_cmp, b_cmp);
     .           }
     .           
     .           static inline int
     .           edata_esnead_comp(const edata_t *a, const edata_t *b) {
     .           	int ret;
     .           
     .           	ret = edata_esn_comp(a, b);
 5,624 ( 0.00%)  	if (ret != 0) {
     .           		return ret;
     .           	}
     .           
     .           	ret = edata_ead_comp(a, b);
     .           	return ret;
     .           }
     .           
     .           ph_proto(, edata_avail, edata_t)
-- line 695 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c
--------------------------------------------------------------------------------
Ir              

-- line 2 ----------------------------------------
     .           #include "jemalloc/internal/jemalloc_internal_includes.h"
     .           #include "jemalloc/internal/sz.h"
     .           
     .           JEMALLOC_ALIGNED(CACHELINE)
     .           size_t sz_pind2sz_tab[SC_NPSIZES+1];
     .           size_t sz_large_pad;
     .           
     .           size_t
 4,452 ( 0.00%)  sz_psz_quantize_floor(size_t size) {
     .           	size_t ret;
     .           	pszind_t pind;
     .           
     .           	assert(size > 0);
     .           	assert((size & PAGE_MASK) == 0);
     .           
23,024 ( 0.01%)  	pind = sz_psz2ind(size - sz_large_pad + 1);
 2,323 ( 0.00%)  	if (pind == 0) {
     .           		/*
     .           		 * Avoid underflow.  This short-circuit would also do the right
     .           		 * thing for all sizes in the range for which there are
     .           		 * PAGE-spaced size classes, but it's simplest to just handle
     .           		 * the one case that would cause erroneous results.
     .           		 */
   785 ( 0.00%)  		return size;
     .           	}
19,693 ( 0.01%)  	ret = sz_pind2sz(pind - 1) + sz_large_pad;
     .           	assert(ret <= size);
     .           	return ret;
 4,452 ( 0.00%)  }
     .           
     .           size_t
 2,608 ( 0.00%)  sz_psz_quantize_ceil(size_t size) {
     .           	size_t ret;
     .           
     .           	assert(size > 0);
     .           	assert(size - sz_large_pad <= SC_LARGE_MAXCLASS);
     .           	assert((size & PAGE_MASK) == 0);
     .           
     .           	ret = sz_psz_quantize_floor(size);
 2,372 ( 0.00%)  	if (ret < size) {
     .           		/*
     .           		 * Skip a quantization that may have an adequately large extent,
     .           		 * because under-sized extents may be mixed in.  This only
     .           		 * happens when an unusual size is requested, i.e. for aligned
     .           		 * allocation, and is just one of several places where linear
     .           		 * search would potentially find sufficiently aligned available
     .           		 * memory somewhere lower.
     .           		 */
     .           		ret = sz_pind2sz(sz_psz2ind(ret - sz_large_pad + 1)) +
     .           		    sz_large_pad;
     .           	}
     .           	return ret;
 2,608 ( 0.00%)  }
     .           
     .           static void
     .           sz_boot_pind2sz_tab(const sc_data_t *sc_data) {
     1 ( 0.00%)  	int pind = 0;
   117 ( 0.00%)  	for (unsigned i = 0; i < SC_NSIZES; i++) {
     .           		const sc_t *sc = &sc_data->sc[i];
   464 ( 0.00%)  		if (sc->psz) {
   798 ( 0.00%)  			sz_pind2sz_tab[pind] = (ZU(1) << sc->lg_base)
   995 ( 0.00%)  			    + (ZU(sc->ndelta) << sc->lg_delta);
   199 ( 0.00%)  			pind++;
     .           		}
     .           	}
     3 ( 0.00%)  	for (int i = pind; i <= (int)SC_NPSIZES; i++) {
     5 ( 0.00%)  		sz_pind2sz_tab[pind] = sc_data->large_maxclass + PAGE;
     .           	}
     .           }
     .           
     .           JEMALLOC_ALIGNED(CACHELINE)
     .           size_t sz_index2size_tab[SC_NSIZES];
     .           
     .           static void
     .           sz_boot_index2size_tab(const sc_data_t *sc_data) {
   116 ( 0.00%)  	for (unsigned i = 0; i < SC_NSIZES; i++) {
     .           		const sc_t *sc = &sc_data->sc[i];
   698 ( 0.00%)  		sz_index2size_tab[i] = (ZU(1) << sc->lg_base)
 1,160 ( 0.00%)  		    + (ZU(sc->ndelta) << (sc->lg_delta));
     .           	}
     .           }
     .           
     .           /*
     .            * To keep this table small, we divide sizes by the tiny min size, which gives
     .            * the smallest interval for which the result can change.
     .            */
     .           JEMALLOC_ALIGNED(CACHELINE)
     .           uint8_t sz_size2index_tab[(SC_LOOKUP_MAXCLASS >> SC_LG_TINY_MIN) + 1];
     .           
     .           static void
     .           sz_boot_size2index_tab(const sc_data_t *sc_data) {
     .           	size_t dst_max = (SC_LOOKUP_MAXCLASS >> SC_LG_TINY_MIN) + 1;
     1 ( 0.00%)  	size_t dst_ind = 0;
   117 ( 0.00%)  	for (unsigned sc_ind = 0; sc_ind < SC_NSIZES && dst_ind < dst_max;
    29 ( 0.00%)  	    sc_ind++) {
     .           		const sc_t *sc = &sc_data->sc[sc_ind];
   117 ( 0.00%)  		size_t sz = (ZU(1) << sc->lg_base)
   203 ( 0.00%)  		    + (ZU(sc->ndelta) << sc->lg_delta);
   116 ( 0.00%)  		size_t max_ind = ((sz + (ZU(1) << SC_LG_TINY_MIN) - 1)
     .           				   >> SC_LG_TINY_MIN);
    87 ( 0.00%)  		for (; dst_ind <= max_ind && dst_ind < dst_max; dst_ind++) {
   538 ( 0.00%)  			sz_size2index_tab[dst_ind] = sc_ind;
     .           		}
     .           	}
     .           }
     .           
     .           void
     8 ( 0.00%)  sz_boot(const sc_data_t *sc_data, bool cache_oblivious) {
     4 ( 0.00%)  	sz_large_pad = cache_oblivious ? PAGE : 0;
     .           	sz_boot_pind2sz_tab(sc_data);
     .           	sz_boot_index2size_tab(sc_data);
     .           	sz_boot_size2index_tab(sc_data);
     4 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/thread_event.c
--------------------------------------------------------------------------------
Ir              

-- line 189 ----------------------------------------
     .            * remote thread will (at some arbitrary point in the future) zero out one half
     .            * of the owner thread's next_event_fast, but that's always safe (it just sends
     .            * it down the slow path earlier).
     .            */
     .           static void
     .           te_ctx_next_event_fast_update(te_ctx_t *ctx) {
     .           	uint64_t next_event = te_ctx_next_event_get(ctx);
     .           	uint64_t next_event_fast = (next_event <= TE_NEXT_EVENT_FAST_MAX) ?
10,925 ( 0.00%)  	    next_event : 0U;
     .           	te_ctx_next_event_fast_set(ctx, next_event_fast);
     .           }
     .           
     .           void
     5 ( 0.00%)  te_recompute_fast_threshold(tsd_t *tsd) {
 4,378 ( 0.00%)  	if (tsd_state_get(tsd) != tsd_state_nominal) {
     .           		/* Check first because this is also called on purgatory. */
     .           		te_next_event_fast_set_non_nominal(tsd);
     3 ( 0.00%)  		return;
     .           	}
     .           
     .           	te_ctx_t ctx;
     .           	te_ctx_get(tsd, &ctx, true);
     .           	te_ctx_next_event_fast_update(&ctx);
     .           	te_ctx_get(tsd, &ctx, false);
     .           	te_ctx_next_event_fast_update(&ctx);
     .           
     .           	atomic_fence(ATOMIC_SEQ_CST);
 4,370 ( 0.00%)  	if (tsd_state_get(tsd) != tsd_state_nominal) {
     .           		te_next_event_fast_set_non_nominal(tsd);
     .           	}
     3 ( 0.00%)  }
     .           
     .           static void
     .           te_adjust_thresholds_helper(tsd_t *tsd, te_ctx_t *ctx,
     .               uint64_t wait) {
     .           	/*
     .           	 * The next threshold based on future events can only be adjusted after
     .           	 * progressing the last_event counter (which is set to current).
     .           	 */
     .           	assert(te_ctx_current_bytes_get(ctx) == te_ctx_last_event_get(ctx));
     .           	assert(wait <= TE_MAX_START_WAIT);
     .           
 2,184 ( 0.00%)  	uint64_t next_event = te_ctx_last_event_get(ctx) + (wait <=
 6,556 ( 0.00%)  	    TE_MAX_INTERVAL ? wait : TE_MAX_INTERVAL);
     .           	te_ctx_next_event_set(tsd, ctx, next_event);
     .           }
     .           
     .           static uint64_t
     .           te_clip_event_wait(uint64_t event_wait) {
     .           	assert(event_wait > 0U);
     .           	if (TE_MIN_START_WAIT > 1U &&
     .           	    unlikely(event_wait < TE_MIN_START_WAIT)) {
-- line 240 ----------------------------------------
-- line 243 ----------------------------------------
     .           	if (TE_MAX_START_WAIT < UINT64_MAX &&
     .           	    unlikely(event_wait > TE_MAX_START_WAIT)) {
     .           		event_wait = TE_MAX_START_WAIT;
     .           	}
     .           	return event_wait;
     .           }
     .           
     .           void
26,184 ( 0.01%)  te_event_trigger(tsd_t *tsd, te_ctx_t *ctx) {
     .           	/* usize has already been added to thread_allocated. */
     .           	uint64_t bytes_after = te_ctx_current_bytes_get(ctx);
     .           	/* The subtraction is intentionally susceptible to underflow. */
 4,364 ( 0.00%)  	uint64_t accumbytes = bytes_after - te_ctx_last_event_get(ctx);
     .           
     .           	te_ctx_last_event_set(ctx, bytes_after);
     .           
10,910 ( 0.00%)  	bool allow_event_trigger = tsd_nominal(tsd) &&
     .           	    tsd_reentrancy_level_get(tsd) == 0;
 2,182 ( 0.00%)  	bool is_alloc = ctx->is_alloc;
     .           	uint64_t wait = TE_MAX_START_WAIT;
     .           
     .           #define E(event, condition, alloc_event)				\
     .           	bool is_##event##_triggered = false;				\
     .           	if (is_alloc == alloc_event && condition) {			\
     .           		uint64_t event_wait = event##_event_wait_get(tsd);	\
     .           		assert(event_wait <= TE_MAX_START_WAIT);		\
     .           		if (event_wait > accumbytes) {				\
-- line 269 ----------------------------------------
-- line 276 ----------------------------------------
     .           		}							\
     .           		event_wait = te_clip_event_wait(event_wait);		\
     .           		event##_event_wait_set(tsd, event_wait);		\
     .           		if (event_wait < wait) {				\
     .           			wait = event_wait;				\
     .           		}							\
     .           	}
     .           
82,958 ( 0.02%)  	ITERATE_OVER_ALL_EVENTS
 3,294 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_gc_new_event_wait (1,098x)
 3,294 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/peak_event.c:_rjem_je_peak_alloc_new_event_wait (1,098x)
 3,252 ( 0.00%)  => ???:_rjem_je_tcache_gc_dalloc_new_event_wait (1,084x)
 3,252 ( 0.00%)  => ???:_rjem_je_peak_dalloc_new_event_wait (1,084x)
     .           #undef E
     .           
     .           	assert(wait <= TE_MAX_START_WAIT);
     .           	te_adjust_thresholds_helper(tsd, ctx, wait);
     .           	te_assert_invariants(tsd);
     .           
     .           #define E(event, condition, alloc_event)				\
     .           	if (is_alloc == alloc_event && condition &&			\
     .           	    is_##event##_triggered) {					\
     .           		assert(allow_event_trigger);				\
     .           		uint64_t elapsed = event##_fetch_elapsed(tsd);		\
     .           		event##_event_handler(tsd, elapsed);			\
     .           	}
     .           
46,962 ( 0.01%)  	ITERATE_OVER_ALL_EVENTS
658,795 ( 0.18%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_gc_event_handler (1,098x)
628,953 ( 0.17%)  => ???:_rjem_je_tcache_gc_dalloc_event_handler (1,084x)
14,423 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/peak_event.c:_rjem_je_peak_alloc_event_handler (1,098x)
14,114 ( 0.00%)  => ???:_rjem_je_peak_dalloc_event_handler (1,084x)
     .           #undef E
     .           
     .           	te_assert_invariants(tsd);
16,372 ( 0.00%)  }
     .           
     .           static void
     .           te_init(tsd_t *tsd, bool is_alloc) {
     .           	te_ctx_t ctx;
     .           	te_ctx_get(tsd, &ctx, is_alloc);
     .           	/*
     .           	 * Reset the last event to current, which starts the events from a clean
     .           	 * state.  This is necessary when re-init the tsd event counters.
-- line 311 ----------------------------------------
-- line 323 ----------------------------------------
     .           		uint64_t event_wait = event##_new_event_wait(tsd);	\
     .           		event_wait = te_clip_event_wait(event_wait);		\
     .           		event##_event_wait_set(tsd, event_wait);		\
     .           		if (event_wait < wait) {				\
     .           			wait = event_wait;				\
     .           		}							\
     .           	}
     .           
    20 ( 0.00%)  	ITERATE_OVER_ALL_EVENTS
     3 ( 0.00%)  => ???:_rjem_je_peak_dalloc_new_event_wait (1x)
     3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_gc_new_event_wait (1x)
     3 ( 0.00%)  => ???:_rjem_je_tcache_gc_dalloc_new_event_wait (1x)
     3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/peak_event.c:_rjem_je_peak_alloc_new_event_wait (1x)
     .           #undef E
     .           	te_adjust_thresholds_helper(tsd, &ctx, wait);
     .           }
     .           
     .           void
     9 ( 0.00%)  tsd_te_init(tsd_t *tsd) {
     .           	/* Make sure no overflow for the bytes accumulated on event_trigger. */
     .           	assert(TE_MAX_INTERVAL <= UINT64_MAX - SC_LARGE_MAXCLASS + 1);
     .           	te_init(tsd, true);
     .           	te_init(tsd, false);
     .           	te_assert_invariants(tsd);
     6 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.8.5/src/distributions/float.rs
--------------------------------------------------------------------------------
Ir               

-- line 145 ----------------------------------------
      .                           let value: $uty = rng.gen();
      .                           let fraction = value >> (float_size - $fraction_bits);
      .                           fraction.into_float_with_exponent(0) - (1.0 - EPSILON / 2.0)
      .                       }
      .                   }
      .               }
      .           }
      .           
700,000 ( 0.19%)  float_impls! { f32, u32, f32, u32, 23, 127 }
      .           float_impls! { f64, u64, f64, u64, 52, 1023 }
      .           
      .           #[cfg(feature = "simd_support")]
      .           float_impls! { f32x2, u32x2, f32, u32, 23, 127 }
      .           #[cfg(feature = "simd_support")]
      .           float_impls! { f32x4, u32x4, f32, u32, 23, 127 }
      .           #[cfg(feature = "simd_support")]
      .           float_impls! { f32x8, u32x8, f32, u32, 23, 127 }
-- line 161 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c
--------------------------------------------------------------------------------
Ir              

-- line 12 ----------------------------------------
     .           static void
     .           pa_nactive_sub(pa_shard_t *shard, size_t sub_pages) {
     .           	assert(atomic_load_zu(&shard->nactive, ATOMIC_RELAXED) >= sub_pages);
     .           	atomic_fetch_sub_zu(&shard->nactive, sub_pages, ATOMIC_RELAXED);
     .           }
     .           
     .           bool
     .           pa_central_init(pa_central_t *central, base_t *base, bool hpa,
     1 ( 0.00%)      hpa_hooks_t *hpa_hooks) {
     .           	bool err;
     2 ( 0.00%)  	if (hpa) {
     .           		err = hpa_central_init(&central->hpa, base, hpa_hooks);
     .           		if (err) {
     .           			return true;
     .           		}
     .           	}
     .           	return false;
     2 ( 0.00%)  }
     .           
     .           bool
     .           pa_shard_init(tsdn_t *tsdn, pa_shard_t *shard, pa_central_t *central,
     .               emap_t *emap, base_t *base, unsigned ind, pa_shard_stats_t *stats,
     .               malloc_mutex_t *stats_mtx, nstime_t *cur_time,
     .               size_t pac_oversize_threshold, ssize_t dirty_decay_ms,
    22 ( 0.00%)      ssize_t muzzy_decay_ms) {
     .           	/* This will change eventually, but for now it should hold. */
     .           	assert(base_ind_get(base) == ind);
     6 ( 0.00%)  	if (edata_cache_init(&shard->edata_cache, base)) {
   157 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata_cache.c:_rjem_je_edata_cache_init (1x)
     .           		return true;
     .           	}
     .           
    18 ( 0.00%)  	if (pac_init(tsdn, &shard->pac, base, emap, &shard->edata_cache,
12,174 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:_rjem_je_pac_init (1x)
     .           	    cur_time, pac_oversize_threshold, dirty_decay_ms, muzzy_decay_ms,
     .           	    &stats->pac_stats, stats_mtx)) {
     .           		return true;
     .           	}
     .           
     2 ( 0.00%)  	shard->ind = ind;
     .           
     1 ( 0.00%)  	shard->ever_used_hpa = false;
     .           	atomic_store_b(&shard->use_hpa, false, ATOMIC_RELAXED);
     .           
     .           	atomic_store_zu(&shard->nactive, 0, ATOMIC_RELAXED);
     .           
     2 ( 0.00%)  	shard->stats_mtx = stats_mtx;
     .           	shard->stats = stats;
     .           	memset(shard->stats, 0, sizeof(*shard->stats));
     .           
     2 ( 0.00%)  	shard->central = central;
     2 ( 0.00%)  	shard->emap = emap;
     .           	shard->base = base;
     .           
     .           	return false;
     8 ( 0.00%)  }
     .           
     .           bool
     .           pa_shard_enable_hpa(tsdn_t *tsdn, pa_shard_t *shard,
     .               const hpa_shard_opts_t *hpa_opts, const sec_opts_t *hpa_sec_opts) {
     .           	if (hpa_shard_init(&shard->hpa_shard, &shard->central->hpa, shard->emap,
     .           	    shard->base, &shard->edata_cache, shard->ind, hpa_opts)) {
     .           		return true;
     .           	}
-- line 73 ----------------------------------------
-- line 110 ----------------------------------------
     .           		sec_flush(tsdn, &shard->hpa_sec);
     .           		hpa_shard_disable(tsdn, &shard->hpa_shard);
     .           	}
     .           }
     .           
     .           static pai_t *
     .           pa_get_pai(pa_shard_t *shard, edata_t *edata) {
     .           	return (edata_pai_get(edata) == EXTENT_PAI_PAC
 6,092 ( 0.00%)  	    ? &shard->pac.pai : &shard->hpa_sec.pai);
     .           }
     .           
     .           edata_t *
     .           pa_alloc(tsdn_t *tsdn, pa_shard_t *shard, size_t size, size_t alignment,
     .               bool slab, szind_t szind, bool zero, bool guarded,
17,520 ( 0.00%)      bool *deferred_work_generated) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           	assert(!guarded || alignment <= PAGE);
     .           
     .           	edata_t *edata = NULL;
 4,672 ( 0.00%)  	if (!guarded && pa_shard_uses_hpa(shard)) {
 2,336 ( 0.00%)  		edata = pai_alloc(tsdn, &shard->hpa_sec.pai, size, alignment,
     .           		    zero, /* guarded */ false, slab, deferred_work_generated);
     .           	}
     .           	/*
     .           	 * Fall back to the PAC if the HPA is off or couldn't serve the given
     .           	 * allocation request.
     .           	 */
     .           	if (edata == NULL) {
 1,168 ( 0.00%)  		edata = pai_alloc(tsdn, &shard->pac.pai, size, alignment, zero,
     .           		    guarded, slab, deferred_work_generated);
     .           	}
 4,672 ( 0.00%)  	if (edata != NULL) {
     .           		assert(edata_size_get(edata) == size);
 2,336 ( 0.00%)  		pa_nactive_add(shard, size >> LG_PAGE);
 9,344 ( 0.00%)  		emap_remap(tsdn, shard->emap, edata, szind, slab);
89,698 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_remap (1,168x)
     .           		edata_szind_set(edata, szind);
     .           		edata_slab_set(edata, slab);
 4,420 ( 0.00%)  		if (slab && (size > 2 * PAGE)) {
 1,338 ( 0.00%)  			emap_register_interior(tsdn, shard->emap, edata, szind);
18,383 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_register_interior (223x)
     .           		}
     .           		assert(edata_arena_ind_get(edata) == shard->ind);
     .           	}
     .           	return edata;
10,512 ( 0.00%)  }
     .           
     .           bool
     .           pa_expand(tsdn_t *tsdn, pa_shard_t *shard, edata_t *edata, size_t old_size,
 4,956 ( 0.00%)      size_t new_size, szind_t szind, bool zero, bool *deferred_work_generated) {
     .           	assert(new_size > old_size);
     .           	assert(edata_size_get(edata) == old_size);
     .           	assert((new_size & PAGE_MASK) == 0);
 1,239 ( 0.00%)  	if (edata_guarded_get(edata)) {
   258 ( 0.00%)  		return true;
     .           	}
 1,136 ( 0.00%)  	size_t expand_amount = new_size - old_size;
     .           
     .           	pai_t *pai = pa_get_pai(shard, edata);
     .           
     .           	bool error = pai_expand(tsdn, pai, edata, old_size, new_size, zero,
     .           	    deferred_work_generated);
 1,652 ( 0.00%)  	if (error) {
     .           		return true;
     .           	}
     .           
   155 ( 0.00%)  	pa_nactive_add(shard, expand_amount >> LG_PAGE);
     .           	edata_szind_set(edata, szind);
   775 ( 0.00%)  	emap_remap(tsdn, shard->emap, edata, szind, /* slab */ false);
10,695 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_remap (155x)
   155 ( 0.00%)  	return false;
 3,717 ( 0.00%)  }
     .           
     .           bool
     .           pa_shrink(tsdn_t *tsdn, pa_shard_t *shard, edata_t *edata, size_t old_size,
     .               size_t new_size, szind_t szind, bool *deferred_work_generated) {
     .           	assert(new_size < old_size);
     .           	assert(edata_size_get(edata) == old_size);
     .           	assert((new_size & PAGE_MASK) == 0);
     .           	if (edata_guarded_get(edata)) {
-- line 187 ----------------------------------------
-- line 199 ----------------------------------------
     .           
     .           	edata_szind_set(edata, szind);
     .           	emap_remap(tsdn, shard->emap, edata, szind, /* slab */ false);
     .           	return false;
     .           }
     .           
     .           void
     .           pa_dalloc(tsdn_t *tsdn, pa_shard_t *shard, edata_t *edata,
12,210 ( 0.00%)      bool *deferred_work_generated) {
 4,440 ( 0.00%)  	emap_remap(tsdn, shard->emap, edata, SC_NSIZES, /* slab */ false);
36,630 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_remap (1,110x)
 2,220 ( 0.00%)  	if (edata_slab_get(edata)) {
 1,208 ( 0.00%)  		emap_deregister_interior(tsdn, shard->emap, edata);
18,668 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_deregister_interior (302x)
     .           		/*
     .           		 * The slab state of the extent isn't cleared.  It may be used
     .           		 * by the pai implementation, e.g. to make caching decisions.
     .           		 */
     .           	}
     .           	edata_addr_set(edata, edata_base_get(edata));
     .           	edata_szind_set(edata, SC_NSIZES);
     .           	pa_nactive_sub(shard, edata_size_get(edata) >> LG_PAGE);
     .           	pai_t *pai = pa_get_pai(shard, edata);
     .           	pai_dalloc(tsdn, pai, edata, deferred_work_generated);
 5,550 ( 0.00%)  }
     .           
     .           bool
     .           pa_shard_retain_grow_limit_get_set(tsdn_t *tsdn, pa_shard_t *shard,
     .               size_t *old_limit, size_t *new_limit) {
     .           	return pac_retain_grow_limit_get_set(tsdn, &shard->pac, old_limit,
     .           	    new_limit);
     .           }
     .           
-- line 229 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/cache_bin.h
--------------------------------------------------------------------------------
Ir                 

-- line 139 ----------------------------------------
        .           	ql_elm(cache_bin_array_descriptor_t) link;
        .           	/* Pointers to the tcache bins. */
        .           	cache_bin_t *bins;
        .           };
        .           
        .           static inline void
        .           cache_bin_array_descriptor_init(cache_bin_array_descriptor_t *descriptor,
        .               cache_bin_t *bins) {
        3 ( 0.00%)  	ql_elm_new(descriptor, link);
        .           	descriptor->bins = bins;
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE bool
        .           cache_bin_nonfast_aligned(const void *ptr) {
        .           	if (!config_uaf_detection) {
        .           		return false;
        .           	}
-- line 155 ----------------------------------------
-- line 166 ----------------------------------------
        .           	 * not the desired behavior.
        .           	 */
        .           	return ((uintptr_t)ptr & san_cache_bin_nonfast_mask) == 0;
        .           }
        .           
        .           /* Returns ncached_max: Upper limit on ncached. */
        .           static inline cache_bin_sz_t
        .           cache_bin_info_ncached_max(cache_bin_info_t *info) {
    1,996 ( 0.00%)  	return info->ncached_max;
        .           }
        .           
        .           /*
        .            * Internal.
        .            *
        .            * Asserts that the pointer associated with earlier is <= the one associated
        .            * with later.
        .            */
-- line 182 ----------------------------------------
-- line 198 ----------------------------------------
        .           	/*
        .           	 * When it's racy, bin->low_bits_full can be modified concurrently. It
        .           	 * can cross the uint16_t max value and become less than
        .           	 * bin->low_bits_empty at the time of the check.
        .           	 */
        .           	if (!racy) {
        .           		cache_bin_assert_earlier(bin, earlier, later);
        .           	}
   31,579 ( 0.01%)  	return later - earlier;
        .           }
        .           
        .           /*
        .            * Number of items currently cached in the bin, without checking ncached_max.
        .            * We require specifying whether or not the request is racy or not (i.e. whether
        .            * or not concurrent modifications are possible).
        .            */
        .           static inline cache_bin_sz_t
        .           cache_bin_ncached_get_internal(cache_bin_t *bin, bool racy) {
        .           	cache_bin_sz_t diff = cache_bin_diff(bin,
    8,961 ( 0.00%)  	    (uint16_t)(uintptr_t)bin->stack_head, bin->low_bits_empty, racy);
    3,443 ( 0.00%)  	cache_bin_sz_t n = diff / sizeof(void *);
        .           	/*
        .           	 * We have undefined behavior here; if this function is called from the
        .           	 * arena stats updating code, then stack_head could change from the
        .           	 * first line to the next one.  Morally, these loads should be atomic,
        .           	 * but compilers won't currently generate comparisons with in-memory
        .           	 * operands against atomics, and these variables get accessed on the
        .           	 * fast paths.  This should still be "safe" in the sense of generating
        .           	 * the correct assembly for the foreseeable future, though.
-- line 226 ----------------------------------------
-- line 248 ----------------------------------------
        .            * A pointer to the position one past the end of the backing array.
        .            *
        .            * Do not call if racy, because both 'bin->stack_head' and 'bin->low_bits_full'
        .            * are subject to concurrent modifications.
        .            */
        .           static inline void **
        .           cache_bin_empty_position_get(cache_bin_t *bin) {
        .           	cache_bin_sz_t diff = cache_bin_diff(bin,
    3,435 ( 0.00%)  	    (uint16_t)(uintptr_t)bin->stack_head, bin->low_bits_empty,
        .           	    /* racy */ false);
    4,901 ( 0.00%)  	uintptr_t empty_bits = (uintptr_t)bin->stack_head + diff;
        .           	void **ret = (void **)empty_bits;
        .           
        .           	assert(ret >= bin->stack_head);
        .           
        .           	return ret;
        .           }
        .           
        .           /*
-- line 266 ----------------------------------------
-- line 271 ----------------------------------------
        .            *
        .            * No values are concurrently modified, so should be safe to read in a
        .            * multithreaded environment. Currently concurrent access happens only during
        .            * arena statistics collection.
        .            */
        .           static inline uint16_t
        .           cache_bin_low_bits_low_bound_get(cache_bin_t *bin, cache_bin_info_t *info) {
        .           	return (uint16_t)bin->low_bits_empty -
    3,689 ( 0.00%)  	    info->ncached_max * sizeof(void *);
        .           }
        .           
        .           /*
        .            * Internal.
        .            *
        .            * A pointer to the position with the lowest address of the backing array.
        .            */
        .           static inline void **
-- line 287 ----------------------------------------
-- line 306 ----------------------------------------
        .           /*
        .            * Get low water, but without any of the correctness checking we do for the
        .            * caller-usable version, if we are temporarily breaking invariants (like
        .            * ncached >= low_water during flush).
        .            */
        .           static inline cache_bin_sz_t
        .           cache_bin_low_water_get_internal(cache_bin_t *bin) {
        .           	return cache_bin_diff(bin, bin->low_bits_low_water,
    4,136 ( 0.00%)  	    bin->low_bits_empty, /* racy */ false) / sizeof(void *);
        .           }
        .           
        .           /* Returns the numeric value of low water in [0, ncached]. */
        .           static inline cache_bin_sz_t
        .           cache_bin_low_water_get(cache_bin_t *bin, cache_bin_info_t *info) {
        .           	cache_bin_sz_t low_water = cache_bin_low_water_get_internal(bin);
        .           	assert(low_water <= cache_bin_info_ncached_max(info));
        .           	assert(low_water <= cache_bin_ncached_get_local(bin, info));
-- line 322 ----------------------------------------
-- line 328 ----------------------------------------
        .           }
        .           
        .           /*
        .            * Indicates that the current cache bin position should be the low water mark
        .            * going forward.
        .            */
        .           static inline void
        .           cache_bin_low_water_set(cache_bin_t *bin) {
    4,883 ( 0.00%)  	bin->low_bits_low_water = (uint16_t)(uintptr_t)bin->stack_head;
        .           }
        .           
        .           static inline void
        .           cache_bin_low_water_adjust(cache_bin_t *bin) {
    1,392 ( 0.00%)  	if (cache_bin_ncached_get_internal(bin, /* racy */ false)
        .           	    < cache_bin_low_water_get_internal(bin)) {
        .           		cache_bin_low_water_set(bin);
        .           	}
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE void *
        .           cache_bin_alloc_impl(cache_bin_t *bin, bool *success, bool adjust_low_water) {
        .           	/*
-- line 349 ----------------------------------------
-- line 353 ----------------------------------------
        .           	 * and eagerly checking ret would cause pipeline stall (waiting for the
        .           	 * cacheline).
        .           	 */
        .           
        .           	/*
        .           	 * This may read from the empty position; however the loaded value won't
        .           	 * be used.  It's safe because the stack has one more slot reserved.
        .           	 */
1,517,719 ( 0.42%)  	void *ret = *bin->stack_head;
        .           	uint16_t low_bits = (uint16_t)(uintptr_t)bin->stack_head;
  253,095 ( 0.07%)  	void **new_head = bin->stack_head + 1;
        .           
        .           	/*
        .           	 * Note that the low water mark is at most empty; if we pass this check,
        .           	 * we know we're non-empty.
        .           	 */
  506,190 ( 0.14%)  	if (likely(low_bits != bin->low_bits_low_water)) {
  249,817 ( 0.07%)  		bin->stack_head = new_head;
      687 ( 0.00%)  		*success = true;
      687 ( 0.00%)  		return ret;
        .           	}
        .           	if (!adjust_low_water) {
        .           		*success = false;
        .           		return NULL;
        .           	}
        .           	/*
        .           	 * In the fast-path case where we call alloc_easy and then alloc, the
        .           	 * previous checking and computation is optimized away -- we didn't
        .           	 * actually commit any of our operations.
        .           	 */
    6,556 ( 0.00%)  	if (likely(low_bits != bin->low_bits_empty)) {
    1,999 ( 0.00%)  		bin->stack_head = new_head;
    1,999 ( 0.00%)  		bin->low_bits_low_water = (uint16_t)(uintptr_t)new_head;
        .           		*success = true;
      956 ( 0.00%)  		return ret;
        .           	}
        .           	*success = false;
        .           	return NULL;
        .           }
        .           
        .           /*
        .            * Allocate an item out of the bin, failing if we're at the low-water mark.
        .            */
-- line 395 ----------------------------------------
-- line 419 ----------------------------------------
        .           	bin->stack_head += n;
        .           	cache_bin_low_water_adjust(bin);
        .           
        .           	return n;
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE bool
        .           cache_bin_full(cache_bin_t *bin) {
1,263,254 ( 0.35%)  	return ((uint16_t)(uintptr_t)bin->stack_head == bin->low_bits_full);
        .           }
        .           
        .           /*
        .            * Free an object into the given bin.  Fails only if the bin is full.
        .            */
        .           JEMALLOC_ALWAYS_INLINE bool
        .           cache_bin_dalloc_easy(cache_bin_t *bin, void *ptr) {
  503,904 ( 0.14%)  	if (unlikely(cache_bin_full(bin))) {
        .           		return false;
        .           	}
        .           
  503,904 ( 0.14%)  	bin->stack_head--;
  251,952 ( 0.07%)  	*bin->stack_head = ptr;
        .           	cache_bin_assert_earlier(bin, bin->low_bits_full,
        .           	    (uint16_t)(uintptr_t)bin->stack_head);
        .           
    3,357 ( 0.00%)  	return true;
        .           }
        .           
        .           /* Returns false if failed to stash (i.e. bin is full). */
        .           JEMALLOC_ALWAYS_INLINE bool
        .           cache_bin_stash(cache_bin_t *bin, void *ptr) {
        .           	if (cache_bin_full(bin)) {
        .           		return false;
        .           	}
-- line 452 ----------------------------------------
-- line 565 ----------------------------------------
        .           /*
        .            * Start a fill.  The bin must be empty, and This must be followed by a
        .            * finish_fill call before doing any alloc/dalloc operations on the bin.
        .            */
        .           static inline void
        .           cache_bin_init_ptr_array_for_fill(cache_bin_t *bin, cache_bin_info_t *info,
        .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nfill) {
        .           	cache_bin_assert_empty(bin, info);
    1,374 ( 0.00%)  	arr->ptr = cache_bin_empty_position_get(bin) - nfill;
        .           }
        .           
        .           /*
        .            * While nfill in cache_bin_init_ptr_array_for_fill is the number we *intend* to
        .            * fill, nfilled here is the number we actually filled (which may be less, in
        .            * case of OOM.
        .            */
        .           static inline void
        .           cache_bin_finish_fill(cache_bin_t *bin, cache_bin_info_t *info,
        .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nfilled) {
        .           	cache_bin_assert_empty(bin, info);
        .           	void **empty_position = cache_bin_empty_position_get(bin);
    1,374 ( 0.00%)  	if (nfilled < arr->n) {
    3,435 ( 0.00%)  		memmove(empty_position - nfilled, empty_position - arr->n,
        .           		    nfilled * sizeof(void *));
        .           	}
    1,374 ( 0.00%)  	bin->stack_head = empty_position - nfilled;
        .           }
        .           
        .           /*
        .            * Same deal, but with flush.  Unlike fill (which can fail), the user must flush
        .            * everything we give them.
        .            */
        .           static inline void
        .           cache_bin_init_ptr_array_for_flush(cache_bin_t *bin, cache_bin_info_t *info,
        .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nflush) {
    2,784 ( 0.00%)  	arr->ptr = cache_bin_empty_position_get(bin) - nflush;
        .           	assert(cache_bin_ncached_get_local(bin, info) == 0
        .           	    || *arr->ptr != NULL);
        .           }
        .           
        .           static inline void
        .           cache_bin_finish_flush(cache_bin_t *bin, cache_bin_info_t *info,
        .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nflushed) {
      696 ( 0.00%)  	unsigned rem = cache_bin_ncached_get_local(bin, info) - nflushed;
    3,480 ( 0.00%)  	memmove(bin->stack_head + nflushed, bin->stack_head,
        .           	    rem * sizeof(void *));
    1,392 ( 0.00%)  	bin->stack_head = bin->stack_head + nflushed;
        .           	cache_bin_low_water_adjust(bin);
        .           }
        .           
        .           static inline void
        .           cache_bin_init_ptr_array_for_stashed(cache_bin_t *bin, szind_t binind,
        .               cache_bin_info_t *info, cache_bin_ptr_array_t *arr,
        .               cache_bin_sz_t nstashed) {
        .           	assert(nstashed > 0);
-- line 619 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_chacha-0.3.1/src/guts.rs
--------------------------------------------------------------------------------
Ir              

-- line 65 ----------------------------------------
     .               x.c = x.c.shuffle_lane_words2301();
     .               x.d = x.d.shuffle_lane_words3012();
     .               x
     .           }
     .           
     .           impl ChaCha {
     .               #[inline(always)]
     .               pub fn new(key: &[u8; 32], nonce: &[u8]) -> Self {
    65 ( 0.00%)          init_chacha(key, nonce)
   286 ( 0.00%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ppv-lite86-0.2.17/src/x86_64/mod.rs:rand_chacha::guts::init_chacha (1x)
     .               }
     .           
     .               #[inline(always)]
     .               fn pos64<M: Machine>(&self, m: M) -> u64 {
 6,250 ( 0.00%)          let d: M::u32x4 = m.unpack(self.d);
     .                   ((d.extract(1) as u64) << 32) | d.extract(0) as u64
     .               }
     .           
     .               /// Produce 4 blocks of output, advancing the state
     .               #[inline(always)]
     .               pub fn refill4(&mut self, drounds: u32, out: &mut [u8; BUFSZ]) {
15,613 ( 0.00%)          refill_wide(self, drounds, out)
 7,272 ( 0.00%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/ppv-lite86-0.2.17/src/x86_64/mod.rs:rand_chacha::guts::refill_wide (12x)
     .               }
     .           
     .               #[inline(always)]
     .               pub fn set_block_pos(&mut self, value: u64) {
     .                   set_stream_param(self, STREAM_PARAM_BLOCK, value)
     .               }
     .           
     .               #[inline(always)]
-- line 93 ----------------------------------------
-- line 115 ----------------------------------------
     .           #[inline(always)]
     .           fn refill_wide_impl<Mach: Machine>(
     .               m: Mach, state: &mut ChaCha, drounds: u32, out: &mut [u8; BUFSZ],
     .           ) {
     .               let k = m.vec([0x6170_7865, 0x3320_646e, 0x7962_2d32, 0x6b20_6574]);
     .               let mut pos = state.pos64(m);
     .               let d0: Mach::u32x4 = m.unpack(state.d);
     .               pos = pos.wrapping_add(1);
 3,125 ( 0.00%)      let d1 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .               pos = pos.wrapping_add(1);
 3,125 ( 0.00%)      let d2 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .               pos = pos.wrapping_add(1);
 3,125 ( 0.00%)      let d3 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .           
     .               let b = m.unpack(state.b);
     .               let c = m.unpack(state.c);
     .               let mut x = State {
     .                   a: Mach::u32x4x4::from_lanes([k, k, k, k]),
     .                   b: Mach::u32x4x4::from_lanes([b, b, b, b]),
     .                   c: Mach::u32x4x4::from_lanes([c, c, c, c]),
     .                   d: m.unpack(Mach::u32x4x4::from_lanes([d0, d1, d2, d3]).into()),
-- line 135 ----------------------------------------
-- line 142 ----------------------------------------
     .               let d0: Mach::u32x4 = m.unpack(state.d);
     .               pos = pos.wrapping_add(1);
     .               let d1 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .               pos = pos.wrapping_add(1);
     .               let d2 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .               pos = pos.wrapping_add(1);
     .               let d3 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .               pos = pos.wrapping_add(1);
 6,250 ( 0.00%)      let d4 = d0.insert((pos >> 32) as u32, 1).insert(pos as u32, 0);
     .           
     .               let (a, b, c, d) = (
     .                   x.a.to_lanes(),
     .                   x.b.to_lanes(),
     .                   x.c.to_lanes(),
     .                   x.d.to_lanes(),
     .               );
     .               let sb = m.unpack(state.b);
     .               let sc = m.unpack(state.c);
     .               let sd = [m.unpack(state.d), d1, d2, d3];
 3,125 ( 0.00%)      state.d = d4.into();
     .               let mut words = out.chunks_exact_mut(16);
25,000 ( 0.01%)      for ((((&a, &b), &c), &d), &sd) in a.iter().zip(&b).zip(&c).zip(&d).zip(&sd) {
     .                   (a + k).write_le(words.next().unwrap());
     .                   (b + sb).write_le(words.next().unwrap());
     .                   (c + sc).write_le(words.next().unwrap());
     .                   (d + sd).write_le(words.next().unwrap());
     .               }
     .           }
     .           
     .           dispatch!(m, Mach, {
-- line 171 ----------------------------------------
-- line 228 ----------------------------------------
     .           
     .           fn read_u32le(xs: &[u8]) -> u32 {
     .               assert_eq!(xs.len(), 4);
     .               u32::from(xs[0]) | (u32::from(xs[1]) << 8) | (u32::from(xs[2]) << 16) | (u32::from(xs[3]) << 24)
     .           }
     .           
     .           dispatch_light128!(m, Mach, {
     .               fn init_chacha(key: &[u8; 32], nonce: &[u8]) -> ChaCha {
    65 ( 0.00%)          let ctr_nonce = [
     .                       0,
    26 ( 0.00%)              if nonce.len() == 12 {
     .                           read_u32le(&nonce[0..4])
     .                       } else {
     .                           0
     .                       },
    26 ( 0.00%)              read_u32le(&nonce[nonce.len() - 8..nonce.len() - 4]),
     .                       read_u32le(&nonce[nonce.len() - 4..]),
     .                   ];
     .                   let key0: Mach::u32x4 = m.read_le(&key[..16]);
     .                   let key1: Mach::u32x4 = m.read_le(&key[16..]);
    26 ( 0.00%)          ChaCha {
     .                       b: key0.into(),
     .                       c: key1.into(),
     .                       d: ctr_nonce.into(),
     .                   }
     .               }
     .           });
     .           
     .           dispatch_light128!(m, Mach, {
-- line 256 ----------------------------------------

     1 ( 0.00%)  <counts for unidentified lines in /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_chacha-0.3.1/src/guts.rs>

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.8.5/src/distributions/utils.rs
--------------------------------------------------------------------------------
Ir               

-- line 351 ----------------------------------------
      .                           i as $ty
      .                       }
      .                   }
      .           
      .                   impl FloatAsSIMD for $ty {}
      .               };
      .           }
      .           
500,000 ( 0.14%)  scalar_float_impl!(f32, u32);
      .           scalar_float_impl!(f64, u64);
      .           
      .           
      .           #[cfg(feature = "simd_support")]
      .           macro_rules! simd_impl {
      .               ($ty:ident, $f_scalar:ident, $mty:ident, $uty:ident) => {
      .                   impl FloatSIMDUtils for $ty {
      .                       type Mask = $mty;
-- line 367 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c
--------------------------------------------------------------------------------
Ir              

-- line 15 ----------------------------------------
     .               bool *deferred_work_generated);
     .           static uint64_t pac_time_until_deferred_work(tsdn_t *tsdn, pai_t *self);
     .           
     .           static inline void
     .           pac_decay_data_get(pac_t *pac, extent_state_t state,
     .               decay_t **r_decay, pac_decay_stats_t **r_decay_stats, ecache_t **r_ecache) {
     .           	switch(state) {
     .           	case extent_state_dirty:
    32 ( 0.00%)  		*r_decay = &pac->decay_dirty;
     .           		*r_decay_stats = &pac->stats->decay_dirty;
     .           		*r_ecache = &pac->ecache_dirty;
     .           		return;
     .           	case extent_state_muzzy:
     .           		*r_decay = &pac->decay_muzzy;
     .           		*r_decay_stats = &pac->stats->decay_muzzy;
     .           		*r_ecache = &pac->ecache_muzzy;
     .           		return;
-- line 31 ----------------------------------------
-- line 33 ----------------------------------------
     .           		unreachable();
     .           	}
     .           }
     .           
     .           bool
     .           pac_init(tsdn_t *tsdn, pac_t *pac, base_t *base, emap_t *emap,
     .               edata_cache_t *edata_cache, nstime_t *cur_time,
     .               size_t pac_oversize_threshold, ssize_t dirty_decay_ms,
    20 ( 0.00%)      ssize_t muzzy_decay_ms, pac_stats_t *pac_stats, malloc_mutex_t *stats_mtx) {
     .           	unsigned ind = base_ind_get(base);
     .           	/*
     .           	 * Delay coalescing for dirty extents despite the disruptive effect on
     .           	 * memory layout for best-fit extent allocation, since cached extents
     .           	 * are likely to be reused soon after deallocation, and the cost of
     .           	 * merging/splitting extents is non-trivial.
     .           	 */
    13 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_dirty, extent_state_dirty, ind,
 3,630 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
     .           	    /* delay_coalesce */ true)) {
     .           		return true;
     .           	}
     .           	/*
     .           	 * Coalesce muzzy extents immediately, because operations on them are in
     .           	 * the critical path much less often than for dirty extents.
     .           	 */
     8 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_muzzy, extent_state_muzzy, ind,
 3,630 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
     .           	    /* delay_coalesce */ false)) {
     .           		return true;
     .           	}
     .           	/*
     .           	 * Coalesce retained extents immediately, in part because they will
     .           	 * never be evicted (and therefore there's no opportunity for delayed
     .           	 * coalescing), but also because operations on retained extents are not
     .           	 * in the critical path.
     .           	 */
     8 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_retained, extent_state_retained,
 3,630 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
     .           	    ind, /* delay_coalesce */ false)) {
     .           		return true;
     .           	}
     2 ( 0.00%)  	exp_grow_init(&pac->exp_grow);
     4 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/exp_grow.c:_rjem_je_exp_grow_init (1x)
     7 ( 0.00%)  	if (malloc_mutex_init(&pac->grow_mtx, "extent_grow",
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           	    WITNESS_RANK_EXTENT_GROW, malloc_mutex_rank_exclusive)) {
     .           		return true;
     .           	}
     .           	atomic_store_zu(&pac->oversize_threshold, pac_oversize_threshold,
     .           	    ATOMIC_RELAXED);
     6 ( 0.00%)  	if (decay_init(&pac->decay_dirty, cur_time, dirty_decay_ms)) {
   483 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/decay.c:_rjem_je_decay_init (1x)
     .           		return true;
     .           	}
     6 ( 0.00%)  	if (decay_init(&pac->decay_muzzy, cur_time, muzzy_decay_ms)) {
   423 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/decay.c:_rjem_je_decay_init (1x)
     .           		return true;
     .           	}
     .           	if (san_bump_alloc_init(&pac->sba)) {
     .           		return true;
     .           	}
     .           
     2 ( 0.00%)  	pac->base = base;
     .           	pac->emap = emap;
     1 ( 0.00%)  	pac->edata_cache = edata_cache;
     .           	pac->stats = pac_stats;
     2 ( 0.00%)  	pac->stats_mtx = stats_mtx;
     .           	atomic_store_zu(&pac->extent_sn_next, 0, ATOMIC_RELAXED);
     .           
    10 ( 0.00%)  	pac->pai.alloc = &pac_alloc_impl;
     .           	pac->pai.alloc_batch = &pai_alloc_batch_default;
     .           	pac->pai.expand = &pac_expand_impl;
     .           	pac->pai.shrink = &pac_shrink_impl;
     .           	pac->pai.dalloc = &pac_dalloc_impl;
     .           	pac->pai.dalloc_batch = &pai_dalloc_batch_default;
     2 ( 0.00%)  	pac->pai.time_until_deferred_work = &pac_time_until_deferred_work;
     .           
     1 ( 0.00%)  	return false;
     8 ( 0.00%)  }
     .           
     .           static inline bool
     .           pac_may_have_muzzy(pac_t *pac) {
     .           	return pac_decay_ms_get(pac, extent_state_muzzy) != 0;
     .           }
     .           
     .           static edata_t *
     .           pac_alloc_real(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, size_t size,
14,016 ( 0.00%)      size_t alignment, bool zero, bool guarded) {
     .           	assert(!guarded || alignment <= PAGE);
     .           
12,848 ( 0.00%)  	edata_t *edata = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_dirty,
1,606,033 ( 0.44%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_alloc (1,168x)
     .           	    NULL, size, alignment, zero, guarded);
     .           
 6,112 ( 0.00%)  	if (edata == NULL && pac_may_have_muzzy(pac)) {
     .           		edata = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_muzzy,
     .           		    NULL, size, alignment, zero, guarded);
     .           	}
     .           	if (edata == NULL) {
 1,360 ( 0.00%)  		edata = ecache_alloc_grow(tsdn, pac, ehooks,
269,214 ( 0.07%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_alloc_grow (136x)
     .           		    &pac->ecache_retained, NULL, size, alignment, zero,
     .           		    guarded);
   408 ( 0.00%)  		if (config_stats && edata != NULL) {
     .           			atomic_fetch_add_zu(&pac->stats->pac_mapped, size,
     .           			    ATOMIC_RELAXED);
     .           		}
     .           	}
     .           
     .           	return edata;
 9,344 ( 0.00%)  }
     .           
     .           static edata_t *
     .           pac_alloc_new_guarded(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, size_t size,
     .               size_t alignment, bool zero, bool frequent_reuse) {
     .           	assert(alignment <= PAGE);
     .           
     .           	edata_t *edata;
     .           	if (san_bump_enabled() && frequent_reuse) {
-- line 142 ----------------------------------------
-- line 158 ----------------------------------------
     .           	    edata_size_get(edata) == size));
     .           
     .           	return edata;
     .           }
     .           
     .           static edata_t *
     .           pac_alloc_impl(tsdn_t *tsdn, pai_t *self, size_t size, size_t alignment,
     .               bool zero, bool guarded, bool frequent_reuse,
19,856 ( 0.01%)      bool *deferred_work_generated) {
     .           	pac_t *pac = (pac_t *)self;
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
     .           	edata_t *edata = NULL;
     .           	/*
     .           	 * The condition is an optimization - not frequently reused guarded
     .           	 * allocations are never put in the ecache.  pac_alloc_real also
     .           	 * doesn't grow retained for guarded allocations.  So pac_alloc_real
     .           	 * for such allocations would always return NULL.
     .           	 * */
 3,504 ( 0.00%)  	if (!guarded || frequent_reuse) {
12,848 ( 0.00%)  		edata =	pac_alloc_real(tsdn, pac, ehooks, size, alignment,
1,919,743 ( 0.53%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:pac_alloc_real (1,168x)
     .           		    zero, guarded);
     .           	}
 4,672 ( 0.00%)  	if (edata == NULL && guarded) {
     .           		/* No cached guarded extents; creating a new one. */
     .           		edata = pac_alloc_new_guarded(tsdn, pac, ehooks, size,
     .           		    alignment, zero, frequent_reuse);
     .           	}
     .           
     .           	return edata;
 9,344 ( 0.00%)  }
     .           
     .           static bool
     .           pac_expand_impl(tsdn_t *tsdn, pai_t *self, edata_t *edata, size_t old_size,
 6,195 ( 0.00%)      size_t new_size, bool zero, bool *deferred_work_generated) {
     .           	pac_t *pac = (pac_t *)self;
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
     .           	size_t mapped_add = 0;
   413 ( 0.00%)  	size_t expand_amount = new_size - old_size;
     .           
   826 ( 0.00%)  	if (ehooks_merge_will_fail(ehooks)) {
     .           		return true;
     .           	}
 5,782 ( 0.00%)  	edata_t *trail = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_dirty,
300,347 ( 0.08%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_alloc (413x)
     .           	    edata, expand_amount, PAGE, zero, /* guarded*/ false);
 2,065 ( 0.00%)  	if (trail == NULL) {
 3,510 ( 0.00%)  		trail = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_muzzy,
63,452 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_alloc (270x)
     .           		    edata, expand_amount, PAGE, zero, /* guarded*/ false);
     .           	}
 1,080 ( 0.00%)  	if (trail == NULL) {
 3,510 ( 0.00%)  		trail = ecache_alloc_grow(tsdn, pac, ehooks,
113,668 ( 0.03%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_alloc_grow (270x)
     .           		    &pac->ecache_retained, edata, expand_amount, PAGE, zero,
     .           		    /* guarded */ false);
     .           		mapped_add = expand_amount;
     .           	}
 1,080 ( 0.00%)  	if (trail == NULL) {
     .           		return true;
     .           	}
 1,240 ( 0.00%)  	if (extent_merge_wrapper(tsdn, pac, ehooks, edata, trail)) {
60,565 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_merge_wrapper (155x)
     .           		extent_dalloc_wrapper(tsdn, pac, ehooks, trail);
   258 ( 0.00%)  		return true;
     .           	}
    24 ( 0.00%)  	if (config_stats && mapped_add > 0) {
     .           		atomic_fetch_add_zu(&pac->stats->pac_mapped, mapped_add,
     .           		    ATOMIC_RELAXED);
     .           	}
   143 ( 0.00%)  	return false;
 3,304 ( 0.00%)  }
     .           
     .           static bool
     .           pac_shrink_impl(tsdn_t *tsdn, pai_t *self, edata_t *edata, size_t old_size,
     .               size_t new_size, bool *deferred_work_generated) {
     .           	pac_t *pac = (pac_t *)self;
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
     .           	size_t shrink_amount = old_size - new_size;
-- line 234 ----------------------------------------
-- line 244 ----------------------------------------
     .           	}
     .           	ecache_dalloc(tsdn, pac, ehooks, &pac->ecache_dirty, trail);
     .           	*deferred_work_generated = true;
     .           	return false;
     .           }
     .           
     .           static void
     .           pac_dalloc_impl(tsdn_t *tsdn, pai_t *self, edata_t *edata,
14,430 ( 0.00%)      bool *deferred_work_generated) {
     .           	pac_t *pac = (pac_t *)self;
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
 2,220 ( 0.00%)  	if (edata_guarded_get(edata)) {
     .           		/*
     .           		 * Because cached guarded extents do exact fit only, large
     .           		 * guarded extents are restored on dalloc eagerly (otherwise
     .           		 * they will not be reused efficiently).  Slab sizes have a
     .           		 * limited number of size classes, and tend to cycle faster.
     .           		 *
     .           		 * In the case where coalesce is restrained (VirtualFree on
     .           		 * Windows), guarded extents are also not cached -- otherwise
-- line 264 ----------------------------------------
-- line 269 ----------------------------------------
     .           		if (!edata_slab_get(edata) || !maps_coalesce) {
     .           			assert(edata_size_get(edata) >= SC_LARGE_MINCLASS ||
     .           			    !maps_coalesce);
     .           			san_unguard_pages_two_sided(tsdn, ehooks, edata,
     .           			    pac->emap);
     .           		}
     .           	}
     .           
 6,660 ( 0.00%)  	ecache_dalloc(tsdn, pac, ehooks, &pac->ecache_dirty, edata);
1,343,878 ( 0.37%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_dalloc (1,110x)
     .           	/* Purging of deallocated pages is deferred */
 1,110 ( 0.00%)  	*deferred_work_generated = true;
 8,880 ( 0.00%)  }
     .           
     .           static inline uint64_t
     .           pac_ns_until_purge(tsdn_t *tsdn, decay_t *decay, size_t npages) {
     .           	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
     .           		/* Use minimal interval if decay is contended. */
     .           		return BACKGROUND_THREAD_DEFERRED_MIN;
     .           	}
     .           	uint64_t result = decay_ns_until_purge(decay, npages,
-- line 288 ----------------------------------------
-- line 342 ----------------------------------------
     .               size_t npages_limit, size_t npages_decay_max,
     .               edata_list_inactive_t *result) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
     .           	/* Stash extents according to npages_limit. */
     .           	size_t nstashed = 0;
    39 ( 0.00%)  	while (nstashed < npages_decay_max) {
    24 ( 0.00%)  		edata_t *edata = ecache_evict(tsdn, pac, ehooks, ecache,
33,337 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_ecache_evict (3x)
     .           		    npages_limit);
     6 ( 0.00%)  		if (edata == NULL) {
     .           			break;
     .           		}
     .           		edata_list_inactive_append(result, edata);
     9 ( 0.00%)  		nstashed += edata_size_get(edata) >> LG_PAGE;
     .           	}
     .           	return nstashed;
     .           }
     .           
     .           static size_t
     .           pac_decay_stashed(tsdn_t *tsdn, pac_t *pac, decay_t *decay,
     .               pac_decay_stats_t *decay_stats, ecache_t *ecache, bool fully_decay,
     .               edata_list_inactive_t *decay_extents) {
     .           	bool err;
     .           
     6 ( 0.00%)  	size_t nmadvise = 0;
     6 ( 0.00%)  	size_t nunmapped = 0;
     3 ( 0.00%)  	size_t npurged = 0;
     .           
     .           	ehooks_t *ehooks = pac_ehooks_get(pac);
     .           
     .           	bool try_muzzy = !fully_decay
    24 ( 0.00%)  	    && pac_decay_ms_get(pac, extent_state_muzzy) != 0;
     .           
    15 ( 0.00%)  	for (edata_t *edata = edata_list_inactive_first(decay_extents); edata !=
     .           	    NULL; edata = edata_list_inactive_first(decay_extents)) {
     .           		edata_list_inactive_remove(decay_extents, edata);
     .           
     .           		size_t size = edata_size_get(edata);
     3 ( 0.00%)  		size_t npages = size >> LG_PAGE;
     .           
     3 ( 0.00%)  		nmadvise++;
     3 ( 0.00%)  		npurged += npages;
     .           
     9 ( 0.00%)  		switch (ecache->state) {
     .           		case extent_state_active:
     .           			not_reached();
     .           		case extent_state_dirty:
     9 ( 0.00%)  			if (try_muzzy) {
     .           				err = extent_purge_lazy_wrapper(tsdn, ehooks,
     .           				    edata, /* offset */ 0, size);
     .           				if (!err) {
     .           					ecache_dalloc(tsdn, pac, ehooks,
     .           					    &pac->ecache_muzzy, edata);
     9 ( 0.00%)  					break;
     .           				}
     .           			}
     .           			JEMALLOC_FALLTHROUGH;
     .           		case extent_state_muzzy:
    15 ( 0.00%)  			extent_dalloc_wrapper(tsdn, pac, ehooks, edata);
 3,082 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_dalloc_wrapper (3x)
     3 ( 0.00%)  			nunmapped += npages;
     .           			break;
     .           		case extent_state_retained:
     .           		default:
     .           			not_reached();
     .           		}
     .           	}
     .           
     .           	if (config_stats) {
-- line 411 ----------------------------------------
-- line 412 ----------------------------------------
     .           		LOCKEDINT_MTX_LOCK(tsdn, *pac->stats_mtx);
     .           		locked_inc_u64(tsdn, LOCKEDINT_MTX(*pac->stats_mtx),
     .           		    &decay_stats->npurge, 1);
     .           		locked_inc_u64(tsdn, LOCKEDINT_MTX(*pac->stats_mtx),
     .           		    &decay_stats->nmadvise, nmadvise);
     .           		locked_inc_u64(tsdn, LOCKEDINT_MTX(*pac->stats_mtx),
     .           		    &decay_stats->purged, npurged);
     .           		LOCKEDINT_MTX_UNLOCK(tsdn, *pac->stats_mtx);
     6 ( 0.00%)  		atomic_fetch_sub_zu(&pac->stats->pac_mapped,
     .           		    nunmapped << LG_PAGE, ATOMIC_RELAXED);
     .           	}
     .           
     .           	return npurged;
     .           }
     .           
     .           /*
     .            * npages_limit: Decay at most npages_decay_max pages without violating the
     .            * invariant: (ecache_npages_get(ecache) >= npages_limit).  We need an upper
     .            * bound on number of pages in order to prevent unbounded growth (namely in
     .            * stashed), otherwise unbounded new pages could be added to extents during the
     .            * current decay run, so that the purging thread never finishes.
     .            */
     .           static void
    42 ( 0.00%)  pac_decay_to_limit(tsdn_t *tsdn, pac_t *pac, decay_t *decay,
     .               pac_decay_stats_t *decay_stats, ecache_t *ecache, bool fully_decay,
     .               size_t npages_limit, size_t npages_decay_max) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 1);
     .           
     6 ( 0.00%)  	if (decay->purging || npages_decay_max == 0) {
     .           		return;
     .           	}
     3 ( 0.00%)  	decay->purging = true;
     .           	malloc_mutex_unlock(tsdn, &decay->mtx);
     .           
     .           	edata_list_inactive_t decay_extents;
     .           	edata_list_inactive_init(&decay_extents);
     .           	size_t npurge = pac_stash_decayed(tsdn, pac, ecache, npages_limit,
     .           	    npages_decay_max, &decay_extents);
     3 ( 0.00%)  	if (npurge != 0) {
     .           		size_t npurged = pac_decay_stashed(tsdn, pac, decay,
     .           		    decay_stats, ecache, fully_decay, &decay_extents);
     .           		assert(npurged == npurge);
     .           	}
     .           
     .           	malloc_mutex_lock(tsdn, &decay->mtx);
     6 ( 0.00%)  	decay->purging = false;
    24 ( 0.00%)  }
     .           
     .           void
     .           pac_decay_all(tsdn_t *tsdn, pac_t *pac, decay_t *decay,
     .               pac_decay_stats_t *decay_stats, ecache_t *ecache, bool fully_decay) {
     .           	malloc_mutex_assert_owner(tsdn, &decay->mtx);
     .           	pac_decay_to_limit(tsdn, pac, decay, decay_stats, ecache, fully_decay,
     .           	    /* npages_limit */ 0, ecache_npages_get(ecache));
     .           }
     .           
     .           static void
     .           pac_decay_try_purge(tsdn_t *tsdn, pac_t *pac, decay_t *decay,
     .               pac_decay_stats_t *decay_stats, ecache_t *ecache,
     .               size_t current_npages, size_t npages_limit) {
    10 ( 0.00%)  	if (current_npages > npages_limit) {
    42 ( 0.00%)  		pac_decay_to_limit(tsdn, pac, decay, decay_stats, ecache,
37,004 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:pac_decay_to_limit.part.0 (3x)
     .           		    /* fully_decay */ false, npages_limit,
     .           		    current_npages - npages_limit);
     .           	}
     .           }
     .           
     .           bool
     .           pac_maybe_decay_purge(tsdn_t *tsdn, pac_t *pac, decay_t *decay,
     .               pac_decay_stats_t *decay_stats, ecache_t *ecache,
   112 ( 0.00%)      pac_purge_eagerness_t eagerness) {
     .           	malloc_mutex_assert_owner(tsdn, &decay->mtx);
     .           
     .           	/* Purge all or nothing if the option is disabled. */
     .           	ssize_t decay_ms = decay_ms_read(decay);
    16 ( 0.00%)  	if (decay_ms <= 0) {
     .           		if (decay_ms == 0) {
     .           			pac_decay_to_limit(tsdn, pac, decay, decay_stats,
     .           			    ecache, /* fully_decay */ false,
     .           			    /* npages_limit */ 0, ecache_npages_get(ecache));
     .           		}
     .           		return false;
     .           	}
     .           
-- line 496 ----------------------------------------
-- line 497 ----------------------------------------
     .           	/*
     .           	 * If the deadline has been reached, advance to the current epoch and
     .           	 * purge to the new limit if necessary.  Note that dirty pages created
     .           	 * during the current epoch are not subject to purge until a future
     .           	 * epoch, so as a result purging only happens during epoch advances, or
     .           	 * being triggered by background threads (scheduled event).
     .           	 */
     .           	nstime_t time;
    40 ( 0.00%)  	nstime_init_update(&time);
   272 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/nstime.c:_rjem_je_nstime_init_update (8x)
     .           	size_t npages_current = ecache_npages_get(ecache);
    32 ( 0.00%)  	bool epoch_advanced = decay_maybe_advance_epoch(decay, &time,
 5,556 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/decay.c:_rjem_je_decay_maybe_advance_epoch (8x)
     .           	    npages_current);
    24 ( 0.00%)  	if (eagerness == PAC_PURGE_ALWAYS
    35 ( 0.00%)  	    || (epoch_advanced && eagerness == PAC_PURGE_ON_EPOCH_ADVANCE)) {
     .           		size_t npages_limit = decay_npages_limit_get(decay);
     .           		pac_decay_try_purge(tsdn, pac, decay, decay_stats, ecache,
     .           		    npages_current, npages_limit);
     .           	}
     .           
     .           	return epoch_advanced;
    88 ( 0.00%)  }
     .           
     .           bool
     .           pac_decay_ms_set(tsdn_t *tsdn, pac_t *pac, extent_state_t state,
     .               ssize_t decay_ms, pac_purge_eagerness_t eagerness) {
     .           	decay_t *decay;
     .           	pac_decay_stats_t *decay_stats;
     .           	ecache_t *ecache;
     .           	pac_decay_data_get(pac, state, &decay, &decay_stats, &ecache);
-- line 525 ----------------------------------------
-- line 542 ----------------------------------------
     .           	decay_reinit(decay, &cur_time, decay_ms);
     .           	pac_maybe_decay_purge(tsdn, pac, decay, decay_stats, ecache, eagerness);
     .           	malloc_mutex_unlock(tsdn, &decay->mtx);
     .           
     .           	return false;
     .           }
     .           
     .           ssize_t
     8 ( 0.00%)  pac_decay_ms_get(pac_t *pac, extent_state_t state) {
     .           	decay_t *decay;
     .           	pac_decay_stats_t *decay_stats;
     .           	ecache_t *ecache;
     .           	pac_decay_data_get(pac, state, &decay, &decay_stats, &ecache);
     .           	return decay_ms_read(decay);
     8 ( 0.00%)  }
     .           
     .           void
     .           pac_reset(tsdn_t *tsdn, pac_t *pac) {
     .           	/*
     .           	 * No-op for now; purging is still done at the arena-level.  It should
     .           	 * get moved in here, though.
     .           	 */
     .           	(void)tsdn;
-- line 564 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/sz.h
--------------------------------------------------------------------------------
Ir                 

-- line 51 ----------------------------------------
        .            */
        .           extern size_t sz_large_pad;
        .           
        .           extern void sz_boot(const sc_data_t *sc_data, bool cache_oblivious);
        .           
        .           JEMALLOC_ALWAYS_INLINE pszind_t
        .           sz_psz2ind(size_t psz) {
        .           	assert(psz > 0);
   34,542 ( 0.01%)  	if (unlikely(psz > SC_LARGE_MAXCLASS)) {
        .           		return SC_NPSIZES;
        .           	}
        .           	/* x is the lg of the first base >= psz. */
        .           	pszind_t x = lg_ceil(psz);
        .           	/*
        .           	 * sc.h introduces a lot of size classes. These size classes are divided
        .           	 * into different size class groups. There is a very special size class
        .           	 * group, each size class in or after it is an integer multiple of PAGE.
        .           	 * We call it first_ps_rg. It means first page size regular group. The
        .           	 * range of first_ps_rg is (base, base * 2], and base == PAGE *
        .           	 * SC_NGROUP. off_to_first_ps_rg begins from 1, instead of 0. e.g.
        .           	 * off_to_first_ps_rg is 1 when psz is (PAGE * SC_NGROUP + 1).
        .           	 */
    5,756 ( 0.00%)  	pszind_t off_to_first_ps_rg = (x < SC_LG_NGROUP + LG_PAGE) ?
   34,542 ( 0.01%)  	    0 : x - (SC_LG_NGROUP + LG_PAGE);
        .           
        .           	/*
        .           	 * Same as sc_s::lg_delta.
        .           	 * Delta for off_to_first_ps_rg == 1 is PAGE,
        .           	 * for each increase in offset, it's multiplied by two.
        .           	 * Therefore, lg_delta = LG_PAGE + (off_to_first_ps_rg - 1).
        .           	 */
        .           	pszind_t lg_delta = (off_to_first_ps_rg == 0) ?
   39,264 ( 0.01%)  	    LG_PAGE : LG_PAGE + (off_to_first_ps_rg - 1);
        .           
        .           	/*
        .           	 * Let's write psz in binary, e.g. 0011 for 0x3, 0111 for 0x7.
        .           	 * The leftmost bits whose len is lg_base decide the base of psz.
        .           	 * The rightmost bits whose len is lg_delta decide (pgz % PAGE).
        .           	 * The middle bits whose len is SC_LG_NGROUP decide ndelta.
        .           	 * ndelta is offset to the first size class in the size class group,
        .           	 * starts from 1.
-- line 91 ----------------------------------------
-- line 92 ----------------------------------------
        .           	 * If you don't know lg_base, ndelta or lg_delta, see sc.h.
        .           	 * |xxxxxxxxxxxxxxxxxxxx|------------------------|yyyyyyyyyyyyyyyyyyyyy|
        .           	 * |<-- len: lg_base -->|<-- len: SC_LG_NGROUP-->|<-- len: lg_delta -->|
        .           	 *                      |<--      ndelta      -->|
        .           	 * rg_inner_off = ndelta - 1
        .           	 * Why use (psz - 1)?
        .           	 * To handle case: psz % (1 << lg_delta) == 0.
        .           	 */
   21,992 ( 0.01%)  	pszind_t rg_inner_off = (((psz - 1)) >> lg_delta) & (SC_NGROUP - 1);
        .           
    5,756 ( 0.00%)  	pszind_t base_ind = off_to_first_ps_rg << SC_LG_NGROUP;
   20,418 ( 0.01%)  	pszind_t ind = base_ind + rg_inner_off;
   15,599 ( 0.00%)  	return ind;
        .           }
        .           
        .           static inline size_t
        .           sz_pind2sz_compute(pszind_t pind) {
        .           	if (unlikely(pind == SC_NPSIZES)) {
        .           		return SC_LARGE_MAXCLASS + PAGE;
        .           	}
        .           	size_t grp = pind >> SC_LG_NGROUP;
-- line 112 ----------------------------------------
-- line 121 ----------------------------------------
        .           	size_t mod_size = (mod+1) << lg_delta;
        .           
        .           	size_t sz = grp_size + mod_size;
        .           	return sz;
        .           }
        .           
        .           static inline size_t
        .           sz_pind2sz_lookup(pszind_t pind) {
    6,969 ( 0.00%)  	size_t ret = (size_t)sz_pind2sz_tab[pind];
        .           	assert(ret == sz_pind2sz_compute(pind));
        .           	return ret;
        .           }
        .           
        .           static inline size_t
        .           sz_pind2sz(pszind_t pind) {
        .           	assert(pind < SC_NPSIZES + 1);
        .           	return sz_pind2sz_lookup(pind);
        .           }
        .           
        .           static inline size_t
        .           sz_psz2u(size_t psz) {
        8 ( 0.00%)  	if (unlikely(psz > SC_LARGE_MAXCLASS)) {
        .           		return SC_LARGE_MAXCLASS + PAGE;
        .           	}
        4 ( 0.00%)  	size_t x = lg_floor((psz<<1)-1);
        2 ( 0.00%)  	size_t lg_delta = (x < SC_LG_NGROUP + LG_PAGE + 1) ?
        6 ( 0.00%)  	    LG_PAGE : x - SC_LG_NGROUP - 1;
        6 ( 0.00%)  	size_t delta = ZU(1) << lg_delta;
        .           	size_t delta_mask = delta - 1;
        6 ( 0.00%)  	size_t usize = (psz + delta_mask) & ~delta_mask;
        .           	return usize;
        .           }
        .           
        .           static inline szind_t
        .           sz_size2index_compute(size_t size) {
   21,222 ( 0.01%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
        .           		return SC_NSIZES;
        .           	}
        .           
        .           	if (size == 0) {
        .           		return 0;
        .           	}
        .           #if (SC_NTINY != 0)
        .           	if (size <= (ZU(1) << SC_LG_TINY_MAXCLASS)) {
        .           		szind_t lg_tmin = SC_LG_TINY_MAXCLASS - SC_NTINY + 1;
        .           		szind_t lg_ceil = lg_floor(pow2_ceil_zu(size));
        .           		return (lg_ceil < lg_tmin ? 0 : lg_ceil - lg_tmin);
        .           	}
        .           #endif
        .           	{
    7,426 ( 0.00%)  		szind_t x = lg_floor((size<<1)-1);
   22,398 ( 0.01%)  		szind_t shift = (x < SC_LG_NGROUP + LG_QUANTUM) ? 0 :
        .           		    x - (SC_LG_NGROUP + LG_QUANTUM);
        .           		szind_t grp = shift << SC_LG_NGROUP;
        .           
    7,426 ( 0.00%)  		szind_t lg_delta = (x < SC_LG_NGROUP + LG_QUANTUM + 1)
   21,108 ( 0.01%)  		    ? LG_QUANTUM : x - SC_LG_NGROUP - 1;
        .           
   16,479 ( 0.00%)  		size_t delta_inverse_mask = ZU(-1) << lg_delta;
   29,547 ( 0.01%)  		szind_t mod = ((((size-1) & delta_inverse_mask) >> lg_delta)) &
        .           		    ((ZU(1) << SC_LG_NGROUP) - 1);
        .           
    7,852 ( 0.00%)  		szind_t index = SC_NTINY + grp + mod;
    4,406 ( 0.00%)  		return index;
        .           	}
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE szind_t
        .           sz_size2index_lookup_impl(size_t size) {
        .           	assert(size <= SC_LOOKUP_MAXCLASS);
  544,154 ( 0.15%)  	return sz_size2index_tab[(size + (ZU(1) << SC_LG_TINY_MIN) - 1)
  519,177 ( 0.14%)  	    >> SC_LG_TINY_MIN];
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE szind_t
        .           sz_size2index_lookup(size_t size) {
        .           	szind_t ret = sz_size2index_lookup_impl(size);
        .           	assert(ret == sz_size2index_compute(size));
        .           	return ret;
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE szind_t
        .           sz_size2index(size_t size) {
   37,012 ( 0.01%)  	if (likely(size <= SC_LOOKUP_MAXCLASS)) {
        .           		return sz_size2index_lookup(size);
        .           	}
        .           	return sz_size2index_compute(size);
        .           }
        .           
        .           static inline size_t
        .           sz_index2size_compute(szind_t index) {
        .           #if (SC_NTINY > 0)
-- line 212 ----------------------------------------
-- line 230 ----------------------------------------
        .           
        .           		size_t usize = grp_size + mod_size;
        .           		return usize;
        .           	}
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_index2size_lookup_impl(szind_t index) {
1,061,954 ( 0.29%)  	return sz_index2size_tab[index];
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_index2size_lookup(szind_t index) {
        .           	size_t ret = sz_index2size_lookup_impl(index);
        .           	assert(ret == sz_index2size_compute(index));
        .           	return ret;
        .           }
-- line 246 ----------------------------------------
-- line 254 ----------------------------------------
        .           JEMALLOC_ALWAYS_INLINE void
        .           sz_size2index_usize_fastpath(size_t size, szind_t *ind, size_t *usize) {
        .           	*ind = sz_size2index_lookup_impl(size);
        .           	*usize = sz_index2size_lookup_impl(*ind);
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_s2u_compute(size_t size) {
   13,551 ( 0.00%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
        .           		return 0;
        .           	}
        .           
        .           	if (size == 0) {
        .           		size++;
        .           	}
        .           #if (SC_NTINY > 0)
        .           	if (size <= (ZU(1) << SC_LG_TINY_MAXCLASS)) {
        .           		size_t lg_tmin = SC_LG_TINY_MAXCLASS - SC_NTINY + 1;
        .           		size_t lg_ceil = lg_floor(pow2_ceil_zu(size));
        .           		return (lg_ceil < lg_tmin ? (ZU(1) << lg_tmin) :
        .           		    (ZU(1) << lg_ceil));
        .           	}
        .           #endif
        .           	{
   10,392 ( 0.00%)  		size_t x = lg_floor((size<<1)-1);
    5,203 ( 0.00%)  		size_t lg_delta = (x < SC_LG_NGROUP + LG_QUANTUM + 1)
   15,678 ( 0.00%)  		    ?  LG_QUANTUM : x - SC_LG_NGROUP - 1;
    9,284 ( 0.00%)  		size_t delta = ZU(1) << lg_delta;
        .           		size_t delta_mask = delta - 1;
   15,609 ( 0.00%)  		size_t usize = (size + delta_mask) & ~delta_mask;
    4,678 ( 0.00%)  		return usize;
        .           	}
        .           }
        .           
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_s2u_lookup(size_t size) {
        .           	size_t ret = sz_index2size_lookup(sz_size2index_lookup(size));
        .           
        .           	assert(ret == sz_s2u_compute(size));
-- line 292 ----------------------------------------
-- line 294 ----------------------------------------
        .           }
        .           
        .           /*
        .            * Compute usable size that would result from allocating an object with the
        .            * specified size.
        .            */
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_s2u(size_t size) {
   31,214 ( 0.01%)  	if (likely(size <= SC_LOOKUP_MAXCLASS)) {
        .           		return sz_s2u_lookup(size);
        .           	}
        .           	return sz_s2u_compute(size);
        .           }
        .           
        .           /*
        .            * Compute usable size that would result from allocating an object with the
        .            * specified size and alignment.
-- line 310 ----------------------------------------
-- line 311 ----------------------------------------
        .            */
        .           JEMALLOC_ALWAYS_INLINE size_t
        .           sz_sa2u(size_t size, size_t alignment) {
        .           	size_t usize;
        .           
        .           	assert(alignment != 0 && ((alignment - 1) & alignment) == 0);
        .           
        .           	/* Try for a small size class. */
    3,278 ( 0.00%)  	if (size <= SC_SMALL_MAXCLASS && alignment <= PAGE) {
        .           		/*
        .           		 * Round size up to the nearest multiple of alignment.
        .           		 *
        .           		 * This done, we can take advantage of the fact that for each
        .           		 * small size class, every object is aligned at the smallest
        .           		 * power of two that is non-zero in the base two representation
        .           		 * of the size.  For example:
        .           		 *
-- line 327 ----------------------------------------
-- line 334 ----------------------------------------
        .           		usize = sz_s2u(ALIGNMENT_CEILING(size, alignment));
        .           		if (usize < SC_LARGE_MINCLASS) {
        .           			return usize;
        .           		}
        .           	}
        .           
        .           	/* Large size class.  Beware of overflow. */
        .           
    2,460 ( 0.00%)  	if (unlikely(alignment > SC_LARGE_MAXCLASS)) {
        .           		return 0;
        .           	}
        .           
        .           	/* Make sure result is a large size class. */
    1,640 ( 0.00%)  	if (size <= SC_LARGE_MINCLASS) {
        .           		usize = SC_LARGE_MINCLASS;
        .           	} else {
        .           		usize = sz_s2u(size);
    1,582 ( 0.00%)  		if (usize < size) {
        .           			/* size_t overflow. */
        .           			return 0;
        .           		}
        .           	}
        .           
        .           	/*
        .           	 * Calculate the multi-page mapping that large_palloc() would need in
        .           	 * order to guarantee the alignment.
        .           	 */
    5,738 ( 0.00%)  	if (usize + sz_large_pad + PAGE_CEILING(alignment) - PAGE < usize) {
        .           		/* size_t overflow. */
        4 ( 0.00%)  		return 0;
        .           	}
        .           	return usize;
        .           }
        .           
        .           size_t sz_psz_quantize_floor(size_t size);
        .           size_t sz_psz_quantize_ceil(size_t size);
        .           
        .           #endif /* JEMALLOC_INTERNAL_SIZE_H */
-- line 371 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c
--------------------------------------------------------------------------------
Ir              

-- line 2 ----------------------------------------
     .           #include "jemalloc/internal/jemalloc_internal_includes.h"
     .           
     .           #include "jemalloc/internal/eset.h"
     .           
     .           #define ESET_NPSIZES (SC_NPSIZES + 1)
     .           
     .           static void
     .           eset_bin_init(eset_bin_t *bin) {
 1,350 ( 0.00%)  	edata_heap_new(&bin->heap);
 4,800 ( 0.00%)  => ???:_rjem_je_edata_heap_new (1,200x)
     .           	/*
     .           	 * heap_min doesn't need initialization; it gets filled in when the bin
     .           	 * goes from non-empty to empty.
     .           	 */
     .           }
     .           
     .           static void
     .           eset_bin_stats_init(eset_bin_stats_t *bin_stats) {
     .           	atomic_store_zu(&bin_stats->nextents, 0, ATOMIC_RELAXED);
     .           	atomic_store_zu(&bin_stats->nbytes, 0, ATOMIC_RELAXED);
     .           }
     .           
     .           void
    90 ( 0.00%)  eset_init(eset_t *eset, extent_state_t state) {
 1,650 ( 0.00%)  	for (unsigned i = 0; i < ESET_NPSIZES; i++) {
     .           		eset_bin_init(&eset->bins[i]);
     .           		eset_bin_stats_init(&eset->bin_stats[i]);
     .           	}
     .           	fb_init(eset->bitmap, ESET_NPSIZES);
     .           	edata_list_inactive_init(&eset->lru);
    12 ( 0.00%)  	eset->state = state;
    48 ( 0.00%)  }
     .           
     .           size_t
    80 ( 0.00%)  eset_npages_get(eset_t *eset) {
     .           	return atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
    80 ( 0.00%)  }
     .           
     .           size_t
     .           eset_nextents_get(eset_t *eset, pszind_t pind) {
     .           	return atomic_load_zu(&eset->bin_stats[pind].nextents, ATOMIC_RELAXED);
     .           }
     .           
     .           size_t
     .           eset_nbytes_get(eset_t *eset, pszind_t pind) {
     .           	return atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
     .           }
     .           
     .           static void
     .           eset_stats_add(eset_t *eset, pszind_t pind, size_t sz) {
     .           	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
     .           	    ATOMIC_RELAXED);
 2,243 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nextents, cur + 1,
     .           	    ATOMIC_RELAXED);
     .           	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
 2,243 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur + sz,
     .           	    ATOMIC_RELAXED);
     .           }
     .           
     .           static void
     .           eset_stats_sub(eset_t *eset, pszind_t pind, size_t sz) {
     .           	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
     .           	    ATOMIC_RELAXED);
 2,209 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nextents, cur - 1,
     .           	    ATOMIC_RELAXED);
     .           	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
 2,209 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur - sz,
     .           	    ATOMIC_RELAXED);
     .           }
     .           
     .           void
29,159 ( 0.01%)  eset_insert(eset_t *eset, edata_t *edata) {
     .           	assert(edata_state_get(edata) == eset->state);
     .           
     .           	size_t size = edata_size_get(edata);
 6,729 ( 0.00%)  	size_t psz = sz_psz_quantize_floor(size);
60,635 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor (2,243x)
     .           	pszind_t pind = sz_psz2ind(psz);
     .           
     .           	edata_cmp_summary_t edata_cmp_summary = edata_cmp_summary_get(edata);
26,916 ( 0.01%)  	if (edata_heap_empty(&eset->bins[pind].heap)) {
 8,972 ( 0.00%)  => ???:_rjem_je_edata_heap_empty (2,243x)
     .           		fb_set(eset->bitmap, ESET_NPSIZES, (size_t)pind);
     .           		/* Only element is automatically the min element. */
 7,480 ( 0.00%)  		eset->bins[pind].heap_min = edata_cmp_summary;
     .           	} else {
     .           		/*
     .           		 * There's already a min element; update the summary if we're
     .           		 * about to insert a lower one.
     .           		 */
 2,241 ( 0.00%)  		if (edata_cmp_summary_comp(edata_cmp_summary,
     .           		    eset->bins[pind].heap_min) < 0) {
 1,488 ( 0.00%)  			eset->bins[pind].heap_min = edata_cmp_summary;
     .           		}
     .           	}
 4,486 ( 0.00%)  	edata_heap_insert(&eset->bins[pind].heap, edata);
76,550 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_insert (2,243x)
     .           
     .           	if (config_stats) {
     .           		eset_stats_add(eset, pind, size);
     .           	}
     .           
     .           	edata_list_inactive_append(&eset->lru, edata);
 2,243 ( 0.00%)  	size_t npages = size >> LG_PAGE;
     .           	/*
     .           	 * All modifications to npages hold the mutex (as asserted above), so we
     .           	 * don't need an atomic fetch-add; we can get by with a load followed by
     .           	 * a store.
     .           	 */
     .           	size_t cur_eset_npages =
     .           	    atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
 2,243 ( 0.00%)  	atomic_store_zu(&eset->npages, cur_eset_npages + npages,
     .           	    ATOMIC_RELAXED);
17,944 ( 0.00%)  }
     .           
     .           void
24,299 ( 0.01%)  eset_remove(eset_t *eset, edata_t *edata) {
     .           	assert(edata_state_get(edata) == eset->state ||
     .           	    edata_state_in_transition(edata_state_get(edata)));
     .           
     .           	size_t size = edata_size_get(edata);
 6,627 ( 0.00%)  	size_t psz = sz_psz_quantize_floor(size);
59,734 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor (2,209x)
     .           	pszind_t pind = sz_psz2ind(psz);
     .           	if (config_stats) {
     .           		eset_stats_sub(eset, pind, size);
     .           	}
     .           
     .           	edata_cmp_summary_t edata_cmp_summary = edata_cmp_summary_get(edata);
 8,836 ( 0.00%)  	edata_heap_remove(&eset->bins[pind].heap, edata);
76,767 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_remove (2,209x)
11,045 ( 0.00%)  	if (edata_heap_empty(&eset->bins[pind].heap)) {
 8,836 ( 0.00%)  => ???:_rjem_je_edata_heap_empty (2,209x)
     .           		fb_unset(eset->bitmap, ESET_NPSIZES, (size_t)pind);
     .           	} else {
     .           		/*
     .           		 * This is a little weird; we compare if the summaries are
     .           		 * equal, rather than if the edata we removed was the heap
     .           		 * minimum.  The reason why is that getting the heap minimum
     .           		 * can cause a pairing heap merge operation.  We can avoid this
     .           		 * if we only update the min if it's changed, in which case the
     .           		 * summaries of the removed element and the min element should
     .           		 * compare equal.
     .           		 */
 2,196 ( 0.00%)  		if (edata_cmp_summary_comp(edata_cmp_summary,
     .           		    eset->bins[pind].heap_min) == 0) {
 1,812 ( 0.00%)  			eset->bins[pind].heap_min = edata_cmp_summary_get(
   906 ( 0.00%)  			    edata_heap_first(&eset->bins[pind].heap));
11,355 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_first (453x)
     .           		}
     .           	}
     .           	edata_list_inactive_remove(&eset->lru, edata);
 2,209 ( 0.00%)  	size_t npages = size >> LG_PAGE;
     .           	/*
     .           	 * As in eset_insert, we hold eset->mtx and so don't need atomic
     .           	 * operations for updating eset->npages.
     .           	 */
     .           	size_t cur_extents_npages =
     .           	    atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
     .           	assert(cur_extents_npages >= npages);
 2,209 ( 0.00%)  	atomic_store_zu(&eset->npages,
     .           	    cur_extents_npages - (size >> LG_PAGE), ATOMIC_RELAXED);
17,672 ( 0.00%)  }
     .           
     .           /*
     .            * Find an extent with size [min_size, max_size) to satisfy the alignment
     .            * requirement.  For each size, try only the first extent in the heap.
     .            */
     .           static edata_t *
     .           eset_fit_alignment(eset_t *eset, size_t min_size, size_t max_size,
     .               size_t alignment) {
-- line 164 ----------------------------------------
-- line 200 ----------------------------------------
     .            * the returned size that we'll allow.  This can reduce fragmentation by
     .            * avoiding reusing and splitting large extents for smaller sizes.  In practice,
     .            * it's set to opt_lg_extent_max_active_fit for the dirty eset and SC_PTR_BITS
     .            * for others.
     .            */
     .           static edata_t *
     .           eset_first_fit(eset_t *eset, size_t size, bool exact_only,
     .               unsigned lg_max_fit) {
 1,179 ( 0.00%)  	edata_t *ret = NULL;
 2,358 ( 0.00%)  	edata_cmp_summary_t ret_summ JEMALLOC_CC_SILENCE_INIT({0});
     .           
 5,216 ( 0.00%)  	pszind_t pind = sz_psz2ind(sz_psz_quantize_ceil(size));
40,100 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c:_rjem_je_sz_psz_quantize_ceil (1,304x)
     .           
 2,608 ( 0.00%)  	if (exact_only) {
   250 ( 0.00%)  		return edata_heap_empty(&eset->bins[pind].heap) ? NULL :
     .           		    edata_heap_first(&eset->bins[pind].heap);
     .           	}
     .           
 1,179 ( 0.00%)  	for (pszind_t i =
     .           	    (pszind_t)fb_ffs(eset->bitmap, ESET_NPSIZES, (size_t)pind);
12,259 ( 0.00%)  	    i < ESET_NPSIZES;
 7,557 ( 0.00%)  	    i = (pszind_t)fb_ffs(eset->bitmap, ESET_NPSIZES, (size_t)i + 1)) {
     .           		assert(!edata_heap_empty(&eset->bins[i].heap));
     .           		if (lg_max_fit == SC_PTR_BITS) {
     .           			/*
     .           			 * We'll shift by this below, and shifting out all the
     .           			 * bits is undefined.  Decreasing is safe, since the
     .           			 * page size is larger than 1 byte.
     .           			 */
 5,934 ( 0.00%)  			lg_max_fit = SC_PTR_BITS - 1;
     .           		}
22,925 ( 0.01%)  		if ((sz_pind2sz(i) >> lg_max_fit) > size) {
     .           			break;
     .           		}
17,269 ( 0.00%)  		if (ret == NULL || edata_cmp_summary_comp(
     .           		    eset->bins[i].heap_min, ret_summ) < 0) {
     .           			/*
     .           			 * We grab the edata as early as possible, even though
     .           			 * we might change it later.  Practically, a large
     .           			 * portion of eset_fit calls succeed at the first valid
     .           			 * index, so this doesn't cost much, and we get the
     .           			 * effect of prefetching the edata as early as possible.
     .           			 */
10,577 ( 0.00%)  			edata_t *edata = edata_heap_first(&eset->bins[i].heap);
42,107 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_first (1,511x)
     .           			assert(edata_size_get(edata) >= size);
     .           			assert(ret == NULL || edata_snad_comp(edata, ret) < 0);
     .           			assert(ret == NULL || edata_cmp_summary_comp(
     .           			    eset->bins[i].heap_min,
     .           			    edata_cmp_summary_get(edata)) == 0);
     .           			ret = edata;
10,230 ( 0.00%)  			ret_summ = eset->bins[i].heap_min;
     .           		}
 8,302 ( 0.00%)  		if (i == SC_NPSIZES) {
     .           			break;
     .           		}
     .           		assert(i < SC_NPSIZES);
     .           	}
     .           
     .           	return ret;
     .           }
     .           
     .           edata_t *
     .           eset_fit(eset_t *eset, size_t esize, size_t alignment, bool exact_only,
14,344 ( 0.00%)      unsigned lg_max_fit) {
 5,216 ( 0.00%)  	size_t max_size = esize + PAGE_CEILING(alignment) - PAGE;
     .           	/* Beware size_t wrap-around. */
 3,912 ( 0.00%)  	if (max_size < esize) {
     .           		return NULL;
     .           	}
     .           
     .           	edata_t *edata = eset_first_fit(eset, max_size, exact_only, lg_max_fit);
     .           
 6,145 ( 0.00%)  	if (alignment > PAGE && edata == NULL) {
     .           		/*
     .           		 * max_size guarantees the alignment requirement but is rather
     .           		 * pessimistic.  Next we try to satisfy the aligned allocation
     .           		 * with sizes in [esize, max_size).
     .           		 */
     .           		edata = eset_fit_alignment(eset, esize, max_size, alignment);
     .           	}
     .           
     .           	return edata;
11,736 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c
--------------------------------------------------------------------------------
Ir              

-- line 6 ----------------------------------------
     .           enum emap_lock_result_e {
     .           	emap_lock_result_success,
     .           	emap_lock_result_failure,
     .           	emap_lock_result_no_extent
     .           };
     .           typedef enum emap_lock_result_e emap_lock_result_t;
     .           
     .           bool
     1 ( 0.00%)  emap_init(emap_t *emap, base_t *base, bool zeroed) {
     2 ( 0.00%)  	return rtree_new(&emap->rtree, base, zeroed);
   139 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/rtree.c:_rjem_je_rtree_new (1x)
     .           }
     .           
     .           void
     .           emap_update_edata_state(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
70,406 ( 0.02%)      extent_state_t state) {
     .           	witness_assert_positive_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE);
     .           
     .           	edata_state_set(edata, state);
     .           
     .           	EMAP_DECLARE_RTREE_CTX;
     .           	rtree_leaf_elm_t *elm1 = rtree_leaf_elm_lookup(tsdn, &emap->rtree,
     .           	    rtree_ctx, (uintptr_t)edata_base_get(edata), /* dependent */ true,
     .           	    /* init_missing */ false);
     .           	assert(elm1 != NULL);
10,058 ( 0.00%)  	rtree_leaf_elm_t *elm2 = edata_size_get(edata) == PAGE ? NULL :
     .           	    rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
     .           	    (uintptr_t)edata_last_get(edata), /* dependent */ true,
     .           	    /* init_missing */ false);
     .           
     .           	rtree_leaf_elm_state_update(tsdn, &emap->rtree, elm1, elm2, state);
     .           
     .           	emap_assert_mapped(tsdn, emap, edata);
55,319 ( 0.02%)  }
     .           
     .           static inline edata_t *
     .           emap_try_acquire_edata_neighbor_impl(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
     .               extent_pai_t pai, extent_state_t expected_state, bool forward,
55,272 ( 0.02%)      bool expanding) {
     .           	witness_assert_positive_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE);
     .           	assert(!edata_guarded_get(edata));
     .           	assert(!expanding || forward);
     .           	assert(!edata_state_in_transition(expected_state));
     .           	assert(expected_state == extent_state_dirty ||
     .           	       expected_state == extent_state_muzzy ||
     .           	       expected_state == extent_state_retained);
     .           
 7,896 ( 0.00%)  	void *neighbor_addr = forward ? edata_past_get(edata) :
     .           	    edata_before_get(edata);
     .           	/*
     .           	 * This is subtle; the rtree code asserts that its input pointer is
     .           	 * non-NULL, and this is a useful thing to check.  But it's possible
     .           	 * that edata corresponds to an address of (void *)PAGE (in practice,
     .           	 * this has only been observed on FreeBSD when address-space
     .           	 * randomization is on, but it could in principle happen anywhere).  In
     .           	 * this case, edata_before_get(edata) is NULL, triggering the assert.
     .           	 */
 7,896 ( 0.00%)  	if (neighbor_addr == NULL) {
 2,717 ( 0.00%)  		return NULL;
     .           	}
     .           
     .           	EMAP_DECLARE_RTREE_CTX;
     .           	rtree_leaf_elm_t *elm = rtree_leaf_elm_lookup(tsdn, &emap->rtree,
     .           	    rtree_ctx, (uintptr_t)neighbor_addr, /* dependent*/ false,
     .           	    /* init_missing */ false);
 7,896 ( 0.00%)  	if (elm == NULL) {
     .           		return NULL;
     .           	}
     .           
     .           	rtree_contents_t neighbor_contents = rtree_leaf_elm_read(tsdn,
     .           	    &emap->rtree, elm, /* dependent */ true);
     .           	if (!extent_can_acquire_neighbor(edata, neighbor_contents, pai,
     .           	    expected_state, forward, expanding)) {
     .           		return NULL;
     .           	}
     .           
     .           	/* From this point, the neighbor edata can be safely acquired. */
     .           	edata_t *neighbor = neighbor_contents.edata;
     .           	assert(edata_state_get(neighbor) == expected_state);
 3,693 ( 0.00%)  	emap_update_edata_state(tsdn, emap, neighbor, extent_state_merging);
92,969 ( 0.03%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (1,231x)
     .           	if (expanding) {
     .           		extent_assert_can_expand(edata, neighbor);
     .           	} else {
     .           		extent_assert_can_coalesce(edata, neighbor);
     .           	}
     .           
 1,231 ( 0.00%)  	return neighbor;
47,376 ( 0.01%)  }
     .           
     .           edata_t *
     .           emap_try_acquire_edata_neighbor(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
 8,985 ( 0.00%)      extent_pai_t pai, extent_state_t expected_state, bool forward) {
11,980 ( 0.00%)  	return emap_try_acquire_edata_neighbor_impl(tsdn, emap, edata, pai,
307,666 ( 0.08%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:emap_try_acquire_edata_neighbor_impl (2,995x)
     .           	    expected_state, forward, /* expand */ false);
 5,990 ( 0.00%)  }
     .           
     .           edata_t *
     .           emap_try_acquire_edata_neighbor_expand(tsdn_t *tsdn, emap_t *emap,
 2,859 ( 0.00%)      edata_t *edata, extent_pai_t pai, extent_state_t expected_state) {
     .           	/* Try expanding forward. */
 3,812 ( 0.00%)  	return emap_try_acquire_edata_neighbor_impl(tsdn, emap, edata, pai,
104,396 ( 0.03%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:emap_try_acquire_edata_neighbor_impl (953x)
     .           	    expected_state, /* forward */ true, /* expand */ true);
 1,906 ( 0.00%)  }
     .           
     .           void
     .           emap_release_edata(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
   208 ( 0.00%)      extent_state_t new_state) {
     .           	assert(emap_edata_in_transition(tsdn, emap, edata));
     .           	assert(emap_edata_is_acquired(tsdn, emap, edata));
     .           
   208 ( 0.00%)  	emap_update_edata_state(tsdn, emap, edata, new_state);
15,314 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (208x)
     .           }
     .           
     .           static bool
     .           emap_rtree_leaf_elms_lookup(tsdn_t *tsdn, emap_t *emap, rtree_ctx_t *rtree_ctx,
     .               const edata_t *edata, bool dependent, bool init_missing,
     .               rtree_leaf_elm_t **r_elm_a, rtree_leaf_elm_t **r_elm_b) {
 4,264 ( 0.00%)  	*r_elm_a = rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
     .           	    (uintptr_t)edata_base_get(edata), dependent, init_missing);
 4,448 ( 0.00%)  	if (!dependent && *r_elm_a == NULL) {
     .           		return true;
     .           	}
     .           	assert(*r_elm_a != NULL);
     .           
 4,264 ( 0.00%)  	*r_elm_b = rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
     .           	    (uintptr_t)edata_last_get(edata), dependent, init_missing);
    12 ( 0.00%)  	if (!dependent && *r_elm_b == NULL) {
     .           		return true;
     .           	}
     .           	assert(*r_elm_b != NULL);
     .           
     .           	return false;
     .           }
     .           
     .           static void
     .           emap_rtree_write_acquired(tsdn_t *tsdn, emap_t *emap, rtree_leaf_elm_t *elm_a,
     .               rtree_leaf_elm_t *elm_b, edata_t *edata, szind_t szind, bool slab) {
     .           	rtree_contents_t contents;
     .           	contents.edata = edata;
     .           	contents.metadata.szind = szind;
     .           	contents.metadata.slab = slab;
16,211 ( 0.00%)  	contents.metadata.is_head = (edata == NULL) ? false :
     .           	    edata_is_head_get(edata);
     .           	contents.metadata.state = (edata == NULL) ? 0 : edata_state_get(edata);
     .           	rtree_leaf_elm_write(tsdn, &emap->rtree, elm_a, contents);
 6,482 ( 0.00%)  	if (elm_b != NULL) {
     .           		rtree_leaf_elm_write(tsdn, &emap->rtree, elm_b, contents);
     .           	}
     .           }
     .           
     .           bool
     .           emap_register_boundary(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
   102 ( 0.00%)      szind_t szind, bool slab) {
     .           	assert(edata_state_get(edata) == extent_state_active);
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
     .           	rtree_leaf_elm_t *elm_a, *elm_b;
     .           	bool err = emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, edata,
     .           	    false, true, &elm_a, &elm_b);
     .           	if (err) {
     .           		return true;
     .           	}
     .           	assert(rtree_leaf_elm_read(tsdn, &emap->rtree, elm_a,
     .           	    /* dependent */ false).edata == NULL);
     .           	assert(rtree_leaf_elm_read(tsdn, &emap->rtree, elm_b,
     .           	    /* dependent */ false).edata == NULL);
     .           	emap_rtree_write_acquired(tsdn, emap, elm_a, elm_b, edata, szind, slab);
     6 ( 0.00%)  	return false;
    66 ( 0.00%)  }
     .           
     .           /* Invoked *after* emap_register_boundary. */
     .           void
     .           emap_register_interior(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
 3,568 ( 0.00%)      szind_t szind) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
     .           	assert(edata_slab_get(edata));
     .           	assert(edata_state_get(edata) == extent_state_active);
     .           
     .           	if (config_debug) {
     .           		/* Making sure the boundary is registered already. */
     .           		rtree_leaf_elm_t *elm_a, *elm_b;
-- line 188 ----------------------------------------
-- line 202 ----------------------------------------
     .           	rtree_contents_t contents;
     .           	contents.edata = edata;
     .           	contents.metadata.szind = szind;
     .           	contents.metadata.slab = true;
     .           	contents.metadata.state = extent_state_active;
     .           	contents.metadata.is_head = false; /* Not allowed to access. */
     .           
     .           	assert(edata_size_get(edata) > (2 << LG_PAGE));
   446 ( 0.00%)  	rtree_write_range(tsdn, &emap->rtree, rtree_ctx,
     .           	    (uintptr_t)edata_base_get(edata) + PAGE,
     .           	    (uintptr_t)edata_last_get(edata) - PAGE, contents);
 2,453 ( 0.00%)  }
     .           
     .           void
     .           emap_deregister_boundary(tsdn_t *tsdn, emap_t *emap, edata_t *edata) {
     .           	/*
     .           	 * The edata must be either in an acquired state, or protected by state
     .           	 * based locks.
     .           	 */
     .           	if (!emap_edata_is_acquired(tsdn, emap, edata)) {
-- line 221 ----------------------------------------
-- line 228 ----------------------------------------
     .           
     .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, edata,
     .           	    true, false, &elm_a, &elm_b);
     .           	emap_rtree_write_acquired(tsdn, emap, elm_a, elm_b, NULL, SC_NSIZES,
     .           	    false);
     .           }
     .           
     .           void
 4,228 ( 0.00%)  emap_deregister_interior(tsdn_t *tsdn, emap_t *emap, edata_t *edata) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
     .           	assert(edata_slab_get(edata));
   604 ( 0.00%)  	if (edata_size_get(edata) > (2 << LG_PAGE)) {
   390 ( 0.00%)  		rtree_clear_range(tsdn, &emap->rtree, rtree_ctx,
     .           		    (uintptr_t)edata_base_get(edata) + PAGE,
     .           		    (uintptr_t)edata_last_get(edata) - PAGE);
     .           	}
 3,322 ( 0.00%)  }
     .           
     .           void
     .           emap_remap(tsdn_t *tsdn, emap_t *emap, edata_t *edata, szind_t szind,
43,794 ( 0.01%)      bool slab) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
 4,866 ( 0.00%)  	if (szind != SC_NSIZES) {
     .           		rtree_contents_t contents;
     .           		contents.edata = edata;
     .           		contents.metadata.szind = szind;
     .           		contents.metadata.slab = slab;
     .           		contents.metadata.is_head = edata_is_head_get(edata);
     .           		contents.metadata.state = edata_state_get(edata);
     .           
 1,323 ( 0.00%)  		rtree_write(tsdn, &emap->rtree, rtree_ctx,
     .           		    (uintptr_t)edata_addr_get(edata), contents);
     .           		/*
     .           		 * Recall that this is called only for active->inactive and
     .           		 * inactive->active transitions (since only active extents have
     .           		 * meaningful values for szind and slab).  Active, non-slab
     .           		 * extents only need to handle lookups at their head (on
     .           		 * deallocation), so we don't bother filling in the end
     .           		 * boundary.
     .           		 *
     .           		 * For slab extents, we do the end-mapping change.  This still
     .           		 * leaves the interior unmodified; an emap_register_interior
     .           		 * call is coming in those cases, though.
     .           		 */
 3,344 ( 0.00%)  		if (slab && edata_size_get(edata) > PAGE) {
   257 ( 0.00%)  			uintptr_t key = (uintptr_t)edata_past_get(edata)
     .           			    - (uintptr_t)PAGE;
     .           			rtree_write(tsdn, &emap->rtree, rtree_ctx, key,
     .           			    contents);
     .           		}
     .           	}
27,020 ( 0.01%)  }
     .           
     .           bool
     .           emap_split_prepare(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
19,962 ( 0.01%)      edata_t *edata, size_t size_a, edata_t *trail, size_t size_b) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
     .           	/*
     .           	 * We use incorrect constants for things like arena ind, zero, ranged,
     .           	 * and commit state, and head status.  This is a fake edata_t, used to
     .           	 * facilitate a lookup.
     .           	 */
     .           	edata_t lead = {0};
-- line 293 ----------------------------------------
-- line 294 ----------------------------------------
     .           	edata_init(&lead, 0U, edata_addr_get(edata), size_a, false, 0, 0,
     .           	    extent_state_active, false, false, EXTENT_PAI_PAC, EXTENT_NOT_HEAD);
     .           
     .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, &lead, false, true,
     .           	    &prepare->lead_elm_a, &prepare->lead_elm_b);
     .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, trail, false, true,
     .           	    &prepare->trail_elm_a, &prepare->trail_elm_b);
     .           
 4,436 ( 0.00%)  	if (prepare->lead_elm_a == NULL || prepare->lead_elm_b == NULL
 4,436 ( 0.00%)  	    || prepare->trail_elm_a == NULL || prepare->trail_elm_b == NULL) {
     .           		return true;
     .           	}
     .           	return false;
12,199 ( 0.00%)  }
     .           
     .           void
     .           emap_split_commit(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
 2,218 ( 0.00%)      edata_t *lead, size_t size_a, edata_t *trail, size_t size_b) {
     .           	/*
     .           	 * We should think about not writing to the lead leaf element.  We can
     .           	 * get into situations where a racing realloc-like call can disagree
     .           	 * with a size lookup request.  I think it's fine to declare that these
     .           	 * situations are race bugs, but there's an argument to be made that for
     .           	 * things like xallocx, a size lookup call should return either the old
     .           	 * size or the new size, but not anything else.
     .           	 */
 2,218 ( 0.00%)  	emap_rtree_write_acquired(tsdn, emap, prepare->lead_elm_a,
     .           	    prepare->lead_elm_b, lead, SC_NSIZES, /* slab */ false);
 2,218 ( 0.00%)  	emap_rtree_write_acquired(tsdn, emap, prepare->trail_elm_a,
     .           	    prepare->trail_elm_b, trail, SC_NSIZES, /* slab */ false);
 1,109 ( 0.00%)  }
     .           
     .           void
     .           emap_merge_prepare(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
18,414 ( 0.01%)      edata_t *lead, edata_t *trail) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, lead, true, false,
     .           	    &prepare->lead_elm_a, &prepare->lead_elm_b);
     .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, trail, true, false,
     .           	    &prepare->trail_elm_a, &prepare->trail_elm_b);
11,253 ( 0.00%)  }
     .           
     .           void
     .           emap_merge_commit(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
 1,023 ( 0.00%)      edata_t *lead, edata_t *trail) {
     .           	rtree_contents_t clear_contents;
     .           	clear_contents.edata = NULL;
     .           	clear_contents.metadata.szind = SC_NSIZES;
     .           	clear_contents.metadata.slab = false;
     .           	clear_contents.metadata.is_head = false;
     .           	clear_contents.metadata.state = (extent_state_t)0;
     .           
 3,069 ( 0.00%)  	if (prepare->lead_elm_b != NULL) {
     .           		rtree_leaf_elm_write(tsdn, &emap->rtree,
     .           		    prepare->lead_elm_b, clear_contents);
     .           	}
     .           
     .           	rtree_leaf_elm_t *merged_b;
 2,046 ( 0.00%)  	if (prepare->trail_elm_b != NULL) {
 1,023 ( 0.00%)  		rtree_leaf_elm_write(tsdn, &emap->rtree,
     .           		    prepare->trail_elm_a, clear_contents);
 1,023 ( 0.00%)  		merged_b = prepare->trail_elm_b;
     .           	} else {
     .           		merged_b = prepare->trail_elm_a;
     .           	}
     .           
 1,023 ( 0.00%)  	emap_rtree_write_acquired(tsdn, emap, prepare->lead_elm_a, merged_b,
     .           	    lead, SC_NSIZES, false);
 1,023 ( 0.00%)  }
     .           
     .           void
     .           emap_do_assert_mapped(tsdn_t *tsdn, emap_t *emap, edata_t *edata) {
     .           	EMAP_DECLARE_RTREE_CTX;
     .           
     .           	rtree_contents_t contents = rtree_read(tsdn, &emap->rtree, rtree_ctx,
     .           	    (uintptr_t)edata_base_get(edata));
     .           	assert(contents.edata == edata);
-- line 370 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs
--------------------------------------------------------------------------------
Ir                 

-- line 65 ----------------------------------------
        .           
        .           /// If `align` is less than `_Alignof(max_align_t)`, and if the requested
        .           /// allocation `size` is larger than the alignment, we are guaranteed to get a
        .           /// suitably aligned allocation by default, without passing extra flags, and
        .           /// this function returns `0`.
        .           ///
        .           /// Otherwise, it returns the alignment flag to pass to the jemalloc APIs.
        .           fn layout_to_flags(align: usize, size: usize) -> c_int {
5,519,888 ( 1.52%)      if align <= ALIGNOF_MAX_ALIGN_T && align <= size {
        .                   0
        .               } else {
        .                   ffi::MALLOCX_ALIGN(align)
        .               }
  501,808 ( 0.14%)  }
        .           
        .           // Assumes a condition that always must hold.
        .           macro_rules! assume {
        .               ($e:expr) => {
        .                   debug_assert!($e);
        .                   if !($e) {
        .                       core::hint::unreachable_unchecked();
        .                   }
-- line 86 ----------------------------------------
-- line 95 ----------------------------------------
        .           /// allowing usage in collections.
        .           #[derive(Copy, Clone, Default, Debug)]
        .           pub struct Jemalloc;
        .           
        .           unsafe impl GlobalAlloc for Jemalloc {
        .               #[inline]
        .               unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        .                   assume!(layout.size() != 0);
  744,663 ( 0.21%)          let flags = layout_to_flags(layout.align(), layout.size());
   19,896 ( 0.01%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:tikv_jemallocator::layout_to_flags (1,658x)
  496,442 ( 0.14%)          let ptr = if flags == 0 {
  248,235 ( 0.07%)              ffi::malloc(layout.size())
  444,884 ( 0.12%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_malloc (1,658x)
        .                   } else {
  248,221 ( 0.07%)              ffi::mallocx(layout.size(), flags)
        .                   };
        .                   ptr as *mut u8
        .               }
        .           
        .               #[inline]
        .               unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 {
        .                   assume!(layout.size() != 0);
        .                   let flags = layout_to_flags(layout.align(), layout.size());
-- line 115 ----------------------------------------
-- line 120 ----------------------------------------
        .                   };
        .                   ptr as *mut u8
        .               }
        .           
        .               #[inline]
        .               unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        .                   assume!(!ptr.is_null());
        .                   assume!(layout.size() != 0);
  747,054 ( 0.21%)          let flags = layout_to_flags(layout.align(), layout.size());
    4,512 ( 0.00%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:tikv_jemallocator::layout_to_flags (376x)
1,331,003 ( 0.37%)          ffi::sdallocx(ptr as *mut c_void, layout.size(), flags)
  185,219 ( 0.05%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_sdallocx (376x)
        .               }
        .           
        .               #[inline]
        .               unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
        .                   assume!(layout.size() != 0);
        .                   assume!(new_size != 0);
   11,310 ( 0.00%)          let flags = layout_to_flags(layout.align(), new_size);
   45,240 ( 0.01%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs:tikv_jemallocator::layout_to_flags (3,770x)
    7,540 ( 0.00%)          let ptr = if flags == 0 {
    3,770 ( 0.00%)              ffi::realloc(ptr as *mut c_void, new_size)
26,758,436 ( 7.38%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_realloc (3,770x)
        .                   } else {
    7,540 ( 0.00%)              ffi::rallocx(ptr as *mut c_void, new_size, flags)
        .                   };
        .                   ptr as *mut u8
        .               }
        .           }
        .           
        .           #[cfg(feature = "alloc_trait")]
        .           unsafe impl Alloc for Jemalloc {
        .               #[inline]
-- line 148 ----------------------------------------

  246,994 ( 0.07%)  <counts for unidentified lines in /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/tikv-jemallocator-0.5.4/src/lib.rs>

--------------------------------------------------------------------------------
-- Auto-annotated source: src/bin/kirk_seidel.rs
--------------------------------------------------------------------------------
Ir                 

        .           #[global_allocator]
       21 ( 0.00%)  static GLOBAL: tikv_jemallocator::Jemalloc = tikv_jemallocator::Jemalloc;
        .           
        .           use rustc_hash::FxHashSet as HashSet;
        .           use rand::Rng;
        .           use std::hash::Hash;
        .           
      241 ( 0.00%)  #[derive(Debug, Clone, Copy, PartialEq, PartialOrd)]
        .           struct Vec2 {
  439,837 ( 0.12%)      x: f32,
        .               y: f32,
        .           }
        .           
        .           impl Vec2 {
        .               fn new(x: f32, y: f32) -> Self {
        .                   Self { x, y }
        .               }
        .           }
        .           
        .           impl Eq for Vec2 {}
        .           
        .           impl Hash for Vec2 {
        .               fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
2,348,493 ( 0.65%)          self.x.to_bits().hash(state);
3,323,006 ( 0.92%)          self.y.to_bits().hash(state);
        .               }
        .           }
        .           
        .           fn kirk_patrick_seidel(points: &[Vec2]) -> Vec<Vec2> {
        3 ( 0.00%)      let mut upper_hull_vec = upper_hull(&points);
157,366,163 (43.39%)  => src/bin/kirk_seidel.rs:kirk_seidel::upper_hull (1x)
        .           
        3 ( 0.00%)      let mut lower_hull_vec = upper_hull(
199,556,245 (55.02%)  => src/bin/kirk_seidel.rs:kirk_seidel::upper_hull (1x)
        .                   &points
        .                       .iter()
        .                       .map(|point| Vec2 {
        .                           x: point.x,
   50,000 ( 0.01%)                  y: -point.y,
        .                       })
        .                       .collect::<Vec<_>>(),
        .               );
        .           
        5 ( 0.00%)      lower_hull_vec = lower_hull_vec
        .                   .iter()
        .                   .map(|point| Vec2 {
        .                       x: point.x,
       28 ( 0.00%)              y: -point.y,
        .                   })
        .                   .collect();
        .           
        4 ( 0.00%)      let upper_hull_max = *upper_hull_vec
        .                   .iter()
        .                   .max_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        1 ( 0.00%)      let upper_hull_min = *upper_hull_vec
        .                   .iter()
        .                   .min_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        3 ( 0.00%)      let lower_hull_max = *lower_hull_vec
        .                   .iter()
        .                   .max_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        1 ( 0.00%)      let lower_hull_min = *lower_hull_vec
        .                   .iter()
        .                   .min_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        .           
        2 ( 0.00%)      if upper_hull_max.x == lower_hull_max.x && upper_hull_min.y != lower_hull_min.y {
        .                   upper_hull_vec.push(lower_hull_max);
        .               }
        .           
        2 ( 0.00%)      if upper_hull_min.x == lower_hull_min.x && upper_hull_min.y != lower_hull_min.y {
        .                   upper_hull_vec.push(lower_hull_min);
        .               }
        .           
        .               upper_hull_vec.extend(lower_hull_vec);
        1 ( 0.00%)      upper_hull_vec
        .           }
        .           
       18 ( 0.00%)  fn upper_hull(points: &[Vec2]) -> Vec<Vec2> {
        .               let mut min_point = Vec2 {
        .                   x: f32::MAX,
        .                   y: f32::MIN,
        .               };
        .               for i in points.iter() {
  999,962 ( 0.28%)          if i.x < min_point.x || (i.x == min_point.x && i.y > min_point.y) {
       24 ( 0.00%)              min_point = *i;
        .                   }
        .               }
        .           
        .               let mut max_point = Vec2 {
        .                   x: f32::MIN,
        .                   y: f32::MAX,
        .               };
        .               for i in points.iter() {
  799,972 ( 0.22%)          if i.x > max_point.x || (i.x == max_point.x && i.y > max_point.y) {
       28 ( 0.00%)              max_point = *i;
        .                   }
        .               }
        .           
        .               if min_point == max_point {
        .                   return vec![min_point];
        .               }
        .           
        8 ( 0.00%)      let mut temporary = vec![min_point, max_point];
        .               temporary.extend(
        .                   points
        .                       .iter()
  999,996 ( 0.28%)              .filter(|p| p.x > min_point.x && p.x < max_point.x),
        .               );
        .           
       12 ( 0.00%)      connect(&min_point, &max_point, &temporary)
349,497,391 (96.36%)  => src/bin/kirk_seidel.rs:kirk_seidel::connect (2x)
       14 ( 0.00%)  }
        .           
      288 ( 0.00%)  fn connect(min: &Vec2, max: &Vec2, points: &[Vec2]) -> Vec<Vec2> {
       96 ( 0.00%)      let median = median_of_medians(&points.iter().map(|point| point.x).collect::<Vec<_>>());
14,584,200 ( 4.02%)  => src/bin/kirk_seidel.rs:kirk_seidel::median_of_medians (2x)
        .           
      216 ( 0.00%)      let (left, right) = bridge(points, median);
197,559,373 (54.47%)  => src/bin/kirk_seidel.rs:kirk_seidel::bridge (2x)
        .           
       24 ( 0.00%)      let mut left_points = vec![left];
  635,786 ( 0.18%)      left_points.extend(points.iter().filter(|p| p.x < left.x));
        .           
       24 ( 0.00%)      let mut right_points = vec![right];
  635,786 ( 0.18%)      right_points.extend(points.iter().filter(|p| p.x > right.x));
        .           
        .               let mut output = vec![];
        .               if left == *min {
       22 ( 0.00%)          output.extend(vec![left]);
        .               } else {
       78 ( 0.00%)          output.extend(connect(&min, &left, &left_points));
64,751,555 (17.85%)  => src/bin/kirk_seidel.rs:kirk_seidel::connect'2 (2x)
        .               }
        .           
        .               if right == *max {
       15 ( 0.00%)          output.extend(vec![right]);
        .               } else {
       45 ( 0.00%)          output.extend(connect(&right, &max, &right_points));
68,274,458 (18.82%)  => src/bin/kirk_seidel.rs:kirk_seidel::connect'2 (2x)
        .               }
        .           
      120 ( 0.00%)      output
        .           }
        .           
    3,200 ( 0.00%)  fn bridge(points: &[Vec2], median: f32) -> (Vec2, Vec2) {
        .               let mut candidates: HashSet<&Vec2> = HashSet::default();
      800 ( 0.00%)      if points.len() == 2 {
       52 ( 0.00%)          return if points[0].x < points[1].x {
       20 ( 0.00%)              (points[0], points[1])
        .                   } else {
       27 ( 0.00%)              (points[1], points[0])
        .                   };
        .               }
        .           
        .               let mut pairs: Vec<(&Vec2, &Vec2)> = Vec::new();
        .           
        .               for chunk in points.chunks(2) {
1,292,739 ( 0.36%)          if chunk.len() == 2 {
  646,095 ( 0.18%)              if chunk[0] > chunk[1] {
        .                           pairs.push((&chunk[1], &chunk[0]));
        .                       } else {
        .                           pairs.push((&chunk[0], &chunk[1]));
        .                       }
        .                   } else {
        .                       candidates.insert(&chunk[0]);
        .                   }
        .               }
        .           
        .               let mut slopes = vec![];
        .           
      774 ( 0.00%)      for (point_i, point_j) in pairs.iter() {
3,878,505 ( 1.07%)          if point_i.x == point_j.x {
        .                       if point_i.y > point_j.y {
        .                           candidates.insert(point_i);
        .                       } else {
        .                           candidates.insert(point_j);
        .                       }
        .                   } else {
        .                       slopes.push((
        .                           point_i,
        .                           point_j,
        .                           (point_i.y - point_j.y) / (point_i.x - point_j.x),
        .                       ));
        .                   }
        .               }
        .           
        .               let median_slope =
    2,322 ( 0.00%)          median_of_medians(&slopes.iter().map(|(_, _, slope)| slope).collect::<Vec<_>>());
12,849,936 ( 3.54%)  => src/bin/kirk_seidel.rs:kirk_seidel::median_of_medians (22x)
        .           
        .               let mut small = Vec::with_capacity(slopes.len());
        .               let mut equal = Vec::with_capacity(slopes.len());
        .               let mut large = Vec::with_capacity(slopes.len());
        .           
      774 ( 0.00%)      for slope in slopes.iter() {
        .                   match slope.2 {
3,231,636 ( 0.89%)              s if s < *median_slope => small.push(slope),
  632,493 ( 0.17%)              s if s == *median_slope => equal.push(slope),
  315,666 ( 0.09%)              s if s > *median_slope => large.push(slope),
        .                       _ => unreachable!(),
        .                   }
        .               }
        .           
        .               // set of points with maximum value of p.y - median_slope * p.x
        .               let max_value = points
        .                   .iter()
1,292,373 ( 0.36%)          .map(|p| p.y - median_slope * p.x)
        .                   .fold(f32::MIN, f32::max);
        .               let max_points: Vec<_> = points
        .                   .iter()
5,169,492 ( 1.43%)          .filter(|p| p.y - median_slope * p.x == max_value)
        .                   .collect();
      387 ( 0.00%)      let min_point = max_points
        .                   .iter()
       28 ( 0.00%)          .min_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        .               let max_point = max_points
        .                   .iter()
       28 ( 0.00%)          .max_by(|a, b| a.x.partial_cmp(&b.x).unwrap())
        .                   .unwrap();
        .           
    2,014 ( 0.00%)      if min_point.x <= median && max_point.x > median {
       66 ( 0.00%)          return (**min_point, **max_point);
    1,526 ( 0.00%)      } else if max_point.x <= median {
      666 ( 0.00%)          for (_, point2, _) in large {
  324,500 ( 0.09%)              candidates.insert(point2);
        .                   }
        .           
      666 ( 0.00%)          for (_, point2, _) in equal {
      444 ( 0.00%)              candidates.insert(point2);
        .                   }
        .           
      666 ( 0.00%)          for (point1, point2, _) in small {
  349,688 ( 0.10%)              candidates.insert(point2);
  349,688 ( 0.10%)              candidates.insert(point1);
        .                   }
      308 ( 0.00%)      } else if min_point.x > median {
      462 ( 0.00%)          for (point1, _, _) in small {
  310,380 ( 0.09%)              candidates.insert(point1);
        .                   }
        .           
      462 ( 0.00%)          for (point1, _, _) in equal {
      308 ( 0.00%)              candidates.insert(point1);
        .                   }
        .           
      616 ( 0.00%)          for (point1, point2, _) in large {
  306,826 ( 0.08%)              candidates.insert(point2);
  306,826 ( 0.08%)              candidates.insert(point1);
        .                   }
        .               }
        .           
    1,504 ( 0.00%)      bridge(
243,156,082 (67.04%)  => src/bin/kirk_seidel.rs:kirk_seidel::bridge'2 (21x)
    1,880 ( 0.00%)          &candidates.into_iter().cloned().collect::<Vec<Vec2>>(),
        .                   median,
        .               )
    5,101 ( 0.00%)  }
   17,730 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_sdallocx (22x)
        .           
    9,040 ( 0.00%)  pub fn median_of_medians<T>(nums: &[T]) -> T
        .           where
        .               T: Clone + Copy + PartialOrd,
        .           {
    5,190 ( 0.00%)      match nums.len() {
        .                   0 => panic!("No median of an empty list"),
       30 ( 0.00%)          1 => nums[0],
    3,870 ( 0.00%)          2..=5 => {
        .                       let mut nums = nums.to_owned();
        .                       nums.sort_unstable_by(|a, b| a.partial_cmp(b).unwrap());
    1,212 ( 0.00%)              nums[nums.len() / 2]
        .                   }
    2,682 ( 0.00%)          _ => median_of_medians(
15,361,722 ( 4.24%)  => src/bin/kirk_seidel.rs:kirk_seidel::median_of_medians'2 (329x)
    1,788 ( 0.00%)              &nums
        .                           .to_vec()
        .                           .chunks(5)
        .                           .map(|chunk| {
        .                               let mut chunk = chunk.to_vec();
        .                               chunk.sort_unstable_by(|a, b| a.partial_cmp(b).unwrap());
  320,640 ( 0.09%)                      chunk[chunk.len() / 2]
        .                           })
        .                           .collect::<Vec<T>>(),
        .                   ),
        .               }
   10,250 ( 0.00%)  }
        .           
        8 ( 0.00%)  fn main() {
        .               let n = 1_00_000;
        3 ( 0.00%)      let mut rng = rand::thread_rng();
    5,078 ( 0.00%)  => /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand-0.8.5/src/rngs/thread.rs:rand::rngs::thread::thread_rng (1x)
        .               let points: Vec<Vec2> = (0..n).map(|_| Vec2::new(rng.gen(), rng.gen())).collect();
        .           
        4 ( 0.00%)      kirk_patrick_seidel(&points);
        8 ( 0.00%)  }

5,186,712 ( 1.43%)  <counts for unidentified lines in src/bin/kirk_seidel.rs>

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/rtree.h
--------------------------------------------------------------------------------
Ir               

-- line 140 ----------------------------------------
      .           	unsigned cumbits = (rtree_levels[RTREE_HEIGHT-1].cumbits -
      .           	    rtree_levels[RTREE_HEIGHT-1].bits);
      .           	return ptrbits - cumbits;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE uintptr_t
      .           rtree_leafkey(uintptr_t key) {
      .           	uintptr_t mask = ~((ZU(1) << rtree_leaf_maskbits()) - 1);
 81,723 ( 0.02%)  	return (key & mask);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE size_t
      .           rtree_cache_direct_map(uintptr_t key) {
 77,951 ( 0.02%)  	return (size_t)((key >> rtree_leaf_maskbits()) &
      .           	    (RTREE_CTX_NCACHE - 1));
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE uintptr_t
      .           rtree_subkey(uintptr_t key, unsigned level) {
      .           	unsigned ptrbits = ZU(1) << (LG_SIZEOF_PTR+3);
      .           	unsigned cumbits = rtree_levels[level].cumbits;
      .           	unsigned shiftbits = ptrbits - cumbits;
      .           	unsigned maskbits = rtree_levels[level].bits;
      .           	uintptr_t mask = (ZU(1) << maskbits) - 1;
 52,849 ( 0.01%)  	return ((key >> shiftbits) & mask);
      .           }
      .           
      .           /*
      .            * Atomic getters.
      .            *
      .            * dependent: Reading a value on behalf of a pointer to a valid allocation
      .            *            is guaranteed to be a clean read even without synchronization,
      .            *            because the rtree update became visible in memory before the
-- line 172 ----------------------------------------
-- line 181 ----------------------------------------
      .               rtree_leaf_elm_t *elm, bool dependent) {
      .           	return (uintptr_t)atomic_load_p(&elm->le_bits, dependent
      .           	    ? ATOMIC_RELAXED : ATOMIC_ACQUIRE);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE uintptr_t
      .           rtree_leaf_elm_bits_encode(rtree_contents_t contents) {
      .           	assert((uintptr_t)contents.edata % (uintptr_t)EDATA_ALIGNMENT == 0);
 10,100 ( 0.00%)  	uintptr_t edata_bits = (uintptr_t)contents.edata
      .           	    & (((uintptr_t)1 << LG_VADDR) - 1);
      .           
  3,132 ( 0.00%)  	uintptr_t szind_bits = (uintptr_t)contents.metadata.szind << LG_VADDR;
  1,329 ( 0.00%)  	uintptr_t slab_bits = (uintptr_t)contents.metadata.slab;
  9,648 ( 0.00%)  	uintptr_t is_head_bits = (uintptr_t)contents.metadata.is_head << 1;
  8,080 ( 0.00%)  	uintptr_t state_bits = (uintptr_t)contents.metadata.state <<
      .           	    RTREE_LEAF_STATE_SHIFT;
      .           	uintptr_t metadata_bits = szind_bits | state_bits | is_head_bits |
      .           	    slab_bits;
      .           	assert((edata_bits & metadata_bits) == 0);
      .           
 16,513 ( 0.00%)  	return edata_bits | metadata_bits;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE rtree_contents_t
      .           rtree_leaf_elm_bits_decode(uintptr_t bits) {
      .           	rtree_contents_t contents;
      .           	/* Do the easy things first. */
      .           	contents.metadata.szind = bits >> LG_VADDR;
      .           	contents.metadata.slab = (bool)(bits & 1);
      .           	contents.metadata.is_head = (bool)(bits & (1 << 1));
      .           
  7,670 ( 0.00%)  	uintptr_t state_bits = (bits & RTREE_LEAF_STATE_MASK) >>
      .           	    RTREE_LEAF_STATE_SHIFT;
      .           	assert(state_bits <= extent_state_max);
      .           	contents.metadata.state = (extent_state_t)state_bits;
      .           
      .           	uintptr_t low_bit_mask = ~((uintptr_t)EDATA_ALIGNMENT - 1);
      .           #    ifdef __aarch64__
      .           	/*
      .           	 * aarch64 doesn't sign extend the highest virtual address bit to set
-- line 220 ----------------------------------------
-- line 221 ----------------------------------------
      .           	 * the higher ones.  Instead, the high bits get zeroed.
      .           	 */
      .           	uintptr_t high_bit_mask = ((uintptr_t)1 << LG_VADDR) - 1;
      .           	/* Mask off metadata. */
      .           	uintptr_t mask = high_bit_mask & low_bit_mask;
      .           	contents.edata = (edata_t *)(bits & mask);
      .           #    else
      .           	/* Restore sign-extended high bits, mask metadata bits. */
 18,981 ( 0.01%)  	contents.edata = (edata_t *)((uintptr_t)((intptr_t)(bits << RTREE_NHIB)
 18,448 ( 0.01%)  	    >> RTREE_NHIB) & low_bit_mask);
      .           #    endif
      .           	assert((uintptr_t)contents.edata % (uintptr_t)EDATA_ALIGNMENT == 0);
      .           	return contents;
      .           }
      .           
      .           #  endif /* RTREE_LEAF_COMPACT */
      .           
      .           JEMALLOC_ALWAYS_INLINE rtree_contents_t
-- line 238 ----------------------------------------
-- line 307 ----------------------------------------
      .           /* The state field can be updated independently (and more frequently). */
      .           JEMALLOC_ALWAYS_INLINE void
      .           rtree_leaf_elm_state_update(tsdn_t *tsdn, rtree_t *rtree,
      .               rtree_leaf_elm_t *elm1, rtree_leaf_elm_t *elm2, extent_state_t state) {
      .           	assert(elm1 != NULL);
      .           #ifdef RTREE_LEAF_COMPACT
      .           	uintptr_t bits = rtree_leaf_elm_bits_read(tsdn, rtree, elm1,
      .           	    /* dependent */ true);
  5,029 ( 0.00%)  	bits &= ~RTREE_LEAF_STATE_MASK;
 10,058 ( 0.00%)  	bits |= state << RTREE_LEAF_STATE_SHIFT;
      .           	atomic_store_p(&elm1->le_bits, (void *)bits, ATOMIC_RELEASE);
  9,699 ( 0.00%)  	if (elm2 != NULL) {
      .           		atomic_store_p(&elm2->le_bits, (void *)bits, ATOMIC_RELEASE);
      .           	}
      .           #else
      .           	unsigned bits = atomic_load_u(&elm1->le_metadata, ATOMIC_RELAXED);
      .           	bits &= ~RTREE_LEAF_STATE_MASK;
      .           	bits |= state << RTREE_LEAF_STATE_SHIFT;
      .           	atomic_store_u(&elm1->le_metadata, bits, ATOMIC_RELEASE);
      .           	if (elm2 != NULL) {
-- line 326 ----------------------------------------
-- line 359 ----------------------------------------
      .           	assert(key != 0);
      .           	assert(!dependent || !init_missing);
      .           
      .           	size_t slot = rtree_cache_direct_map(key);
      .           	uintptr_t leafkey = rtree_leafkey(key);
      .           	assert(leafkey != RTREE_LEAFKEY_INVALID);
      .           
      .           	/* Fast path: L1 direct mapped cache. */
165,407 ( 0.05%)  	if (likely(rtree_ctx->cache[slot].leafkey == leafkey)) {
      .           		rtree_leaf_elm_t *leaf = rtree_ctx->cache[slot].leaf;
      .           		assert(leaf != NULL);
      .           		uintptr_t subkey = rtree_subkey(key, RTREE_HEIGHT-1);
 81,978 ( 0.02%)  		return &leaf[subkey];
      .           	}
      .           	/*
      .           	 * Search the L2 LRU cache.  On hit, swap the matching element into the
      .           	 * slot in L1 cache, and move the position in L2 up by 1.
      .           	 */
      .           #define RTREE_CACHE_CHECK_L2(i) do {					\
      .           	if (likely(rtree_ctx->l2_cache[i].leafkey == leafkey)) {	\
      .           		rtree_leaf_elm_t *leaf = rtree_ctx->l2_cache[i].leaf;	\
-- line 379 ----------------------------------------
-- line 396 ----------------------------------------
      .           		}							\
      .           		rtree_ctx->cache[slot].leafkey = leafkey;		\
      .           		rtree_ctx->cache[slot].leaf = leaf;			\
      .           		uintptr_t subkey = rtree_subkey(key, RTREE_HEIGHT-1);	\
      .           		return &leaf[subkey];					\
      .           	}								\
      .           } while (0)
      .           	/* Check the first cache entry. */
      2 ( 0.00%)  	RTREE_CACHE_CHECK_L2(0);
      .           	/* Search the remaining cache elements. */
      6 ( 0.00%)  	for (unsigned i = 1; i < RTREE_CTX_NCACHE_L2; i++) {
     14 ( 0.00%)  		RTREE_CACHE_CHECK_L2(i);
      .           	}
      .           #undef RTREE_CACHE_CHECK_L2
      .           
     12 ( 0.00%)  	return rtree_leaf_elm_lookup_hard(tsdn, rtree, rtree_ctx, key,
  2,509 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/rtree.c:_rjem_je_rtree_leaf_elm_lookup_hard (1x)
      .           	    dependent, init_missing);
      .           }
      .           
      .           /*
      .            * Returns true on lookup failure.
      .            */
      .           static inline bool
      .           rtree_read_independent(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx,
-- line 419 ----------------------------------------
-- line 478 ----------------------------------------
      .           	 * boundaries have been registered already.  Therefore all the lookups
      .           	 * are dependent w/o init_missing, assuming the range spans across at
      .           	 * most 2 rtree leaf nodes (each covers 1 GiB of vaddr).
      .           	 */
      .           	void *bits;
      .           	unsigned additional;
      .           	rtree_contents_encode(contents, &bits, &additional);
      .           
    613 ( 0.00%)  	rtree_leaf_elm_t *elm = NULL; /* Dead store. */
  5,179 ( 0.00%)  	for (uintptr_t addr = base; addr <= end; addr += PAGE) {
  4,100 ( 0.00%)  		if (addr == base ||
      .           		    (addr & ((ZU(1) << rtree_leaf_maskbits()) - 1)) == 0) {
      .           			elm = rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx, addr,
      .           			    /* dependent */ true, /* init_missing */ false);
      .           			assert(elm != NULL);
      .           		}
      .           		assert(elm == rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx, addr,
      .           		    /* dependent */ true, /* init_missing */ false));
      .           		assert(!clearing || rtree_leaf_elm_read(tsdn, rtree, elm,
      .           		    /* dependent */ true).edata != NULL);
      .           		rtree_leaf_elm_write_commit(tsdn, rtree, elm, bits, additional);
  1,234 ( 0.00%)  		elm++;
      .           	}
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           rtree_write_range(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx,
      .               uintptr_t base, uintptr_t end, rtree_contents_t contents) {
      .           	rtree_write_range_impl(tsdn, rtree, rtree_ctx, base, end, contents,
      .           	    /* clearing */ false);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE bool
      .           rtree_write(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx, uintptr_t key,
      .               rtree_contents_t contents) {
      .           	rtree_leaf_elm_t *elm = rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx,
      .           	    key, /* dependent */ false, /* init_missing */ true);
  3,160 ( 0.00%)  	if (elm == NULL) {
      .           		return true;
      .           	}
      .           
      .           	rtree_leaf_elm_write(tsdn, rtree, elm, contents);
      .           
      .           	return false;
      .           }
      .           
-- line 523 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/fb.h
--------------------------------------------------------------------------------
Ir              

-- line 53 ----------------------------------------
     .           	size_t group_ind = bit / FB_GROUP_BITS;
     .           	size_t bit_ind = bit % FB_GROUP_BITS;
     .           	return (bool)(fb[group_ind] & ((fb_group_t)1 << bit_ind));
     .           }
     .           
     .           static inline void
     .           fb_set(fb_group_t *fb, size_t nbits, size_t bit) {
     .           	assert(bit < nbits);
 2,992 ( 0.00%)  	size_t group_ind = bit / FB_GROUP_BITS;
     .           	size_t bit_ind = bit % FB_GROUP_BITS;
 4,488 ( 0.00%)  	fb[group_ind] |= ((fb_group_t)1 << bit_ind);
     .           }
     .           
     .           static inline void
     .           fb_unset(fb_group_t *fb, size_t nbits, size_t bit) {
     .           	assert(bit < nbits);
 1,477 ( 0.00%)  	size_t group_ind = bit / FB_GROUP_BITS;
     .           	size_t bit_ind = bit % FB_GROUP_BITS;
 4,431 ( 0.00%)  	fb[group_ind] &= ~((fb_group_t)1 << bit_ind);
     .           }
     .           
     .           
     .           /*
     .            * Some implementation details.  This visitation function lets us apply a group
     .            * visitor to each group in the bitmap (potentially modifying it).  The mask
     .            * indicates which bits are logically part of the visitation.
     .            */
-- line 79 ----------------------------------------
-- line 171 ----------------------------------------
     .            *
     .            * Returns the number of bits in the bitmap if no such bit exists.
     .            */
     .           JEMALLOC_ALWAYS_INLINE ssize_t
     .           fb_find_impl(fb_group_t *fb, size_t nbits, size_t start, bool val,
     .               bool forward) {
     .           	assert(start < nbits);
     .           	size_t ngroups = FB_NGROUPS(nbits);
10,910 ( 0.00%)  	ssize_t group_ind = start / FB_GROUP_BITS;
     .           	size_t bit_ind = start % FB_GROUP_BITS;
     .           
     .           	fb_group_t maybe_invert = (val ? 0 : (fb_group_t)-1);
     .           
     .           	fb_group_t group = fb[group_ind];
     .           	group ^= maybe_invert;
     .           	if (forward) {
     .           		/* Only keep ones in bits bit_ind and above. */
21,445 ( 0.01%)  		group &= ~((1LU << bit_ind) - 1);
     .           	} else {
     .           		/*
     .           		 * Only keep ones in bits bit_ind and below.  You might more
     .           		 * naturally express this as (1 << (bit_ind + 1)) - 1, but
     .           		 * that shifts by an invalid amount if bit_ind is one less than
     .           		 * FB_GROUP_BITS.
     .           		 */
     .           		group &= ((2LU << bit_ind) - 1);
     .           	}
     .           	ssize_t group_ind_bound = forward ? (ssize_t)ngroups : -1;
16,130 ( 0.00%)  	while (group == 0) {
 1,740 ( 0.00%)  		group_ind += forward ? 1 : -1;
 5,220 ( 0.00%)  		if (group_ind == group_ind_bound) {
     .           			return forward ? (ssize_t)nbits : (ssize_t)-1;
     .           		}
 4,475 ( 0.00%)  		group = fb[group_ind];
     .           		group ^= maybe_invert;
     .           	}
     .           	assert(group != 0);
 4,585 ( 0.00%)  	size_t bit = forward ? ffs_lu(group) : fls_lu(group);
 9,170 ( 0.00%)  	size_t pos = group_ind * FB_GROUP_BITS + bit;
     .           	/*
     .           	 * The high bits of a partially filled last group are zeros, so if we're
     .           	 * looking for zeros we don't want to report an invalid result.
     .           	 */
     .           	if (forward && !val && pos > nbits) {
     .           		return nbits;
     .           	}
     .           	return pos;
-- line 217 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c
--------------------------------------------------------------------------------
Ir              

-- line 194 ----------------------------------------
     .           			    arena_get_bin(arena, i, j));
     .           		}
     .           	}
     .           }
     .           
     .           static void
     .           arena_background_thread_inactivity_check(tsdn_t *tsdn, arena_t *arena,
     .               bool is_background_thread) {
 2,220 ( 0.00%)  	if (!background_thread_enabled() || is_background_thread) {
     .           		return;
     .           	}
     .           	background_thread_info_t *info =
     .           	    arena_background_thread_info_get(arena);
     .           	if (background_thread_indefinite_sleep(info)) {
     .           		arena_maybe_do_deferred_work(tsdn, arena,
     .           		    &arena->pa_shard.pac.decay_dirty, 0);
     .           	}
     .           }
     .           
     .           /*
     .            * React to deferred work generated by a PAI function.
     .            */
 7,770 ( 0.00%)  void arena_handle_deferred_work(tsdn_t *tsdn, arena_t *arena) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
 2,220 ( 0.00%)  	if (decay_immediately(&arena->pa_shard.pac.decay_dirty)) {
     .           		arena_decay_dirty(tsdn, arena, false, true);
     .           	}
     .           	arena_background_thread_inactivity_check(tsdn, arena, false);
 5,550 ( 0.00%)  }
     .           
     .           static void *
     .           arena_slab_reg_alloc(edata_t *slab, const bin_info_t *bin_info) {
     .           	void *ret;
     .           	slab_data_t *slab_data = edata_slab_data_get(slab);
     .           	size_t regind;
     .           
     .           	assert(edata_nfree_get(slab) > 0);
-- line 232 ----------------------------------------
-- line 250 ----------------------------------------
     .           #if (! defined JEMALLOC_INTERNAL_POPCOUNTL) || (defined BITMAP_USE_TREE)
     .           	for (unsigned i = 0; i < cnt; i++) {
     .           		size_t regind = bitmap_sfu(slab_data->bitmap,
     .           					   &bin_info->bitmap_info);
     .           		*(ptrs + i) = (void *)((uintptr_t)edata_addr_get(slab) +
     .           		    (uintptr_t)(bin_info->reg_size * regind));
     .           	}
     .           #else
   825 ( 0.00%)  	unsigned group = 0;
 1,650 ( 0.00%)  	bitmap_t g = slab_data->bitmap[group];
   825 ( 0.00%)  	unsigned i = 0;
 1,668 ( 0.00%)  	while (i < cnt) {
 2,520 ( 0.00%)  		while (g == 0) {
    45 ( 0.00%)  			g = slab_data->bitmap[++group];
     .           		}
 4,170 ( 0.00%)  		size_t shift = group << LG_BITMAP_GROUP_NBITS;
 2,502 ( 0.00%)  		size_t pop = popcount_lu(g);
 3,336 ( 0.00%)  		if (pop > (cnt - i)) {
     .           			pop = cnt - i;
     .           		}
     .           
     .           		/*
     .           		 * Load from memory locations only once, outside the
     .           		 * hot loop below.
     .           		 */
 1,668 ( 0.00%)  		uintptr_t base = (uintptr_t)edata_addr_get(slab);
 5,223 ( 0.00%)  		uintptr_t regsize = (uintptr_t)bin_info->reg_size;
 2,450 ( 0.00%)  		while (pop--) {
     .           			size_t bit = cfs_lu(&g);
 3,052 ( 0.00%)  			size_t regind = shift + bit;
16,085 ( 0.00%)  			*(ptrs + i) = (void *)(base + regsize * regind);
     .           
 1,591 ( 0.00%)  			i++;
     .           		}
   834 ( 0.00%)  		slab_data->bitmap[group] = g;
     .           	}
     .           #endif
 3,300 ( 0.00%)  	edata_nfree_sub(slab, cnt);
     .           }
     .           
     .           static void
     .           arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
     .           	szind_t index, hindex;
     .           
     .           	cassert(config_stats);
     .           
     .           	if (usize < SC_LARGE_MINCLASS) {
     .           		usize = SC_LARGE_MINCLASS;
     .           	}
     .           	index = sz_size2index(usize);
 5,733 ( 0.00%)  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
     .           
     .           	locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
     .           	    &arena->stats.lstats[hindex].nmalloc, 1);
     .           }
     .           
     .           static void
     .           arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
     .           	szind_t index, hindex;
     .           
     .           	cassert(config_stats);
     .           
     .           	if (usize < SC_LARGE_MINCLASS) {
     .           		usize = SC_LARGE_MINCLASS;
     .           	}
     .           	index = sz_size2index(usize);
 6,464 ( 0.00%)  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
     .           
     .           	locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
     .           	    &arena->stats.lstats[hindex].ndalloc, 1);
     .           }
     .           
     .           static void
     .           arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
     .               size_t usize) {
     .           	arena_large_malloc_stats_update(tsdn, arena, usize);
     .           	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
     .           }
     .           
     .           edata_t *
     .           arena_extent_alloc_large(tsdn_t *tsdn, arena_t *arena, size_t usize,
13,923 ( 0.00%)      size_t alignment, bool zero) {
   819 ( 0.00%)  	bool deferred_work_generated = false;
     .           	szind_t szind = sz_size2index(usize);
 4,095 ( 0.00%)  	size_t esize = usize + sz_large_pad;
     .           
     .           	bool guarded = san_large_extent_decide_guard(tsdn,
     .           	    arena_get_ehooks(arena), esize, alignment);
 9,009 ( 0.00%)  	edata_t *edata = pa_alloc(tsdn, &arena->pa_shard, esize, alignment,
1,492,080 ( 0.41%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_alloc (819x)
     .           	    /* slab */ false, szind, zero, guarded, &deferred_work_generated);
     .           	assert(deferred_work_generated == false);
     .           
 2,457 ( 0.00%)  	if (edata != NULL) {
     .           		if (config_stats) {
     .           			LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
     .           			arena_large_malloc_stats_update(tsdn, arena, usize);
     .           			LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
     .           		}
     .           	}
     .           
     .           	if (edata != NULL && sz_large_pad != 0) {
     .           		arena_cache_oblivious_randomize(tsdn, arena, edata, alignment);
     .           	}
     .           
     .           	return edata;
 9,009 ( 0.00%)  }
     .           
     .           void
   808 ( 0.00%)  arena_extent_dalloc_large_prep(tsdn_t *tsdn, arena_t *arena, edata_t *edata) {
     .           	if (config_stats) {
     .           		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
     .           		arena_large_dalloc_stats_update(tsdn, arena,
     .           		    edata_usize_get(edata));
     .           		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
     .           	}
   808 ( 0.00%)  }
     .           
     .           void
     .           arena_extent_ralloc_large_shrink(tsdn_t *tsdn, arena_t *arena, edata_t *edata,
     .               size_t oldusize) {
     .           	size_t usize = edata_usize_get(edata);
     .           
     .           	if (config_stats) {
     .           		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
-- line 373 ----------------------------------------
-- line 390 ----------------------------------------
     .           
     .           /*
     .            * In situations where we're not forcing a decay (i.e. because the user
     .            * specifically requested it), should we purge ourselves, or wait for the
     .            * background thread to get to it.
     .            */
     .           static pac_purge_eagerness_t
     .           arena_decide_unforced_purge_eagerness(bool is_background_thread) {
    16 ( 0.00%)  	if (is_background_thread) {
     .           		return PAC_PURGE_ALWAYS;
    16 ( 0.00%)  	} else if (!is_background_thread && background_thread_enabled()) {
     .           		return PAC_PURGE_NEVER;
     .           	} else {
     .           		return PAC_PURGE_ON_EPOCH_ADVANCE;
     .           	}
     .           }
     .           
     .           bool
     .           arena_decay_ms_set(tsdn_t *tsdn, arena_t *arena, extent_state_t state,
-- line 408 ----------------------------------------
-- line 431 ----------------------------------------
     .           	}
     .           
     .           	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
     .           		/* No need to wait if another thread is in progress. */
     .           		return true;
     .           	}
     .           	pac_purge_eagerness_t eagerness =
     .           	    arena_decide_unforced_purge_eagerness(is_background_thread);
    40 ( 0.00%)  	bool epoch_advanced = pac_maybe_decay_purge(tsdn, &arena->pa_shard.pac,
43,362 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pac.c:_rjem_je_pac_maybe_decay_purge (8x)
     .           	    decay, decay_stats, ecache, eagerness);
     .           	size_t npages_new;
    16 ( 0.00%)  	if (epoch_advanced) {
     .           		/* Backlog is updated on epoch advance. */
     .           		npages_new = decay_epoch_npages_delta(decay);
     .           	}
     .           	malloc_mutex_unlock(tsdn, &decay->mtx);
     .           
    15 ( 0.00%)  	if (have_background_thread && background_thread_enabled() &&
     .           	    epoch_advanced && !is_background_thread) {
     .           		arena_maybe_do_deferred_work(tsdn, arena, decay, npages_new);
     .           	}
     .           
     .           	return false;
     .           }
     .           
     .           static bool
     .           arena_decay_dirty(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
     .               bool all) {
    16 ( 0.00%)  	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_dirty,
    16 ( 0.00%)  	    &arena->pa_shard.pac.stats->decay_dirty,
     .           	    &arena->pa_shard.pac.ecache_dirty, is_background_thread, all);
     .           }
     .           
     .           static bool
     .           arena_decay_muzzy(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
     .               bool all) {
     .           	if (pa_shard_dont_decay_muzzy(&arena->pa_shard)) {
     .           		return false;
     .           	}
     .           	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_muzzy,
     .           	    &arena->pa_shard.pac.stats->decay_muzzy,
     .           	    &arena->pa_shard.pac.ecache_muzzy, is_background_thread, all);
     .           }
     .           
     .           void
    88 ( 0.00%)  arena_decay(tsdn_t *tsdn, arena_t *arena, bool is_background_thread, bool all) {
    16 ( 0.00%)  	if (all) {
     .           		/*
     .           		 * We should take a purge of "all" to mean "save as much memory
     .           		 * as possible", including flushing any caches (for situations
     .           		 * like thread death, or manual purge calls).
     .           		 */
     .           		sec_flush(tsdn, &arena->pa_shard.hpa_sec);
     .           	}
    16 ( 0.00%)  	if (arena_decay_dirty(tsdn, arena, is_background_thread, all)) {
     .           		return;
     .           	}
     .           	arena_decay_muzzy(tsdn, arena, is_background_thread, all);
    64 ( 0.00%)  }
     .           
     .           static bool
     .           arena_should_decay_early(tsdn_t *tsdn, arena_t *arena, decay_t *decay,
     .               background_thread_info_t *info, nstime_t *remaining_sleep,
     .               size_t npages_new) {
     .           	malloc_mutex_assert_owner(tsdn, &info->mtx);
     .           
     .           	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
-- line 497 ----------------------------------------
-- line 560 ----------------------------------------
     .           /* Called from background threads. */
     .           void
     .           arena_do_deferred_work(tsdn_t *tsdn, arena_t *arena) {
     .           	arena_decay(tsdn, arena, true, false);
     .           	pa_shard_do_deferred_work(tsdn, &arena->pa_shard);
     .           }
     .           
     .           void
 3,322 ( 0.00%)  arena_slab_dalloc(tsdn_t *tsdn, arena_t *arena, edata_t *slab) {
   302 ( 0.00%)  	bool deferred_work_generated = false;
   906 ( 0.00%)  	pa_dalloc(tsdn, &arena->pa_shard, slab, &deferred_work_generated);
319,386 ( 0.09%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_dalloc (302x)
   604 ( 0.00%)  	if (deferred_work_generated) {
   906 ( 0.00%)  		arena_handle_deferred_work(tsdn, arena);
 5,738 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_handle_deferred_work (302x)
     .           	}
 2,718 ( 0.00%)  }
     .           
     .           static void
     .           arena_bin_slabs_nonfull_insert(bin_t *bin, edata_t *slab) {
     .           	assert(edata_nfree_get(slab) > 0);
   214 ( 0.00%)  	edata_heap_insert(&bin->slabs_nonfull, slab);
 2,309 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_insert (78x)
     .           	if (config_stats) {
    78 ( 0.00%)  		bin->stats.nonfull_slabs++;
     .           	}
    20 ( 0.00%)  }
     .           
     .           static void
     .           arena_bin_slabs_nonfull_remove(bin_t *bin, edata_t *slab) {
   180 ( 0.00%)  	edata_heap_remove(&bin->slabs_nonfull, slab);
 1,731 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_remove (60x)
     .           	if (config_stats) {
    60 ( 0.00%)  		bin->stats.nonfull_slabs--;
     .           	}
     .           }
     .           
     .           static edata_t *
     .           arena_bin_slabs_nonfull_tryget(bin_t *bin) {
 2,148 ( 0.00%)  	edata_t *slab = edata_heap_remove_first(&bin->slabs_nonfull);
 4,746 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata.c:_rjem_je_edata_heap_remove_first (716x)
 2,148 ( 0.00%)  	if (slab == NULL) {
     .           		return NULL;
     .           	}
     .           	if (config_stats) {
    18 ( 0.00%)  		bin->stats.reslabs++;
    18 ( 0.00%)  		bin->stats.nonfull_slabs--;
     .           	}
     .           	return slab;
     .           }
     .           
     .           static void
     .           arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, edata_t *slab) {
     .           	assert(edata_nfree_get(slab) == 0);
     .           	/*
     .           	 *  Tracking extents is required by arena_reset, which is not allowed
     .           	 *  for auto arenas.  Bypass this step to avoid touching the edata
     .           	 *  linkage (often results in cache misses) for auto arenas.
     .           	 */
   887 ( 0.00%)  	if (arena_is_auto(arena)) {
     .           		return;
     .           	}
     .           	edata_list_active_append(&bin->slabs_full, slab);
     .           }
     .           
     .           static void
     .           arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
   514 ( 0.00%)  	if (arena_is_auto(arena)) {
     .           		return;
     .           	}
     .           	edata_list_active_remove(&bin->slabs_full, slab);
     .           }
     .           
     .           static void
     .           arena_bin_reset(tsd_t *tsd, arena_t *arena, bin_t *bin) {
     .           	edata_t *slab;
-- line 630 ----------------------------------------
-- line 824 ----------------------------------------
     .           	 * the metadata in this base anymore.
     .           	 */
     .           	arena_prepare_base_deletion(tsd, arena->base);
     .           	base_delete(tsd_tsdn(tsd), arena->base);
     .           }
     .           
     .           static edata_t *
     .           arena_slab_alloc(tsdn_t *tsdn, arena_t *arena, szind_t binind, unsigned binshard,
 5,584 ( 0.00%)      const bin_info_t *bin_info) {
   349 ( 0.00%)  	bool deferred_work_generated = false;
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
     .           	bool guarded = san_slab_extent_decide_guard(tsdn,
     .           	    arena_get_ehooks(arena));
 4,537 ( 0.00%)  	edata_t *slab = pa_alloc(tsdn, &arena->pa_shard, bin_info->slab_size,
673,486 ( 0.19%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_alloc (349x)
     .           	    /* alignment */ PAGE, /* slab */ true, /* szind */ binind,
     .           	     /* zero */ false, guarded, &deferred_work_generated);
     .           
 1,047 ( 0.00%)  	if (deferred_work_generated) {
     .           		arena_handle_deferred_work(tsdn, arena);
     .           	}
     .           
   698 ( 0.00%)  	if (slab == NULL) {
     .           		return NULL;
     .           	}
     .           	assert(edata_slab_get(slab));
     .           
     .           	/* Initialize slab internals. */
     .           	slab_data_t *slab_data = edata_slab_data_get(slab);
     .           	edata_nfree_binshard_set(slab, bin_info->nregs, binshard);
 1,396 ( 0.00%)  	bitmap_init(slab_data->bitmap, &bin_info->bitmap_info, false);
13,043 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/bitmap.c:_rjem_je_bitmap_init (349x)
     .           
     .           	return slab;
 4,188 ( 0.00%)  }
     .           
     .           /*
     .            * Before attempting the _with_fresh_slab approaches below, the _no_fresh_slab
     .            * variants (i.e. through slabcur and nonfull) must be tried first.
     .            */
     .           static void
     .           arena_bin_refill_slabcur_with_fresh_slab(tsdn_t *tsdn, arena_t *arena,
     .               bin_t *bin, szind_t binind, edata_t *fresh_slab) {
-- line 866 ----------------------------------------
-- line 868 ----------------------------------------
     .           	/* Only called after slabcur and nonfull both failed. */
     .           	assert(bin->slabcur == NULL);
     .           	assert(edata_heap_first(&bin->slabs_nonfull) == NULL);
     .           	assert(fresh_slab != NULL);
     .           
     .           	/* A new slab from arena_slab_alloc() */
     .           	assert(edata_nfree_get(fresh_slab) == bin_infos[binind].nregs);
     .           	if (config_stats) {
   349 ( 0.00%)  		bin->stats.nslabs++;
   349 ( 0.00%)  		bin->stats.curslabs++;
     .           	}
   349 ( 0.00%)  	bin->slabcur = fresh_slab;
     .           }
     .           
     .           /* Refill slabcur and then alloc using the fresh slab */
     .           static void *
     .           arena_bin_malloc_with_fresh_slab(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
     .               szind_t binind, edata_t *fresh_slab) {
     .           	malloc_mutex_assert_owner(tsdn, &bin->lock);
     .           	arena_bin_refill_slabcur_with_fresh_slab(tsdn, arena, bin, binind,
-- line 887 ----------------------------------------
-- line 897 ----------------------------------------
     .           	/* Only called after arena_slab_reg_alloc[_batch] failed. */
     .           	assert(bin->slabcur == NULL || edata_nfree_get(bin->slabcur) == 0);
     .           
     .           	if (bin->slabcur != NULL) {
     .           		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
     .           	}
     .           
     .           	/* Look for a usable slab. */
   716 ( 0.00%)  	bin->slabcur = arena_bin_slabs_nonfull_tryget(bin);
     .           	assert(bin->slabcur == NULL || edata_nfree_get(bin->slabcur) > 0);
     .           
    18 ( 0.00%)  	return (bin->slabcur == NULL);
     .           }
     .           
     .           bin_t *
     .           arena_bin_choose(tsdn_t *tsdn, arena_t *arena, szind_t binind,
     .               unsigned *binshard_p) {
     .           	unsigned binshard;
 2,748 ( 0.00%)  	if (tsdn_null(tsdn) || tsd_arena_get(tsdn_tsd(tsdn)) == NULL) {
     .           		binshard = 0;
     .           	} else {
 1,374 ( 0.00%)  		binshard = tsd_binshardsp_get(tsdn_tsd(tsdn))->binshard[binind];
     .           	}
     .           	assert(binshard < bin_infos[binind].n_shards);
     .           	if (binshard_p != NULL) {
     .           		*binshard_p = binshard;
     .           	}
     .           	return arena_get_bin(arena, binind, binshard);
     .           }
     .           
     .           void
     .           arena_cache_bin_fill_small(tsdn_t *tsdn, arena_t *arena,
     .               cache_bin_t *cache_bin, cache_bin_info_t *cache_bin_info, szind_t binind,
12,366 ( 0.00%)      const unsigned nfill) {
     .           	assert(cache_bin_ncached_get_local(cache_bin, cache_bin_info) == 0);
     .           
 3,435 ( 0.00%)  	const bin_info_t *bin_info = &bin_infos[binind];
     .           
     .           	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nfill);
     .           	cache_bin_init_ptr_array_for_fill(cache_bin, cache_bin_info, &ptrs,
     .           	    nfill);
     .           	/*
     .           	 * Bin-local resources are used first: 1) bin->slabcur, and 2) nonfull
     .           	 * slabs.  After both are exhausted, new slabs will be allocated through
     .           	 * arena_slab_alloc().
-- line 941 ----------------------------------------
-- line 956 ----------------------------------------
     .           	 * made_progress below, initialized to true to jump start the first
     .           	 * iteration.
     .           	 *
     .           	 * In other words (again), the loop will only terminate early (i.e. stop
     .           	 * with filled < nfill) after going through the three steps: a) bin
     .           	 * local exhausted, b) unlock and slab_alloc returns null, c) re-lock
     .           	 * and bin local fails again.
     .           	 */
   687 ( 0.00%)  	bool made_progress = true;
   687 ( 0.00%)  	edata_t *fresh_slab = NULL;
     .           	bool alloc_and_retry = false;
   687 ( 0.00%)  	unsigned filled = 0;
     .           	unsigned binshard;
     .           	bin_t *bin = arena_bin_choose(tsdn, arena, binind, &binshard);
     .           
     .           label_refill:
     .           	malloc_mutex_lock(tsdn, &bin->lock);
     .           
 8,407 ( 0.00%)  	while (filled < nfill) {
     .           		/* Try batch-fill from slabcur first. */
 4,623 ( 0.00%)  		edata_t *slabcur = bin->slabcur;
 5,070 ( 0.00%)  		if (slabcur != NULL && edata_nfree_get(slabcur) > 0) {
 3,300 ( 0.00%)  			unsigned tofill = nfill - filled;
     .           			unsigned nfree = edata_nfree_get(slabcur);
 2,475 ( 0.00%)  			unsigned cnt = tofill < nfree ? tofill : nfree;
     .           
     .           			arena_slab_reg_alloc_batch(slabcur, bin_info, cnt,
   825 ( 0.00%)  			    &ptrs.ptr[filled]);
   825 ( 0.00%)  			made_progress = true;
   825 ( 0.00%)  			filled += cnt;
   825 ( 0.00%)  			continue;
     .           		}
     .           		/* Next try refilling slabcur from nonfull slabs. */
     .           		if (!arena_bin_refill_slabcur_no_fresh_slab(tsdn, arena, bin)) {
     .           			assert(bin->slabcur != NULL);
     .           			continue;
     .           		}
     .           
     .           		/* Then see if a new slab was reserved already. */
 2,094 ( 0.00%)  		if (fresh_slab != NULL) {
     .           			arena_bin_refill_slabcur_with_fresh_slab(tsdn, arena,
     .           			    bin, binind, fresh_slab);
     .           			assert(bin->slabcur != NULL);
   349 ( 0.00%)  			fresh_slab = NULL;
     .           			continue;
     .           		}
     .           
     .           		/* Try slab_alloc if made progress (or never did slab_alloc). */
 1,047 ( 0.00%)  		if (made_progress) {
     .           			assert(bin->slabcur == NULL);
     .           			assert(fresh_slab == NULL);
     .           			alloc_and_retry = true;
     .           			/* Alloc a new slab then come back. */
     .           			break;
     .           		}
     .           
     .           		/* OOM. */
     .           
     .           		assert(fresh_slab == NULL);
     .           		assert(!alloc_and_retry);
     .           		break;
     .           	} /* while (filled < nfill) loop. */
     .           
     .           	if (config_stats && !alloc_and_retry) {
 1,374 ( 0.00%)  		bin->stats.nmalloc += filled;
 6,183 ( 0.00%)  		bin->stats.nrequests += cache_bin->tstats.nrequests;
     .           		bin->stats.curregs += filled;
   687 ( 0.00%)  		bin->stats.nfills++;
   687 ( 0.00%)  		cache_bin->tstats.nrequests = 0;
     .           	}
     .           
     .           	malloc_mutex_unlock(tsdn, &bin->lock);
     .           
     .           	if (alloc_and_retry) {
     .           		assert(fresh_slab == NULL);
     .           		assert(filled < nfill);
     .           		assert(made_progress);
     .           
 2,443 ( 0.00%)  		fresh_slab = arena_slab_alloc(tsdn, arena, binind, binshard,
710,261 ( 0.20%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:arena_slab_alloc (349x)
     .           		    bin_info);
     .           		/* fresh_slab NULL case handled in the for loop. */
     .           
     .           		alloc_and_retry = false;
   349 ( 0.00%)  		made_progress = false;
     .           		goto label_refill;
     .           	}
     .           	assert(filled == nfill || (fresh_slab == NULL && !made_progress));
     .           
     .           	/* Release if allocated but not used. */
 1,374 ( 0.00%)  	if (fresh_slab != NULL) {
     .           		assert(edata_nfree_get(fresh_slab) == bin_info->nregs);
     .           		arena_slab_dalloc(tsdn, arena, fresh_slab);
     .           		fresh_slab = NULL;
     .           	}
     .           
     .           	cache_bin_finish_fill(cache_bin, cache_bin_info, &ptrs, filled);
     .           	arena_decay_tick(tsdn, arena);
 7,553 ( 0.00%)  }
     .           
     .           size_t
     .           arena_fill_small_fresh(tsdn_t *tsdn, arena_t *arena, szind_t binind,
     .               void **ptrs, size_t nfill, bool zero) {
     .           	assert(binind < SC_NBINS);
     .           	const bin_info_t *bin_info = &bin_infos[binind];
     .           	const size_t nregs = bin_info->nregs;
     .           	assert(nregs > 0);
-- line 1061 ----------------------------------------
-- line 1184 ----------------------------------------
     .           	}
     .           	arena_decay_tick(tsdn, arena);
     .           
     .           	return ret;
     .           }
     .           
     .           void *
     .           arena_malloc_hard(tsdn_t *tsdn, arena_t *arena, size_t size, szind_t ind,
 7,412 ( 0.00%)      bool zero) {
     .           	assert(!tsdn_null(tsdn) || arena != NULL);
     .           
   872 ( 0.00%)  	if (likely(!tsdn_null(tsdn))) {
     .           		arena = arena_choose_maybe_huge(tsdn_tsd(tsdn), arena, size);
     .           	}
     .           	if (unlikely(arena == NULL)) {
     .           		return NULL;
     .           	}
     .           
   872 ( 0.00%)  	if (likely(size <= SC_SMALL_MAXCLASS)) {
     .           		return arena_malloc_small(tsdn, arena, ind, zero);
     .           	}
 3,488 ( 0.00%)  	return large_malloc(tsdn, arena, sz_index2size(ind), zero);
886,936 ( 0.24%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_malloc (436x)
 3,052 ( 0.00%)  }
     .           
     .           void *
     .           arena_palloc(tsdn_t *tsdn, arena_t *arena, size_t usize, size_t alignment,
    16 ( 0.00%)      bool zero, tcache_t *tcache) {
     .           	void *ret;
     .           
     2 ( 0.00%)  	if (usize <= SC_SMALL_MAXCLASS) {
     .           		/* Small; alignment doesn't require special slab placement. */
     .           
     .           		/* usize should be a result of sz_sa2u() */
     .           		assert((usize & (alignment - 1)) == 0);
     .           
     .           		/*
     .           		 * Small usize can't come from an alignment larger than a page.
     .           		 */
     .           		assert(alignment <= PAGE);
     .           
     .           		ret = arena_malloc(tsdn, arena, usize, sz_size2index(usize),
     .           		    zero, tcache, true);
     .           	} else {
     2 ( 0.00%)  		if (likely(alignment <= CACHELINE)) {
     .           			ret = large_malloc(tsdn, arena, usize, zero);
     .           		} else {
     4 ( 0.00%)  			ret = large_palloc(tsdn, arena, usize, alignment, zero);
 6,834 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_palloc (1x)
     .           		}
     .           	}
     .           	return ret;
     7 ( 0.00%)  }
     .           
     .           void
     .           arena_prof_promote(tsdn_t *tsdn, void *ptr, size_t usize) {
     .           	cassert(config_prof);
     .           	assert(ptr != NULL);
     .           	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
     .           	assert(usize <= SC_SMALL_MAXCLASS);
     .           
-- line 1242 ----------------------------------------
-- line 1289 ----------------------------------------
     .           	} else {
     .           		large_dalloc(tsdn, edata);
     .           	}
     .           }
     .           
     .           static void
     .           arena_dissociate_bin_slab(arena_t *arena, edata_t *slab, bin_t *bin) {
     .           	/* Dissociate slab from bin. */
   604 ( 0.00%)  	if (slab == bin->slabcur) {
   163 ( 0.00%)  		bin->slabcur = NULL;
     .           	} else {
     .           		szind_t binind = edata_szind_get(slab);
     .           		const bin_info_t *bin_info = &bin_infos[binind];
     .           
     .           		/*
     .           		 * The following block's conditional is necessary because if the
     .           		 * slab only contains one region, then it never gets inserted
     .           		 * into the non-full slabs heap.
     .           		 */
   973 ( 0.00%)  		if (bin_info->nregs == 1) {
     .           			arena_bin_slabs_full_remove(arena, bin, slab);
     .           		} else {
     .           			arena_bin_slabs_nonfull_remove(bin, slab);
     .           		}
     .           	}
     .           }
     .           
     .           static void
-- line 1316 ----------------------------------------
-- line 1319 ----------------------------------------
     .           	assert(edata_nfree_get(slab) > 0);
     .           
     .           	/*
     .           	 * Make sure that if bin->slabcur is non-NULL, it refers to the
     .           	 * oldest/lowest non-full slab.  It is okay to NULL slabcur out rather
     .           	 * than proactively keeping it pointing at the oldest/lowest non-full
     .           	 * slab.
     .           	 */
   645 ( 0.00%)  	if (bin->slabcur != NULL && edata_snad_comp(bin->slabcur, slab) > 0) {
     .           		/* Switch slabcur. */
    82 ( 0.00%)  		if (edata_nfree_get(bin->slabcur) > 0) {
     .           			arena_bin_slabs_nonfull_insert(bin, bin->slabcur);
     .           		} else {
     .           			arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
     .           		}
    41 ( 0.00%)  		bin->slabcur = slab;
     .           		if (config_stats) {
    41 ( 0.00%)  			bin->stats.reslabs++;
     .           		}
     .           	} else {
     .           		arena_bin_slabs_nonfull_insert(bin, slab);
     .           	}
     .           }
     .           
     .           static void
     .           arena_dalloc_bin_slab_prepare(tsdn_t *tsdn, edata_t *slab, bin_t *bin) {
     .           	malloc_mutex_assert_owner(tsdn, &bin->lock);
     .           
     .           	assert(slab != bin->slabcur);
     .           	if (config_stats) {
   302 ( 0.00%)  		bin->stats.curslabs--;
     .           	}
     .           }
     .           
     .           void
     .           arena_dalloc_bin_locked_handle_newly_empty(tsdn_t *tsdn, arena_t *arena,
 1,812 ( 0.00%)      edata_t *slab, bin_t *bin) {
     .           	arena_dissociate_bin_slab(arena, slab, bin);
     .           	arena_dalloc_bin_slab_prepare(tsdn, slab, bin);
   906 ( 0.00%)  }
     .           
     .           void
     .           arena_dalloc_bin_locked_handle_newly_nonempty(tsdn_t *tsdn, arena_t *arena,
   693 ( 0.00%)      edata_t *slab, bin_t *bin) {
     .           	arena_bin_slabs_full_remove(arena, bin, slab);
     .           	arena_bin_lower_slab(tsdn, arena, slab, bin);
   396 ( 0.00%)  }
     .           
     .           static void
     .           arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, edata_t *edata, void *ptr) {
     .           	szind_t binind = edata_szind_get(edata);
     .           	unsigned binshard = edata_binshard_get(edata);
     .           	bin_t *bin = arena_get_bin(arena, binind, binshard);
     .           
     .           	malloc_mutex_lock(tsdn, &bin->lock);
-- line 1373 ----------------------------------------
-- line 1389 ----------------------------------------
     .           	arena_t *arena = arena_get_from_edata(edata);
     .           
     .           	arena_dalloc_bin(tsdn, arena, edata, ptr);
     .           	arena_decay_tick(tsdn, arena);
     .           }
     .           
     .           bool
     .           arena_ralloc_no_move(tsdn_t *tsdn, void *ptr, size_t oldsize, size_t size,
63,260 ( 0.02%)      size_t extra, bool zero, size_t *newsize) {
     .           	bool ret;
     .           	/* Calls with non-zero extra had to clamp extra. */
     .           	assert(extra == 0 || size + extra <= SC_LARGE_MAXCLASS);
     .           
     .           	edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global, ptr);
 9,489 ( 0.00%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
 6,326 ( 0.00%)  		ret = true;
     .           		goto done;
     .           	}
     .           
     .           	size_t usize_min = sz_s2u(size);
 3,163 ( 0.00%)  	size_t usize_max = sz_s2u(size + extra);
22,141 ( 0.01%)  	if (likely(oldsize <= SC_SMALL_MAXCLASS && usize_min
     .           	    <= SC_SMALL_MAXCLASS)) {
     .           		/*
     .           		 * Avoid moving the allocation if the size class can be left the
     .           		 * same.
     .           		 */
     .           		assert(bin_infos[sz_size2index(oldsize)].reg_size ==
     .           		    oldsize);
 6,326 ( 0.00%)  		if ((usize_max > SC_SMALL_MAXCLASS
 6,326 ( 0.00%)  		    || sz_size2index(usize_max) != sz_size2index(oldsize))
18,978 ( 0.01%)  		    && (size > oldsize || usize_max < oldsize)) {
     .           			ret = true;
     .           			goto done;
     .           		}
     .           
     .           		arena_t *arena = arena_get_from_edata(edata);
     .           		arena_decay_tick(tsdn, arena);
     .           		ret = false;
     .           	} else if (oldsize >= SC_LARGE_MINCLASS
-- line 1428 ----------------------------------------
-- line 1429 ----------------------------------------
     .           	    && usize_max >= SC_LARGE_MINCLASS) {
     .           		ret = large_ralloc_no_move(tsdn, edata, usize_min, usize_max,
     .           		    zero);
     .           	} else {
     .           		ret = true;
     .           	}
     .           done:
     .           	assert(edata == emap_edata_lookup(tsdn, &arena_emap_global, ptr));
 3,163 ( 0.00%)  	*newsize = edata_usize_get(edata);
     .           
     .           	return ret;
34,793 ( 0.01%)  }
     .           
     .           static void *
     .           arena_ralloc_move_helper(tsdn_t *tsdn, arena_t *arena, size_t usize,
     .               size_t alignment, bool zero, tcache_t *tcache) {
 6,714 ( 0.00%)  	if (alignment == 0) {
     .           		return arena_malloc(tsdn, arena, usize, sz_size2index(usize),
     .           		    zero, tcache, true);
     .           	}
     .           	usize = sz_sa2u(usize, alignment);
     .           	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
     .           		return NULL;
     .           	}
     .           	return ipalloct(tsdn, usize, alignment, zero, tcache, arena);
     .           }
     .           
     .           void *
     .           arena_ralloc(tsdn_t *tsdn, arena_t *arena, void *ptr, size_t oldsize,
     .               size_t size, size_t alignment, bool zero, tcache_t *tcache,
86,710 ( 0.02%)      hook_ralloc_args_t *hook_args) {
 7,540 ( 0.00%)  	size_t usize = alignment == 0 ? sz_s2u(size) : sz_sa2u(size, alignment);
18,850 ( 0.01%)  	if (unlikely(usize == 0 || size > SC_LARGE_MAXCLASS)) {
     .           		return NULL;
     .           	}
     .           
 7,540 ( 0.00%)  	if (likely(usize <= SC_SMALL_MAXCLASS)) {
     .           		/* Try to avoid moving the allocation. */
     .           		UNUSED size_t newsize;
60,704 ( 0.02%)  		if (!arena_ralloc_no_move(tsdn, ptr, oldsize, usize, 0, zero,
338,089 ( 0.09%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_ralloc_no_move (3,163x)
     .           		    &newsize)) {
     .           			hook_invoke_expand(hook_args->is_realloc
     .           			    ? hook_expand_realloc : hook_expand_rallocx,
     .           			    ptr, oldsize, usize, (uintptr_t)ptr,
     .           			    hook_args->args);
     .           			return ptr;
     .           		}
     .           	}
     .           
 2,040 ( 0.00%)  	if (oldsize >= SC_LARGE_MINCLASS
     .           	    && usize >= SC_LARGE_MINCLASS) {
 5,369 ( 0.00%)  		return large_ralloc(tsdn, arena, ptr, usize,
22,343,276 ( 6.16%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_ralloc (413x)
     .           		    alignment, zero, tcache, hook_args);
     .           	}
     .           
     .           	/*
     .           	 * size and oldsize are different enough that we need to move the
     .           	 * object.  In that case, fall back to allocating new space and copying.
     .           	 */
     .           	void *ret = arena_ralloc_move_helper(tsdn, arena, usize, alignment,
     .           	    zero, tcache);
 6,604 ( 0.00%)  	if (ret == NULL) {
     .           		return NULL;
     .           	}
     .           
26,856 ( 0.01%)  	hook_invoke_alloc(hook_args->is_realloc
83,925 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c:_rjem_je_hook_invoke_alloc (3,357x)
     .           	    ? hook_alloc_realloc : hook_alloc_rallocx, ret, (uintptr_t)ret,
 6,714 ( 0.00%)  	    hook_args->args);
23,499 ( 0.01%)  	hook_invoke_dalloc(hook_args->is_realloc
83,925 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c:_rjem_je_hook_invoke_dalloc (3,357x)
     .           	    ? hook_dalloc_realloc : hook_dalloc_rallocx, ptr, hook_args->args);
     .           
     .           	/*
     .           	 * Junk/zero-filling were already done by
     .           	 * ipalloc()/arena_malloc().
     .           	 */
10,071 ( 0.00%)  	size_t copysize = (usize < oldsize) ? usize : oldsize;
     .           	memcpy(ret, ptr, copysize);
     .           	isdalloct(tsdn, ptr, oldsize, tcache, NULL, true);
     .           	return ret;
43,175 ( 0.01%)  }
     .           
     .           ehooks_t *
     .           arena_get_ehooks(arena_t *arena) {
 2,336 ( 0.00%)  	return base_ehooks_get(arena->base);
 1,047 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_base_ehooks_get (349x)
     .           }
     .           
     .           extent_hooks_t *
     .           arena_set_extent_hooks(tsd_t *tsd, arena_t *arena,
     .               extent_hooks_t *extent_hooks) {
     .           	background_thread_info_t *info;
     .           	if (have_background_thread) {
     .           		info = arena_background_thread_info_get(arena);
-- line 1520 ----------------------------------------
-- line 1546 ----------------------------------------
     .           
     .           ssize_t
     .           arena_dirty_decay_ms_default_get(void) {
     .           	return atomic_load_zd(&dirty_decay_ms_default, ATOMIC_RELAXED);
     .           }
     .           
     .           bool
     .           arena_dirty_decay_ms_default_set(ssize_t decay_ms) {
     4 ( 0.00%)  	if (!decay_ms_valid(decay_ms)) {
     9 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/decay.c:_rjem_je_decay_ms_valid (1x)
     .           		return true;
     .           	}
     .           	atomic_store_zd(&dirty_decay_ms_default, decay_ms, ATOMIC_RELAXED);
     .           	return false;
     .           }
     .           
     .           ssize_t
     .           arena_muzzy_decay_ms_default_get(void) {
     .           	return atomic_load_zd(&muzzy_decay_ms_default, ATOMIC_RELAXED);
     .           }
     .           
     .           bool
     .           arena_muzzy_decay_ms_default_set(ssize_t decay_ms) {
     4 ( 0.00%)  	if (!decay_ms_valid(decay_ms)) {
     9 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/decay.c:_rjem_je_decay_ms_valid (1x)
     .           		return true;
     .           	}
     .           	atomic_store_zd(&muzzy_decay_ms_default, decay_ms, ATOMIC_RELAXED);
     2 ( 0.00%)  	return false;
     .           }
     .           
     .           bool
     .           arena_retain_grow_limit_get_set(tsd_t *tsd, arena_t *arena, size_t *old_limit,
     .               size_t *new_limit) {
     .           	assert(opt_retain);
     .           	return pac_retain_grow_limit_get_set(tsd_tsdn(tsd),
     .           	    &arena->pa_shard.pac, old_limit, new_limit);
     .           }
     .           
     .           unsigned
     2 ( 0.00%)  arena_nthreads_get(arena_t *arena, bool internal) {
     .           	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
     2 ( 0.00%)  }
     .           
     .           void
     2 ( 0.00%)  arena_nthreads_inc(arena_t *arena, bool internal) {
     .           	atomic_fetch_add_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
     2 ( 0.00%)  }
     .           
     .           void
     .           arena_nthreads_dec(arena_t *arena, bool internal) {
     .           	atomic_fetch_sub_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
     .           }
     .           
     .           arena_t *
    14 ( 0.00%)  arena_new(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
     .           	arena_t *arena;
     .           	base_t *base;
     .           	unsigned i;
     .           
     2 ( 0.00%)  	if (ind == 0) {
     2 ( 0.00%)  		base = b0get();
     3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_b0get (1x)
     .           	} else {
     .           		base = base_new(tsdn, ind, config->extent_hooks,
     .           		    config->metadata_use_hooks);
     .           		if (base == NULL) {
     .           			return NULL;
     .           		}
     .           	}
     .           
     5 ( 0.00%)  	size_t arena_size = sizeof(arena_t) + sizeof(bin_t) * nbins_total;
     5 ( 0.00%)  	arena = (arena_t *)base_alloc(tsdn, base, arena_size, CACHELINE);
   480 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_base_alloc (1x)
     2 ( 0.00%)  	if (arena == NULL) {
     .           		goto label_error;
     .           	}
     .           
     .           	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
     .           	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
     1 ( 0.00%)  	arena->last_thd = NULL;
     .           
     .           	if (config_stats) {
     .           		if (arena_stats_init(tsdn, &arena->stats)) {
     .           			goto label_error;
     .           		}
     .           
     2 ( 0.00%)  		ql_new(&arena->tcache_ql);
     .           		ql_new(&arena->cache_bin_array_descriptor_ql);
     7 ( 0.00%)  		if (malloc_mutex_init(&arena->tcache_ql_mtx, "tcache_ql",
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           		    WITNESS_RANK_TCACHE_QL, malloc_mutex_rank_exclusive)) {
     .           			goto label_error;
     .           		}
     .           	}
     .           
     1 ( 0.00%)  	atomic_store_u(&arena->dss_prec, (unsigned)extent_dss_prec_get(),
     3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent_dss.c:_rjem_je_extent_dss_prec_get (1x)
     .           	    ATOMIC_RELAXED);
     .           
     .           	edata_list_active_init(&arena->large);
     8 ( 0.00%)  	if (malloc_mutex_init(&arena->large_mtx, "arena_large",
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           	    WITNESS_RANK_ARENA_LARGE, malloc_mutex_rank_exclusive)) {
     .           		goto label_error;
     .           	}
     .           
     .           	nstime_t cur_time;
     3 ( 0.00%)  	nstime_init_update(&cur_time);
    34 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/nstime.c:_rjem_je_nstime_init_update (1x)
    18 ( 0.00%)  	if (pa_shard_init(tsdn, &arena->pa_shard, &arena_pa_central_global,
12,403 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_shard_init (1x)
     .           	    &arena_emap_global, base, ind, &arena->stats.pa_shard_stats,
     .           	    LOCKEDINT_MTX(arena->stats.mtx), &cur_time, oversize_threshold,
     .           	    arena_dirty_decay_ms_default_get(),
     .           	    arena_muzzy_decay_ms_default_get())) {
     .           		goto label_error;
     .           	}
     .           
     .           	/* Initialize bins. */
     .           	atomic_store_u(&arena->binshard_next, 0, ATOMIC_RELEASE);
   113 ( 0.00%)  	for (i = 0; i < nbins_total; i++) {
   216 ( 0.00%)  		bool err = bin_init(&arena->bins[i]);
 5,904 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/bin.c:_rjem_je_bin_init (36x)
    72 ( 0.00%)  		if (err) {
     .           			goto label_error;
     .           		}
     .           	}
     .           
     1 ( 0.00%)  	arena->base = base;
     .           	/* Set arena before creating background threads. */
     3 ( 0.00%)  	arena_set(ind, arena);
     5 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_arena_set (1x)
     1 ( 0.00%)  	arena->ind = ind;
     .           
     2 ( 0.00%)  	nstime_init_update(&arena->create_time);
    34 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/nstime.c:_rjem_je_nstime_init_update (1x)
     .           
     .           	/*
     .           	 * We turn on the HPA if set to.  There are two exceptions:
     .           	 * - Custom extent hooks (we should only return memory allocated from
     .           	 *   them in that case).
     .           	 * - Arena 0 initialization.  In this case, we're mid-bootstrapping, and
     .           	 *   so arena_hpa_global is not yet initialized.
     .           	 */
     3 ( 0.00%)  	if (opt_hpa && ehooks_are_default(base_ehooks_get(base)) && ind != 0) {
     .           		hpa_shard_opts_t hpa_shard_opts = opt_hpa_opts;
     .           		hpa_shard_opts.deferral_allowed = background_thread_enabled();
     .           		if (pa_shard_enable_hpa(tsdn, &arena->pa_shard,
     .           		    &hpa_shard_opts, &opt_hpa_sec_opts)) {
     .           			goto label_error;
     .           		}
     .           	}
     .           
     .           	/* We don't support reentrancy for arena 0 bootstrapping. */
     2 ( 0.00%)  	if (ind != 0) {
     .           		/*
     .           		 * If we're here, then arena 0 already exists, so bootstrapping
     .           		 * is done enough that we should have tsd.
     .           		 */
     .           		assert(!tsdn_null(tsdn));
     .           		pre_reentrancy(tsdn_tsd(tsdn), arena);
     .           		if (test_hooks_arena_new_hook) {
     .           			test_hooks_arena_new_hook();
-- line 1697 ----------------------------------------
-- line 1700 ----------------------------------------
     .           	}
     .           
     .           	return arena;
     .           label_error:
     .           	if (ind != 0) {
     .           		base_delete(tsdn, base);
     .           	}
     .           	return NULL;
    12 ( 0.00%)  }
     .           
     .           arena_t *
     .           arena_choose_huge(tsd_t *tsd) {
     .           	/* huge_arena_ind can be 0 during init (will use a0). */
     .           	if (huge_arena_ind == 0) {
     .           		assert(!malloc_initialized());
     .           	}
     .           
-- line 1716 ----------------------------------------
-- line 1737 ----------------------------------------
     .           			    extent_state_muzzy, 0);
     .           		}
     .           	}
     .           
     .           	return huge_arena;
     .           }
     .           
     .           bool
     3 ( 0.00%)  arena_init_huge(void) {
     .           	bool huge_enabled;
     .           
     .           	/* The threshold should be large size class. */
     5 ( 0.00%)  	if (opt_oversize_threshold > SC_LARGE_MAXCLASS ||
     .           	    opt_oversize_threshold < SC_LARGE_MINCLASS) {
     .           		opt_oversize_threshold = 0;
     1 ( 0.00%)  		oversize_threshold = SC_LARGE_MAXCLASS + PAGE;
     .           		huge_enabled = false;
     .           	} else {
     .           		/* Reserve the index for the huge arena. */
     2 ( 0.00%)  		huge_arena_ind = narenas_total_get();
     3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_narenas_total_get (1x)
     1 ( 0.00%)  		oversize_threshold = opt_oversize_threshold;
     1 ( 0.00%)  		huge_enabled = true;
     .           	}
     .           
     .           	return huge_enabled;
     2 ( 0.00%)  }
     .           
     .           bool
     .           arena_is_huge(unsigned arena_ind) {
     .           	if (huge_arena_ind == 0) {
     .           		return false;
     .           	}
     .           	return (arena_ind == huge_arena_ind);
     .           }
     .           
     .           bool
    12 ( 0.00%)  arena_boot(sc_data_t *sc_data, base_t *base, bool hpa) {
     1 ( 0.00%)  	arena_dirty_decay_ms_default_set(opt_dirty_decay_ms);
     1 ( 0.00%)  	arena_muzzy_decay_ms_default_set(opt_muzzy_decay_ms);
    57 ( 0.00%)  	for (unsigned i = 0; i < SC_NBINS; i++) {
     .           		sc_t *sc = &sc_data->sc[i];
    78 ( 0.00%)  		div_init(&arena_binind_div_info[i],
   288 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/div.c:_rjem_je_div_init (36x)
   219 ( 0.00%)  		    (1U << sc->lg_base) + (sc->ndelta << sc->lg_delta));
     .           	}
     .           
     3 ( 0.00%)  	uint32_t cur_offset = (uint32_t)offsetof(arena_t, bins);
    17 ( 0.00%)  	for (szind_t i = 0; i < SC_NBINS; i++) {
    36 ( 0.00%)  		arena_bin_offsets[i] = cur_offset;
    72 ( 0.00%)  		nbins_total += bin_infos[i].n_shards;
    72 ( 0.00%)  		cur_offset += (uint32_t)(bin_infos[i].n_shards * sizeof(bin_t));
     .           	}
     5 ( 0.00%)  	return pa_central_init(&arena_pa_central_global, base, hpa,
     5 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pa.c:_rjem_je_pa_central_init (1x)
     .           	    &hpa_hooks_default);
     7 ( 0.00%)  }
     .           
     .           void
     .           arena_prefork0(tsdn_t *tsdn, arena_t *arena) {
     .           	pa_shard_prefork0(tsdn, &arena->pa_shard);
     .           }
     .           
     .           void
     .           arena_prefork1(tsdn_t *tsdn, arena_t *arena) {
-- line 1798 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c
--------------------------------------------------------------------------------
Ir              

-- line 82 ----------------------------------------
     .           /******************************************************************************/
     .           
     .           size_t
     .           tcache_salloc(tsdn_t *tsdn, const void *ptr) {
     .           	return arena_salloc(tsdn, ptr);
     .           }
     .           
     .           uint64_t
 1,099 ( 0.00%)  tcache_gc_new_event_wait(tsd_t *tsd) {
     .           	return opt_tcache_gc_incr_bytes;
 2,198 ( 0.00%)  }
     .           
     .           uint64_t
     .           tcache_gc_postponed_event_wait(tsd_t *tsd) {
     .           	return TE_MIN_START_WAIT;
     .           }
     .           
     .           uint64_t
     .           tcache_gc_dalloc_new_event_wait(tsd_t *tsd) {
-- line 100 ----------------------------------------
-- line 105 ----------------------------------------
     .           tcache_gc_dalloc_postponed_event_wait(tsd_t *tsd) {
     .           	return TE_MIN_START_WAIT;
     .           }
     .           
     .           static uint8_t
     .           tcache_gc_item_delay_compute(szind_t szind) {
     .           	assert(szind < SC_NBINS);
     .           	size_t sz = sz_index2size(szind);
 1,974 ( 0.00%)  	size_t item_delay = opt_tcache_gc_delay_bytes / sz;
     .           	size_t delay_max = ZU(1)
     .           	    << (sizeof(((tcache_slow_t *)NULL)->bin_flush_delay_items[0]) * 8);
     .           	if (item_delay >= delay_max) {
     .           		item_delay = delay_max - 1;
     .           	}
 2,668 ( 0.00%)  	return (uint8_t)item_delay;
     .           }
     .           
     .           static void
     .           tcache_gc_small(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
 5,598 ( 0.00%)      szind_t szind) {
     .           	/* Aim to flush 3/4 of items below low-water. */
     .           	assert(szind < SC_NBINS);
     .           
   622 ( 0.00%)  	cache_bin_t *cache_bin = &tcache->bins[szind];
     .           	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin,
     .           	    &tcache_bin_info[szind]);
     .           	cache_bin_sz_t low_water = cache_bin_low_water_get(cache_bin,
     .           	    &tcache_bin_info[szind]);
     .           	assert(!tcache_slow->bin_refilled[szind]);
     .           
 3,110 ( 0.00%)  	size_t nflush = low_water - (low_water >> 2);
 1,866 ( 0.00%)  	if (nflush < tcache_slow->bin_flush_delay_items[szind]) {
     .           		/* Workaround for a conversion warning. */
     .           		uint8_t nflush_uint8 = (uint8_t)nflush;
     .           		assert(sizeof(tcache_slow->bin_flush_delay_items[0]) ==
     .           		    sizeof(nflush_uint8));
     .           		tcache_slow->bin_flush_delay_items[szind] -= nflush_uint8;
     .           		return;
     .           	} else {
     .           		tcache_slow->bin_flush_delay_items[szind]
     .           		    = tcache_gc_item_delay_compute(szind);
     .           	}
     .           
 2,488 ( 0.00%)  	tcache_bin_flush_small(tsd, tcache, cache_bin, szind,
794,032 ( 0.22%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_small (622x)
     .           	    (unsigned)(ncached - nflush));
     .           
     .           	/*
     .           	 * Reduce fill count by 2X.  Limit lg_fill_div such that
     .           	 * the fill count is always at least 1.
     .           	 */
 1,866 ( 0.00%)  	if ((cache_bin_info_ncached_max(&tcache_bin_info[szind])
 1,866 ( 0.00%)  	    >> (tcache_slow->lg_fill_div[szind] + 1)) >= 1) {
   600 ( 0.00%)  		tcache_slow->lg_fill_div[szind]++;
     .           	}
 3,732 ( 0.00%)  }
     .           
     .           static void
     .           tcache_gc_large(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
     .               szind_t szind) {
     .           	/* Like the small GC; flush 3/4 of untouched items. */
     .           	assert(szind >= SC_NBINS);
     .           	cache_bin_t *cache_bin = &tcache->bins[szind];
     .           	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin,
     .           	    &tcache_bin_info[szind]);
     .           	cache_bin_sz_t low_water = cache_bin_low_water_get(cache_bin,
     .           	    &tcache_bin_info[szind]);
   222 ( 0.00%)  	tcache_bin_flush_large(tsd, tcache, cache_bin, szind,
127,497 ( 0.04%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_large (37x)
   185 ( 0.00%)  	    (unsigned)(ncached - low_water + (low_water >> 2)));
    37 ( 0.00%)  }
     .           
     .           static void
     .           tcache_event(tsd_t *tsd) {
     .           	tcache_t *tcache = tcache_get(tsd);
     .           	if (tcache == NULL) {
     .           		return;
     .           	}
     .           
     .           	tcache_slow_t *tcache_slow = tsd_tcache_slowp_get(tsd);
 1,098 ( 0.00%)  	szind_t szind = tcache_slow->next_gc_bin;
     .           	bool is_small = (szind < SC_NBINS);
 3,294 ( 0.00%)  	cache_bin_t *cache_bin = &tcache->bins[szind];
     .           
 8,784 ( 0.00%)  	tcache_bin_flush_stashed(tsd, tcache, cache_bin, szind, is_small);
45,018 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_stashed (1,098x)
     .           
     .           	cache_bin_sz_t low_water = cache_bin_low_water_get(cache_bin,
     .           	    &tcache_bin_info[szind]);
 3,294 ( 0.00%)  	if (low_water > 0) {
 1,110 ( 0.00%)  		if (is_small) {
 1,332 ( 0.00%)  			tcache_gc_small(tsd, tcache_slow, tcache, szind);
424,936 ( 0.12%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:tcache_gc_small (333x)
     .           		} else {
     .           			tcache_gc_large(tsd, tcache_slow, tcache, szind);
     .           		}
 2,722 ( 0.00%)  	} else if (is_small && tcache_slow->bin_refilled[szind]) {
     .           		assert(low_water == 0);
     .           		/*
     .           		 * Increase fill count by 2X for small bins.  Make sure
     .           		 * lg_fill_div stays greater than 0.
     .           		 */
   660 ( 0.00%)  		if (tcache_slow->lg_fill_div[szind] > 1) {
   402 ( 0.00%)  			tcache_slow->lg_fill_div[szind]--;
     .           		}
   477 ( 0.00%)  		tcache_slow->bin_refilled[szind] = false;
     .           	}
     .           	cache_bin_low_water_set(cache_bin);
     .           
 3,294 ( 0.00%)  	tcache_slow->next_gc_bin++;
 2,196 ( 0.00%)  	if (tcache_slow->next_gc_bin == nhbins) {
    25 ( 0.00%)  		tcache_slow->next_gc_bin = 0;
     .           	}
     .           }
     .           
     .           void
10,980 ( 0.00%)  tcache_gc_event_handler(tsd_t *tsd, uint64_t elapsed) {
     .           	assert(elapsed == TE_INVALID_ELAPSED);
     .           	tcache_event(tsd);
 8,784 ( 0.00%)  }
     .           
     .           void
     .           tcache_gc_dalloc_event_handler(tsd_t *tsd, uint64_t elapsed) {
     .           	assert(elapsed == TE_INVALID_ELAPSED);
     .           	tcache_event(tsd);
     .           }
     .           
     .           void *
     .           tcache_alloc_small_hard(tsdn_t *tsdn, arena_t *arena,
     .               tcache_t *tcache, cache_bin_t *cache_bin, szind_t binind,
 6,183 ( 0.00%)      bool *tcache_success) {
   687 ( 0.00%)  	tcache_slow_t *tcache_slow = tcache->tcache_slow;
     .           	void *ret;
     .           
     .           	assert(tcache_slow->arena != NULL);
 2,748 ( 0.00%)  	unsigned nfill = cache_bin_info_ncached_max(&tcache_bin_info[binind])
   687 ( 0.00%)  	    >> tcache_slow->lg_fill_div[binind];
 2,748 ( 0.00%)  	arena_cache_bin_fill_small(tsdn, arena, cache_bin,
1,006,114 ( 0.28%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_cache_bin_fill_small (687x)
     .           	    &tcache_bin_info[binind], binind, nfill);
   687 ( 0.00%)  	tcache_slow->bin_refilled[binind] = true;
     .           	ret = cache_bin_alloc(cache_bin, tcache_success);
     .           
     .           	return ret;
 4,809 ( 0.00%)  }
     .           
     .           static const void *
     .           tcache_bin_flush_ptr_getter(void *arr_ctx, size_t ind) {
     .           	cache_bin_ptr_array_t *arr = (cache_bin_ptr_array_t *)arr_ctx;
 2,991 ( 0.00%)  	return arr->ptr[ind];
     .           }
     .           
     .           static void
     .           tcache_bin_flush_metadata_visitor(void *szind_sum_ctx,
     .               emap_full_alloc_ctx_t *alloc_ctx) {
     .           	size_t *szind_sum = (size_t *)szind_sum_ctx;
     .           	*szind_sum -= alloc_ctx->szind;
     .           	util_prefetch_write_range(alloc_ctx->edata, sizeof(edata_t));
-- line 257 ----------------------------------------
-- line 271 ----------------------------------------
     .           			    /* true_size */ sz_index2size(true_szind),
     .           			    /* input_size */ sz_index2size(szind));
     .           		}
     .           	}
     .           	assert(found_mismatch);
     .           }
     .           
     .           static void
11,397 ( 0.00%)  tcache_bin_flush_edatas_lookup(tsd_t *tsd, cache_bin_ptr_array_t *arr,
     .               szind_t binind, size_t nflush, emap_batch_lookup_result_t *edatas) {
     .           
     .           	/*
     .           	 * This gets compiled away when config_opt_safety_checks is false.
     .           	 * Checks for sized deallocation bugs, failing early rather than
     .           	 * corrupting metadata.
     .           	 */
     .           	size_t szind_sum = binind * nflush;
     .           	emap_edata_lookup_batch(tsd, &arena_emap_global, nflush,
     .           	    &tcache_bin_flush_ptr_getter, (void *)arr,
     .           	    &tcache_bin_flush_metadata_visitor, (void *)&szind_sum,
     .           	    edatas);
     .           	if (config_opt_safety_checks && unlikely(szind_sum != 0)) {
     .           		tcache_bin_flush_size_check_fail(arr, binind, nflush, edatas);
     .           	}
 5,568 ( 0.00%)  }
     .           
     .           JEMALLOC_ALWAYS_INLINE bool
     .           tcache_bin_flush_match(edata_t *edata, unsigned cur_arena_ind,
     .               unsigned cur_binshard, bool small) {
     .           	if (small) {
     .           		return edata_arena_ind_get(edata) == cur_arena_ind
11,460 ( 0.00%)  		    && edata_binshard_get(edata) == cur_binshard;
     .           	} else {
     .           		return edata_arena_ind_get(edata) == cur_arena_ind;
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           tcache_bin_flush_impl(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
   696 ( 0.00%)      szind_t binind, cache_bin_ptr_array_t *ptrs, unsigned nflush, bool small) {
     .           	tcache_slow_t *tcache_slow = tcache->tcache_slow;
     .           	/*
     .           	 * A couple lookup calls take tsdn; declare it once for convenience
     .           	 * instead of calling tsd_tsdn(tsd) all the time.
     .           	 */
     .           	tsdn_t *tsdn = tsd_tsdn(tsd);
     .           
     .           	if (small) {
     .           		assert(binind < SC_NBINS);
     .           	} else {
     .           		assert(binind < nhbins);
     .           	}
 2,784 ( 0.00%)  	arena_t *tcache_arena = tcache_slow->arena;
     .           	assert(tcache_arena != NULL);
     .           
     .           	/*
     .           	 * Variable length array must have > 0 length; the last element is never
     .           	 * touched (it's just included to satisfy the no-zero-length rule).
     .           	 */
12,454 ( 0.00%)  	VARIABLE_ARRAY(emap_batch_lookup_result_t, item_edata, nflush + 1);
 3,554 ( 0.00%)  	tcache_bin_flush_edatas_lookup(tsd, ptrs, binind, nflush, item_edata);
 6,214 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/emap.h:tcache_bin_flush_edatas_lookup.constprop.0 (74x)
     .           
     .           	/*
     .           	 * The slabs where we freed the last remaining object in the slab (and
     .           	 * so need to free the slab itself).
     .           	 * Used only if small == true.
     .           	 */
   622 ( 0.00%)  	unsigned dalloc_count = 0;
 9,330 ( 0.00%)  	VARIABLE_ARRAY(edata_t *, dalloc_slabs, nflush + 1);
     .           
     .           	/*
     .           	 * We're about to grab a bunch of locks.  If one of them happens to be
     .           	 * the one guarding the arena-level stats counters we flush our
     .           	 * thread-local ones to, we do so under one critical section.
     .           	 */
   696 ( 0.00%)  	bool merged_stats = false;
 2,858 ( 0.00%)  	while (nflush > 0) {
     .           		/* Lock the arena, or bin, associated with the first object. */
 1,244 ( 0.00%)  		edata_t *edata = item_edata[0].edata;
     .           		unsigned cur_arena_ind = edata_arena_ind_get(edata);
     .           		arena_t *cur_arena = arena_get(tsdn, cur_arena_ind, false);
     .           
     .           		/*
     .           		 * These assignments are always overwritten when small is true,
     .           		 * and their values are always ignored when small is false, but
     .           		 * to avoid the technical UB when we pass them as parameters, we
     .           		 * need to intialize them.
-- line 357 ----------------------------------------
-- line 370 ----------------------------------------
     .           			 * helpful on the workloads we've looked at, with moving
     .           			 * the bin stats next to the lock seeming to do better.
     .           			 */
     .           		}
     .           
     .           		if (small) {
     .           			malloc_mutex_lock(tsdn, &cur_bin->lock);
     .           		}
   296 ( 0.00%)  		if (!small && !arena_is_auto(cur_arena)) {
     .           			malloc_mutex_lock(tsdn, &cur_arena->large_mtx);
     .           		}
     .           
     .           		/*
     .           		 * If we acquired the right lock and have some stats to flush,
     .           		 * flush them.
     .           		 */
 3,480 ( 0.00%)  		if (config_stats && tcache_arena == cur_arena
 1,392 ( 0.00%)  		    && !merged_stats) {
   696 ( 0.00%)  			merged_stats = true;
     .           			if (small) {
   622 ( 0.00%)  				cur_bin->stats.nflushes++;
 1,866 ( 0.00%)  				cur_bin->stats.nrequests +=
     .           				    cache_bin->tstats.nrequests;
   622 ( 0.00%)  				cache_bin->tstats.nrequests = 0;
     .           			} else {
     .           				arena_stats_large_flush_nrequests_add(tsdn,
     .           				    &tcache_arena->stats, binind,
     .           				    cache_bin->tstats.nrequests);
    74 ( 0.00%)  				cache_bin->tstats.nrequests = 0;
     .           			}
     .           		}
     .           
     .           		/*
     .           		 * Large allocations need special prep done.  Afterwards, we can
     .           		 * drop the large lock.
     .           		 */
     .           		if (!small) {
   553 ( 0.00%)  			for (unsigned i = 0; i < nflush; i++) {
     .           				void *ptr = ptrs->ptr[i];
   200 ( 0.00%)  				edata = item_edata[i].edata;
     .           				assert(ptr != NULL && edata != NULL);
     .           
   252 ( 0.00%)  				if (tcache_bin_flush_match(edata, cur_arena_ind,
     .           				    cur_binshard, small)) {
   526 ( 0.00%)  					large_dalloc_prep_locked(tsdn,
 6,048 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_dalloc_prep_locked (126x)
     .           					    edata);
     .           				}
     .           			}
     .           		}
   370 ( 0.00%)  		if (!small && !arena_is_auto(cur_arena)) {
     .           			malloc_mutex_unlock(tsdn, &cur_arena->large_mtx);
     .           		}
     .           
     .           		/* Deallocate whatever we can. */
     .           		unsigned ndeferred = 0;
     .           		/* Init only to avoid used-uninitialized warning. */
 1,244 ( 0.00%)  		arena_dalloc_bin_locked_info_t dalloc_bin_info = {0};
     .           		if (small) {
     .           			arena_dalloc_bin_locked_begin(&dalloc_bin_info, binind);
     .           		}
10,822 ( 0.00%)  		for (unsigned i = 0; i < nflush; i++) {
 4,457 ( 0.00%)  			void *ptr = ptrs->ptr[i];
 3,930 ( 0.00%)  			edata = item_edata[i].edata;
     .           			assert(ptr != NULL && edata != NULL);
   252 ( 0.00%)  			if (!tcache_bin_flush_match(edata, cur_arena_ind,
     .           			    cur_binshard, small)) {
     .           				/*
     .           				 * The object was allocated either via a
     .           				 * different arena, or a different bin in this
     .           				 * arena.  Either way, stash the object so that
     .           				 * it can be handled in a future pass.
     .           				 */
     .           				ptrs->ptr[ndeferred] = ptr;
-- line 442 ----------------------------------------
-- line 443 ----------------------------------------
     .           				item_edata[ndeferred].edata = edata;
     .           				ndeferred++;
     .           				continue;
     .           			}
     .           			if (small) {
     .           				if (arena_dalloc_bin_locked_step(tsdn,
     .           				    cur_arena, cur_bin, &dalloc_bin_info,
     .           				    binind, edata, ptr)) {
 1,456 ( 0.00%)  					dalloc_slabs[dalloc_count] = edata;
 2,110 ( 0.00%)  					dalloc_count++;
     .           				}
     .           			} else {
     .           				if (large_dalloc_safety_checks(edata, ptr,
     .           				    binind)) {
     .           					/* See the comment in isfree. */
     .           					continue;
     .           				}
   504 ( 0.00%)  				large_dalloc_finish(tsdn, edata);
216,600 ( 0.06%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_dalloc_finish (126x)
     .           			}
     .           		}
     .           
     .           		if (small) {
     .           			arena_dalloc_bin_locked_finish(tsdn, cur_arena, cur_bin,
     .           			    &dalloc_bin_info);
     .           			malloc_mutex_unlock(tsdn, &cur_bin->lock);
     .           		}
   770 ( 0.00%)  		arena_decay_ticks(tsdn, cur_arena, nflush - ndeferred);
     .           		nflush = ndeferred;
     .           	}
     .           
     .           	/* Handle all deferred slab dalloc. */
     .           	assert(small || dalloc_count == 0);
 6,328 ( 0.00%)  	for (unsigned i = 0; i < dalloc_count; i++) {
   302 ( 0.00%)  		edata_t *slab = dalloc_slabs[i];
   604 ( 0.00%)  		arena_slab_dalloc(tsdn, arena_get_from_edata(slab), slab);
333,882 ( 0.09%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_slab_dalloc (302x)
     .           
     .           	}
     .           
 1,392 ( 0.00%)  	if (config_stats && !merged_stats) {
     .           		if (small) {
     .           			/*
     .           			 * The flush loop didn't happen to flush to this
     .           			 * thread's arena, so the stats didn't get merged.
     .           			 * Manually do so now.
     .           			 */
     .           			bin_t *bin = arena_bin_choose(tsdn, tcache_arena,
     .           			    binind, NULL);
-- line 489 ----------------------------------------
-- line 505 ----------------------------------------
     .           JEMALLOC_ALWAYS_INLINE void
     .           tcache_bin_flush_bottom(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
     .               szind_t binind, unsigned rem, bool small) {
     .           	tcache_bin_flush_stashed(tsd, tcache, cache_bin, binind, small);
     .           
     .           	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin,
     .           	    &tcache_bin_info[binind]);
     .           	assert((cache_bin_sz_t)rem <= ncached);
 2,088 ( 0.00%)  	unsigned nflush = ncached - rem;
     .           
   696 ( 0.00%)  	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nflush);
     .           	cache_bin_init_ptr_array_for_flush(cache_bin, &tcache_bin_info[binind],
     .           	    &ptrs, nflush);
     .           
     .           	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nflush,
     .           	    small);
     .           
   696 ( 0.00%)  	cache_bin_finish_flush(cache_bin, &tcache_bin_info[binind], &ptrs,
 2,088 ( 0.00%)  	    ncached - rem);
     .           }
     .           
     .           void
     .           tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
11,818 ( 0.00%)      szind_t binind, unsigned rem) {
     .           	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, true);
 6,842 ( 0.00%)  }
     .           
     .           void
     .           tcache_bin_flush_large(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
 1,332 ( 0.00%)      szind_t binind, unsigned rem) {
     .           	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, false);
   814 ( 0.00%)  }
     .           
     .           /*
     .            * Flushing stashed happens when 1) tcache fill, 2) tcache flush, or 3) tcache
     .            * GC event.  This makes sure that the stashed items do not hold memory for too
     .            * long, and new buffers can only be allocated when nothing is stashed.
     .            *
     .            * The downside is, the time between stash and flush may be relatively short,
     .            * especially when the request rate is high.  It lowers the chance of detecting
     .            * write-after-free -- however that is a delayed detection anyway, and is less
     .            * of a focus than the memory overhead.
     .            */
     .           void
     .           tcache_bin_flush_stashed(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
50,881 ( 0.01%)      szind_t binind, bool is_small) {
18,371 ( 0.01%)  	cache_bin_info_t *info = &tcache_bin_info[binind];
     .           	/*
     .           	 * The two below are for assertion only.  The content of original cached
     .           	 * items remain unchanged -- the stashed items reside on the other end
     .           	 * of the stack.  Checking the stack head and ncached to verify.
     .           	 */
 1,392 ( 0.00%)  	void *head_content = *cache_bin->stack_head;
     .           	cache_bin_sz_t orig_cached = cache_bin_ncached_get_local(cache_bin,
     .           	    info);
     .           
     .           	cache_bin_sz_t nstashed = cache_bin_nstashed_get_local(cache_bin, info);
     .           	assert(orig_cached + nstashed <= cache_bin_info_ncached_max(info));
 8,074 ( 0.00%)  	if (nstashed == 0) {
     .           		return;
     .           	}
     .           
     .           	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nstashed);
     .           	cache_bin_init_ptr_array_for_stashed(cache_bin, binind, info, &ptrs,
     .           	    nstashed);
     .           	san_check_stashed_ptrs(ptrs.ptr, nstashed, sz_index2size(binind));
     .           	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nstashed,
     .           	    is_small);
     .           	cache_bin_finish_flush_stashed(cache_bin, info);
     .           
     .           	assert(cache_bin_nstashed_get_local(cache_bin, info) == 0);
     .           	assert(cache_bin_ncached_get_local(cache_bin, info) == orig_cached);
     .           	assert(head_content == *cache_bin->stack_head);
32,923 ( 0.01%)  }
     .           
     .           void
     .           tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
    14 ( 0.00%)      tcache_t *tcache, arena_t *arena) {
     .           	assert(tcache_slow->arena == NULL);
     1 ( 0.00%)  	tcache_slow->arena = arena;
     .           
     .           	if (config_stats) {
     .           		/* Link into list of extant tcaches. */
     .           		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
     .           
     3 ( 0.00%)  		ql_elm_new(tcache_slow, link);
     4 ( 0.00%)  		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
     1 ( 0.00%)  		cache_bin_array_descriptor_init(
     2 ( 0.00%)  		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
     4 ( 0.00%)  		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
     .           		    &tcache_slow->cache_bin_array_descriptor, link);
     .           
     .           		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
     .           	}
     7 ( 0.00%)  }
     .           
     .           static void
     .           tcache_arena_dissociate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
     .               tcache_t *tcache) {
     .           	arena_t *arena = tcache_slow->arena;
     .           	assert(arena != NULL);
     .           	if (config_stats) {
     .           		/* Unlink from list of extant tcaches. */
-- line 607 ----------------------------------------
-- line 629 ----------------------------------------
     .           void
     .           tcache_arena_reassociate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
     .               tcache_t *tcache, arena_t *arena) {
     .           	tcache_arena_dissociate(tsdn, tcache_slow, tcache);
     .           	tcache_arena_associate(tsdn, tcache_slow, tcache, arena);
     .           }
     .           
     .           bool
     6 ( 0.00%)  tsd_tcache_enabled_data_init(tsd_t *tsd) {
     .           	/* Called upon tsd initialization. */
     1 ( 0.00%)  	tsd_tcache_enabled_set(tsd, opt_tcache);
     1 ( 0.00%)  	tsd_slow_update(tsd);
    20 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tsd.c:_rjem_je_tsd_slow_update (1x)
     .           
     2 ( 0.00%)  	if (opt_tcache) {
     .           		/* Trigger tcache init. */
     2 ( 0.00%)  		tsd_tcache_data_init(tsd);
 9,019 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tsd_tcache_data_init (1x)
     .           	}
     .           
     .           	return false;
     4 ( 0.00%)  }
     .           
     .           static void
    13 ( 0.00%)  tcache_init(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
     .               void *mem) {
     1 ( 0.00%)  	tcache->tcache_slow = tcache_slow;
     1 ( 0.00%)  	tcache_slow->tcache = tcache;
     .           
     .           	memset(&tcache_slow->link, 0, sizeof(ql_elm(tcache_t)));
     1 ( 0.00%)  	tcache_slow->next_gc_bin = 0;
     1 ( 0.00%)  	tcache_slow->arena = NULL;
     2 ( 0.00%)  	tcache_slow->dyn_alloc = mem;
     .           
     .           	/*
     .           	 * We reserve cache bins for all small size classes, even if some may
     .           	 * not get used (i.e. bins higher than nhbins).  This allows the fast
     .           	 * and common paths to access cache bin metadata safely w/o worrying
     .           	 * about which ones are disabled.
     .           	 */
     4 ( 0.00%)  	unsigned n_reserved_bins = nhbins < SC_NBINS ? SC_NBINS : nhbins;
     2 ( 0.00%)  	memset(tcache->bins, 0, sizeof(cache_bin_t) * n_reserved_bins);
     .           
     1 ( 0.00%)  	size_t cur_offset = 0;
     6 ( 0.00%)  	cache_bin_preincrement(tcache_bin_info, nhbins, mem,
     6 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/cache_bin.c:_rjem_je_cache_bin_preincrement (1x)
     .           	    &cur_offset);
   212 ( 0.00%)  	for (unsigned i = 0; i < nhbins; i++) {
    82 ( 0.00%)  		if (i < SC_NBINS) {
    72 ( 0.00%)  			tcache_slow->lg_fill_div[i] = 1;
    36 ( 0.00%)  			tcache_slow->bin_refilled[i] = false;
     .           			tcache_slow->bin_flush_delay_items[i]
     .           			    = tcache_gc_item_delay_compute(i);
     .           		}
    82 ( 0.00%)  		cache_bin_t *cache_bin = &tcache->bins[i];
   246 ( 0.00%)  		cache_bin_init(cache_bin, &tcache_bin_info[i], mem,
   697 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/cache_bin.c:_rjem_je_cache_bin_init (41x)
     .           		    &cur_offset);
     .           	}
     .           	/*
     .           	 * For small size classes beyond tcache_maxclass (i.e. nhbins < NBINS),
     .           	 * their cache bins are initialized to a state to safely and efficiently
     .           	 * fail all fastpath alloc / free, so that no additional check around
     .           	 * nhbins is needed on fastpath.
     .           	 */
     2 ( 0.00%)  	for (unsigned i = nhbins; i < SC_NBINS; i++) {
     .           		/* Disabled small bins. */
     .           		cache_bin_t *cache_bin = &tcache->bins[i];
     .           		void *fake_stack = mem;
     .           		size_t fake_offset = 0;
     .           
     .           		cache_bin_init(cache_bin, &tcache_bin_info[i], fake_stack,
     .           		    &fake_offset);
     .           		assert(tcache_small_bin_disabled(i, cache_bin));
     .           	}
     .           
     5 ( 0.00%)  	cache_bin_postincrement(tcache_bin_info, nhbins, mem,
     6 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/cache_bin.c:_rjem_je_cache_bin_postincrement (1x)
     .           	    &cur_offset);
     .           	/* Sanity check that the whole stack is used. */
     .           	assert(cur_offset == tcache_bin_alloc_size);
    21 ( 0.00%)  }
     .           
     .           /* Initialize auto tcache (embedded in TSD). */
     .           bool
    13 ( 0.00%)  tsd_tcache_data_init(tsd_t *tsd) {
     .           	tcache_slow_t *tcache_slow = tsd_tcache_slowp_get_unsafe(tsd);
     .           	tcache_t *tcache = tsd_tcachep_get_unsafe(tsd);
     .           
     .           	assert(cache_bin_still_zero_initialized(&tcache->bins[0]));
     1 ( 0.00%)  	size_t alignment = tcache_bin_alloc_alignment;
     1 ( 0.00%)  	size_t size = sz_sa2u(tcache_bin_alloc_size, alignment);
     .           
     .           	void *mem = ipallocztm(tsd_tsdn(tsd), size, alignment, true, NULL,
     .           	    true, arena_get(TSDN_NULL, 0, true));
     .           	if (mem == NULL) {
     .           		return true;
     .           	}
     .           
     4 ( 0.00%)  	tcache_init(tsd, tcache_slow, tcache, mem);
 1,907 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:tcache_init.constprop.0 (1x)
     .           	/*
     .           	 * Initialization is a bit tricky here.  After malloc init is done, all
     .           	 * threads can rely on arena_choose and associate tcache accordingly.
     .           	 * However, the thread that does actual malloc bootstrapping relies on
     .           	 * functional tsd, and it can only rely on a0.  In that case, we
     .           	 * associate its tcache to a0 temporarily, and later on
     .           	 * arena_choose_hard() will re-associate properly.
     .           	 */
     1 ( 0.00%)  	tcache_slow->arena = NULL;
     .           	arena_t *arena;
     3 ( 0.00%)  	if (!malloc_initialized()) {
     .           		/* If in initialization, assign to a0. */
     .           		arena = arena_get(tsd_tsdn(tsd), 0, false);
     .           		tcache_arena_associate(tsd_tsdn(tsd), tcache_slow, tcache,
     .           		    arena);
     .           	} else {
     .           		arena = arena_choose(tsd, NULL);
     .           		/* This may happen if thread.tcache.enabled is used. */
     .           		if (tcache_slow->arena == NULL) {
     4 ( 0.00%)  			tcache_arena_associate(tsd_tsdn(tsd), tcache_slow,
   119 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_arena_associate (1x)
     .           			    tcache, arena);
     .           		}
     .           	}
     .           	assert(arena == tcache_slow->arena);
     .           
     1 ( 0.00%)  	return false;
    11 ( 0.00%)  }
     .           
     .           /* Created manual tcache for tcache.create mallctl. */
     .           tcache_t *
     .           tcache_create_explicit(tsd_t *tsd) {
     .           	/*
     .           	 * We place the cache bin stacks, then the tcache_t, then a pointer to
     .           	 * the beginning of the whole allocation (for freeing).  The makes sure
     .           	 * the cache bins have the requested alignment.
-- line 758 ----------------------------------------
-- line 984 ----------------------------------------
     .           	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
     .           	if (tcache != NULL) {
     .           		tcache_destroy(tsd, tcache, false);
     .           	}
     .           }
     .           
     .           static unsigned
     .           tcache_ncached_max_compute(szind_t szind) {
    82 ( 0.00%)  	if (szind >= SC_NBINS) {
     .           		assert(szind < nhbins);
    15 ( 0.00%)  		return opt_tcache_nslots_large;
     .           	}
   288 ( 0.00%)  	unsigned slab_nregs = bin_infos[szind].nregs;
     .           
     .           	/* We may modify these values; start with the opt versions. */
    72 ( 0.00%)  	unsigned nslots_small_min = opt_tcache_nslots_small_min;
     .           	unsigned nslots_small_max = opt_tcache_nslots_small_max;
     .           
     .           	/*
     .           	 * Clamp values to meet our constraints -- even, nonzero, min < max, and
     .           	 * suitable for a cache bin size.
     .           	 */
     .           	if (opt_tcache_nslots_small_max > CACHE_BIN_NCACHED_MAX) {
     .           		nslots_small_max = CACHE_BIN_NCACHED_MAX;
     .           	}
   108 ( 0.00%)  	if (nslots_small_min % 2 != 0) {
    72 ( 0.00%)  		nslots_small_min++;
     .           	}
     .           	if (nslots_small_max % 2 != 0) {
    36 ( 0.00%)  		nslots_small_max--;
     .           	}
     .           	if (nslots_small_min < 2) {
     .           		nslots_small_min = 2;
     .           	}
     .           	if (nslots_small_max < 2) {
     .           		nslots_small_max = 2;
     .           	}
   144 ( 0.00%)  	if (nslots_small_min > nslots_small_max) {
     .           		nslots_small_min = nslots_small_max;
     .           	}
     .           
     .           	unsigned candidate;
    36 ( 0.00%)  	if (opt_lg_tcache_nslots_mul < 0) {
   360 ( 0.00%)  		candidate = slab_nregs >> (-opt_lg_tcache_nslots_mul);
     .           	} else {
     .           		candidate = slab_nregs << opt_lg_tcache_nslots_mul;
     .           	}
    72 ( 0.00%)  	if (candidate % 2 != 0) {
     .           		/*
     .           		 * We need the candidate size to be even -- we assume that we
     .           		 * can divide by two and get a positive number (e.g. when
     .           		 * flushing).
     .           		 */
    72 ( 0.00%)  		++candidate;
     .           	}
   144 ( 0.00%)  	if (candidate <= nslots_small_min) {
     .           		return nslots_small_min;
     .           	} else if (candidate <= nslots_small_max) {
     .           		return candidate;
     .           	} else {
     .           		return nslots_small_max;
     .           	}
     .           }
     .           
     .           bool
     9 ( 0.00%)  tcache_boot(tsdn_t *tsdn, base_t *base) {
     2 ( 0.00%)  	tcache_maxclass = sz_s2u(opt_tcache_max);
     .           	assert(tcache_maxclass <= TCACHE_MAXCLASS_LIMIT);
     2 ( 0.00%)  	nhbins = sz_size2index(tcache_maxclass) + 1;
     .           
     8 ( 0.00%)  	if (malloc_mutex_init(&tcaches_mtx, "tcaches", WITNESS_RANK_TCACHES,
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           	    malloc_mutex_rank_exclusive)) {
     .           		return true;
     .           	}
     .           
     .           	/* Initialize tcache_bin_info.  See comments in tcache_init(). */
     4 ( 0.00%)  	unsigned n_reserved_bins = nhbins < SC_NBINS ? SC_NBINS : nhbins;
     1 ( 0.00%)  	size_t size = n_reserved_bins * sizeof(cache_bin_info_t);
     6 ( 0.00%)  	tcache_bin_info = (cache_bin_info_t *)base_alloc(tsdn, base, size,
   847 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_base_alloc (1x)
     .           	    CACHELINE);
     2 ( 0.00%)  	if (tcache_bin_info == NULL) {
     .           		return true;
     .           	}
     .           
   169 ( 0.00%)  	for (szind_t i = 0; i < nhbins; i++) {
     .           		unsigned ncached_max = tcache_ncached_max_compute(i);
    82 ( 0.00%)  		cache_bin_info_init(&tcache_bin_info[i], ncached_max);
   123 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/cache_bin.c:_rjem_je_cache_bin_info_init (41x)
     .           	}
     2 ( 0.00%)  	for (szind_t i = nhbins; i < SC_NBINS; i++) {
     .           		/* Disabled small bins. */
     .           		cache_bin_info_init(&tcache_bin_info[i], 0);
     .           		assert(tcache_small_bin_disabled(i, NULL));
     .           	}
     .           
    47 ( 0.00%)  	cache_bin_info_compute_alloc(tcache_bin_info, nhbins,
   137 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/cache_bin.c:_rjem_je_cache_bin_info_compute_alloc (1x)
     .           	    &tcache_bin_alloc_size, &tcache_bin_alloc_alignment);
     .           
     .           	return false;
     7 ( 0.00%)  }
     .           
     .           void
     .           tcache_prefork(tsdn_t *tsdn) {
     .           	malloc_mutex_prefork(tsdn, &tcaches_mtx);
     .           }
     .           
     .           void
     .           tcache_postfork_parent(tsdn_t *tsdn) {
-- line 1090 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c
--------------------------------------------------------------------------------
Ir               

-- line 290 ----------------------------------------
      .           	if (unlikely(malloc_init_state == malloc_init_uninitialized)) {
      .           		return malloc_init_hard_a0();
      .           	}
      .           	return false;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE bool
      .           malloc_init(void) {
      4 ( 0.00%)  	if (unlikely(!malloc_initialized()) && malloc_init_hard()) {
 59,077 ( 0.02%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:malloc_init_hard (1x)
      .           		return true;
      .           	}
      .           	return false;
      .           }
      .           
      .           /*
      .            * The a0*() functions are used instead of i{d,}alloc() in situations that
      .            * cannot tolerate TLS variable access.
-- line 306 ----------------------------------------
-- line 364 ----------------------------------------
      .           	if (unlikely(ptr == NULL)) {
      .           		return;
      .           	}
      .           
      .           	a0idalloc(ptr, false);
      .           }
      .           
      .           void
      1 ( 0.00%)  arena_set(unsigned ind, arena_t *arena) {
      .           	atomic_store_p(&arenas[ind], arena, ATOMIC_RELEASE);
      1 ( 0.00%)  }
      .           
      .           static void
      .           narenas_total_set(unsigned narenas) {
      .           	atomic_store_u(&narenas_total, narenas, ATOMIC_RELEASE);
      .           }
      .           
      .           static void
      .           narenas_total_inc(void) {
      .           	atomic_fetch_add_u(&narenas_total, 1, ATOMIC_RELEASE);
      2 ( 0.00%)  }
      .           
      .           unsigned
      1 ( 0.00%)  narenas_total_get(void) {
      .           	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
      1 ( 0.00%)  }
      .           
      .           /* Create a new arena and insert it into the arenas array at index ind. */
      .           static arena_t *
      .           arena_init_locked(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
      .           	arena_t *arena;
      .           
      .           	assert(ind <= narenas_total_get());
      2 ( 0.00%)  	if (ind >= MALLOCX_ARENA_LIMIT) {
      .           		return NULL;
      .           	}
      2 ( 0.00%)  	if (ind == narenas_total_get()) {
      .           		narenas_total_inc();
      .           	}
      .           
      .           	/*
      .           	 * Another thread may have already initialized arenas[ind] if it's an
      .           	 * auto arena.
      .           	 */
      .           	arena = arena_get(tsdn, ind, false);
      2 ( 0.00%)  	if (arena != NULL) {
      .           		assert(arena_is_auto(arena));
      .           		return arena;
      .           	}
      .           
      .           	/* Actually initialize the arena. */
      5 ( 0.00%)  	arena = arena_new(tsdn, ind, config);
 19,632 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_new (1x)
      .           
      1 ( 0.00%)  	return arena;
      .           }
      .           
      .           static void
      .           arena_new_create_background_thread(tsdn_t *tsdn, unsigned ind) {
      2 ( 0.00%)  	if (ind == 0) {
      .           		return;
      .           	}
      .           	/*
      .           	 * Avoid creating a new background thread just for the huge arena, which
      .           	 * purges eagerly by default.
      .           	 */
      .           	if (have_background_thread && !arena_is_huge(ind)) {
      .           		if (background_thread_create(tsdn_tsd(tsdn), ind)) {
-- line 430 ----------------------------------------
-- line 431 ----------------------------------------
      .           			malloc_printf("<jemalloc>: error in background thread "
      .           				      "creation for arena %u. Abort.\n", ind);
      .           			abort();
      .           		}
      .           	}
      .           }
      .           
      .           arena_t *
     12 ( 0.00%)  arena_init(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
      .           	arena_t *arena;
      .           
      .           	malloc_mutex_lock(tsdn, &arenas_lock);
      .           	arena = arena_init_locked(tsdn, ind, config);
      .           	malloc_mutex_unlock(tsdn, &arenas_lock);
      .           
      .           	arena_new_create_background_thread(tsdn, ind);
      .           
      .           	return arena;
      9 ( 0.00%)  }
      .           
      .           static void
      .           arena_bind(tsd_t *tsd, unsigned ind, bool internal) {
      .           	arena_t *arena = arena_get(tsd_tsdn(tsd), ind, false);
      8 ( 0.00%)  	arena_nthreads_inc(arena, internal);
      8 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_nthreads_inc (2x)
      .           
      6 ( 0.00%)  	if (internal) {
      .           		tsd_iarena_set(tsd, arena);
      .           	} else {
      .           		tsd_arena_set(tsd, arena);
      .           		unsigned shard = atomic_fetch_add_u(&arena->binshard_next, 1,
      .           		    ATOMIC_RELAXED);
      .           		tsd_binshards_t *bins = tsd_binshardsp_get(tsd);
     20 ( 0.00%)  		for (unsigned i = 0; i < SC_NBINS; i++) {
      .           			assert(bin_infos[i].n_shards > 0 &&
      .           			    bin_infos[i].n_shards <= BIN_SHARDS_MAX);
    144 ( 0.00%)  			bins->binshard[i] = shard % bin_infos[i].n_shards;
      .           		}
      .           	}
      .           }
      .           
      .           void
      .           arena_migrate(tsd_t *tsd, arena_t *oldarena, arena_t *newarena) {
      .           	assert(oldarena != NULL);
      .           	assert(newarena != NULL);
-- line 474 ----------------------------------------
-- line 495 ----------------------------------------
      .           		tsd_iarena_set(tsd, NULL);
      .           	} else {
      .           		tsd_arena_set(tsd, NULL);
      .           	}
      .           }
      .           
      .           /* Slow path, called only by arena_choose(). */
      .           arena_t *
     14 ( 0.00%)  arena_choose_hard(tsd_t *tsd, bool internal) {
      1 ( 0.00%)  	arena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);
      .           
      3 ( 0.00%)  	if (have_percpu_arena && PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
      .           		unsigned choose = percpu_arena_choose();
      .           		ret = arena_get(tsd_tsdn(tsd), choose, true);
      .           		assert(ret != NULL);
      .           		arena_bind(tsd, arena_ind_get(ret), false);
      .           		arena_bind(tsd, arena_ind_get(ret), true);
      .           
      .           		return ret;
      .           	}
      .           
      3 ( 0.00%)  	if (narenas_auto > 1) {
      .           		unsigned i, j, choose[2], first_null;
      .           		bool is_new_arena[2];
      .           
      .           		/*
      .           		 * Determine binding for both non-internal and internal
      .           		 * allocation.
      .           		 *
      .           		 *   choose[0]: For application allocation.
      .           		 *   choose[1]: For internal metadata allocation.
      .           		 */
      .           
      .           		for (j = 0; j < 2; j++) {
      1 ( 0.00%)  			choose[j] = 0;
      2 ( 0.00%)  			is_new_arena[j] = false;
      .           		}
      .           
      .           		first_null = narenas_auto;
      .           		malloc_mutex_lock(tsd_tsdn(tsd), &arenas_lock);
      .           		assert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);
     52 ( 0.00%)  		for (i = 1; i < narenas_auto; i++) {
     30 ( 0.00%)  			if (arena_get(tsd_tsdn(tsd), i, false) != NULL) {
      .           				/*
      .           				 * Choose the first arena that has the lowest
      .           				 * number of threads assigned to it.
      .           				 */
      .           				for (j = 0; j < 2; j++) {
      .           					if (arena_nthreads_get(arena_get(
      .           					    tsd_tsdn(tsd), i, false), !!j) <
      .           					    arena_nthreads_get(arena_get(
      .           					    tsd_tsdn(tsd), choose[j], false),
      .           					    !!j)) {
      .           						choose[j] = i;
      .           					}
      .           				}
     45 ( 0.00%)  			} else if (first_null == narenas_auto) {
      .           				/*
      .           				 * Record the index of the first uninitialized
      .           				 * arena, in case all extant arenas are in use.
      .           				 *
      .           				 * NB: It is possible for there to be
      .           				 * discontinuities in terms of initialized
      .           				 * versus uninitialized arenas, due to the
      .           				 * "thread.arena" mallctl.
      .           				 */
      .           				first_null = i;
      .           			}
      .           		}
      .           
      .           		for (j = 0; j < 2; j++) {
     16 ( 0.00%)  			if (arena_nthreads_get(arena_get(tsd_tsdn(tsd),
      8 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_nthreads_get (2x)
      .           			    choose[j], false), !!j) == 0 || first_null ==
      .           			    narenas_auto) {
      .           				/*
      .           				 * Use an unloaded arena, or the least loaded
      .           				 * arena if all arenas are already initialized.
      .           				 */
      4 ( 0.00%)  				if (!!j == internal) {
      .           					ret = arena_get(tsd_tsdn(tsd),
      .           					    choose[j], false);
      .           				}
      .           			} else {
      .           				arena_t *arena;
      .           
      .           				/* Initialize a new arena. */
      .           				choose[j] = first_null;
-- line 581 ----------------------------------------
-- line 591 ----------------------------------------
      .           					ret = arena;
      .           				}
      .           			}
      .           			arena_bind(tsd, choose[j], !!j);
      .           		}
      .           		malloc_mutex_unlock(tsd_tsdn(tsd), &arenas_lock);
      .           
      .           		for (j = 0; j < 2; j++) {
      4 ( 0.00%)  			if (is_new_arena[j]) {
      .           				assert(choose[j] > 0);
      .           				arena_new_create_background_thread(
      .           				    tsd_tsdn(tsd), choose[j]);
      .           			}
      .           		}
      .           
      .           	} else {
      .           		ret = arena_get(tsd_tsdn(tsd), 0, false);
      .           		arena_bind(tsd, 0, false);
      .           		arena_bind(tsd, 0, true);
      .           	}
      .           
      .           	return ret;
     12 ( 0.00%)  }
      .           
      .           void
      .           iarena_cleanup(tsd_t *tsd) {
      .           	arena_t *iarena;
      .           
      .           	iarena = tsd_iarena_get(tsd);
      .           	if (iarena != NULL) {
      .           		arena_unbind(tsd, arena_ind_get(iarena), true);
-- line 621 ----------------------------------------
-- line 697 ----------------------------------------
      .           /******************************************************************************/
      .           /*
      .            * Begin initialization functions.
      .            */
      .           
      .           static char *
      .           jemalloc_secure_getenv(const char *name) {
      .           #ifdef JEMALLOC_HAVE_SECURE_GETENV
      5 ( 0.00%)  	return secure_getenv(name);
    621 ( 0.00%)  => ./stdlib/./stdlib/secure-getenv.c:secure_getenv (1x)
      .           #else
      .           #  ifdef JEMALLOC_HAVE_ISSETUGID
      .           	if (issetugid() != 0) {
      .           		return NULL;
      .           	}
      .           #  endif
      .           	return getenv(name);
      .           #endif
-- line 713 ----------------------------------------
-- line 734 ----------------------------------------
      .           	 */
      .           	{
      .           #  if defined(__FreeBSD__) || defined(__DragonFly__)
      .           		cpuset_t set;
      .           #  else
      .           		cpu_set_t set;
      .           #  endif
      .           #  if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
      6 ( 0.00%)  		sched_getaffinity(0, sizeof(set), &set);
     43 ( 0.00%)  => ./posix/../sysdeps/unix/sysv/linux/sched_getaffinity.c:sched_getaffinity@@GLIBC_2.3.4 (1x)
      .           #  else
      .           		pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
      .           #  endif
      4 ( 0.00%)  		result = CPU_COUNT(&set);
    123 ( 0.00%)  => ./posix/./posix/sched_cpucount.c:__sched_cpucount (1x)
      .           	}
      .           #else
      .           	result = sysconf(_SC_NPROCESSORS_ONLN);
      .           #endif
      3 ( 0.00%)  	return ((result == -1) ? 1 : (unsigned)result);
      .           }
      .           
      .           /*
      .            * Ensure that number of CPUs is determistinc, i.e. it is the same based on:
      .            * - sched_getaffinity()
      .            * - _SC_NPROCESSORS_ONLN
      .            * - _SC_NPROCESSORS_CONF
      .            * Since otherwise tricky things is possible with percpu arenas in use.
-- line 759 ----------------------------------------
-- line 959 ----------------------------------------
      .           }
      .           
      .           static void
      .           malloc_slow_flag_init(void) {
      .           	/*
      .           	 * Combine the runtime options into malloc_slow for fast path.  Called
      .           	 * after processing all the options.
      .           	 */
      1 ( 0.00%)  	malloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)
      2 ( 0.00%)  	    | (opt_junk_free ? flag_opt_junk_free : 0)
      4 ( 0.00%)  	    | (opt_zero ? flag_opt_zero : 0)
      3 ( 0.00%)  	    | (opt_utrace ? flag_opt_utrace : 0)
      3 ( 0.00%)  	    | (opt_xmalloc ? flag_opt_xmalloc : 0);
      .           
      1 ( 0.00%)  	malloc_slow = (malloc_slow_flags != 0);
      .           }
      .           
      .           /* Number of sources for initializing malloc_conf */
      .           #define MALLOC_CONF_NSOURCES 5
      .           
      .           static const char *
      .           obtain_malloc_conf(unsigned which_source, char buf[PATH_MAX + 1]) {
      .           	if (config_debug) {
-- line 981 ----------------------------------------
-- line 984 ----------------------------------------
      .           		 * Each source should only be read once, to minimize # of
      .           		 * syscalls on init.
      .           		 */
      .           		assert(read_source++ == which_source);
      .           	}
      .           	assert(which_source < MALLOC_CONF_NSOURCES);
      .           
      .           	const char *ret;
     30 ( 0.00%)  	switch (which_source) {
      .           	case 0:
      .           		ret = config_malloc_conf;
      .           		break;
      .           	case 1:
      3 ( 0.00%)  		if (je_malloc_conf != NULL) {
      .           			/* Use options that were compiled into the program. */
      .           			ret = je_malloc_conf;
      .           		} else {
      .           			/* No configuration specified. */
      .           			ret = NULL;
      .           		}
      .           		break;
      .           	case 2: {
      .           		ssize_t linklen = 0;
      .           #ifndef _WIN32
      3 ( 0.00%)  		int saved_errno = errno;
      5 ( 0.00%)  => ???:0x0000000000115268 (1x)
      .           		const char *linkname =
      .           #  ifdef JEMALLOC_PREFIX
      .           		    "/etc/"JEMALLOC_PREFIX"malloc.conf"
      .           #  else
      .           		    "/etc/malloc.conf"
      .           #  endif
      .           		    ;
      .           
-- line 1016 ----------------------------------------
-- line 1018 ----------------------------------------
      .           		 * Try to use the contents of the "/etc/malloc.conf" symbolic
      .           		 * link's name.
      .           		 */
      .           #ifndef JEMALLOC_READLINKAT
      .           		linklen = readlink(linkname, buf, PATH_MAX);
      .           #else
      .           		linklen = readlinkat(AT_FDCWD, linkname, buf, PATH_MAX);
      .           #endif
      2 ( 0.00%)  		if (linklen == -1) {
      .           			/* No configuration specified. */
      .           			linklen = 0;
      .           			/* Restore errno. */
      .           			set_errno(saved_errno);
      .           		}
      .           #endif
      1 ( 0.00%)  		buf[linklen] = '\0';
      .           		ret = buf;
      1 ( 0.00%)  		break;
      .           	} case 3: {
      .           		const char *envname =
      .           #ifdef JEMALLOC_PREFIX
      .           		    JEMALLOC_CPREFIX"MALLOC_CONF"
      .           #else
      .           		    "MALLOC_CONF"
      .           #endif
      .           		    ;
-- line 1043 ----------------------------------------
-- line 1048 ----------------------------------------
      .           			 * of the MALLOC_CONF environment variable.
      .           			 */
      .           		} else {
      .           			/* No configuration specified. */
      .           			ret = NULL;
      .           		}
      .           		break;
      .           	} case 4: {
      1 ( 0.00%)  		ret = je_malloc_conf_2_conf_harder;
      1 ( 0.00%)  		break;
      .           	} default:
      .           		not_reached();
      .           		ret = NULL;
      .           	}
      .           	return ret;
      .           }
      .           
      .           static void
      .           malloc_conf_init_helper(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
      .               bool initial_call, const char *opts_cache[MALLOC_CONF_NSOURCES],
     36 ( 0.00%)      char buf[PATH_MAX + 1]) {
      .           	static const char *opts_explain[MALLOC_CONF_NSOURCES] = {
      .           		"string specified via --with-malloc-conf",
      .           		"string pointed to by the global variable malloc_conf",
      .           		"\"name\" of the file referenced by the symbolic link named "
      .           		    "/etc/malloc.conf",
      .           		"value of the environment variable MALLOC_CONF",
      .           		"string pointed to by the global variable "
      .           		    "malloc_conf_2_conf_harder",
      .           	};
      .           	unsigned i;
      .           	const char *opts, *k, *v;
      .           	size_t klen, vlen;
      .           
     40 ( 0.00%)  	for (i = 0; i < MALLOC_CONF_NSOURCES; i++) {
      .           		/* Get runtime configuration. */
     30 ( 0.00%)  		if (initial_call) {
     15 ( 0.00%)  			opts_cache[i] = obtain_malloc_conf(i, buf);
      .           		}
     15 ( 0.00%)  		opts = opts_cache[i];
     15 ( 0.00%)  		if (!initial_call && opt_confirm_conf) {
      .           			malloc_printf(
      .           			    "<jemalloc>: malloc_conf #%u (%s): \"%s\"\n",
      .           			    i + 1, opts_explain[i], opts != NULL ? opts : "");
      .           		}
     20 ( 0.00%)  		if (opts == NULL) {
      .           			continue;
      .           		}
      .           
     12 ( 0.00%)  		while (*opts != '\0' && !malloc_conf_next(&opts, &k, &klen, &v,
      .           		    &vlen)) {
      .           
      .           #define CONF_ERROR(msg, k, klen, v, vlen)				\
      .           			if (!initial_call) {				\
      .           				malloc_conf_error(			\
      .           				    msg, k, klen, v, vlen);		\
      .           				cur_opt_valid = false;			\
      .           			}
-- line 1105 ----------------------------------------
-- line 1717 ----------------------------------------
      .           #undef CONF_HANDLE_T_SIGNED
      .           #undef CONF_HANDLE_UNSIGNED
      .           #undef CONF_HANDLE_SIZE_T
      .           #undef CONF_HANDLE_SSIZE_T
      .           #undef CONF_HANDLE_CHAR_P
      .               /* Re-enable diagnostic "-Wtype-limits" */
      .               JEMALLOC_DIAGNOSTIC_POP
      .           		}
      8 ( 0.00%)  		if (opt_abort_conf && had_conf_error) {
      .           			malloc_abort_invalid_conf();
      .           		}
      .           	}
      .           	atomic_store_b(&log_init_done, true, ATOMIC_RELEASE);
     22 ( 0.00%)  }
      .           
      .           static bool
      .           malloc_conf_init_check_deps(void) {
      3 ( 0.00%)  	if (opt_prof_leak_error && !opt_prof_final) {
      .           		malloc_printf("<jemalloc>: prof_leak_error is set w/o "
      .           		    "prof_final.\n");
      .           		return true;
      .           	}
      .           
      .           	return false;
      .           }
      .           
      .           static void
      .           malloc_conf_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS]) {
      4 ( 0.00%)  	const char *opts_cache[MALLOC_CONF_NSOURCES] = {NULL, NULL, NULL, NULL,
      .           		NULL};
      .           	char buf[PATH_MAX + 1];
      .           
      .           	/* The first call only set the confirm_conf option and opts_cache */
      7 ( 0.00%)  	malloc_conf_init_helper(NULL, NULL, true, opts_cache, buf);
    798 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:malloc_conf_init_helper (1x)
      6 ( 0.00%)  	malloc_conf_init_helper(sc_data, bin_shard_sizes, false, opts_cache,
    111 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:malloc_conf_init_helper (1x)
      .           	    NULL);
      .           	if (malloc_conf_init_check_deps()) {
      .           		/* check_deps does warning msg only; abort below if needed. */
      .           		if (opt_abort_conf) {
      2 ( 0.00%)  			malloc_abort_invalid_conf();
      .           		}
      .           	}
      .           }
      .           
      .           #undef MALLOC_CONF_NSOURCES
      .           
      .           static bool
      .           malloc_init_hard_needed(void) {
      7 ( 0.00%)  	if (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==
      4 ( 0.00%)  => ???:0x00000000001152c0 (1x)
      .           	    malloc_init_recursible)) {
      .           		/*
      .           		 * Another thread initialized the allocator before this one
      .           		 * acquired init_lock, or this thread is the initializing
      .           		 * thread, and it is recursively allocating.
      .           		 */
      .           		return false;
      .           	}
      .           #ifdef JEMALLOC_THREADED_INIT
      2 ( 0.00%)  	if (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {
      .           		/* Busy-wait until the initializing thread completes. */
      .           		spin_t spinner = SPIN_INITIALIZER;
      .           		do {
      .           			malloc_mutex_unlock(TSDN_NULL, &init_lock);
      .           			spin_adaptive(&spinner);
      .           			malloc_mutex_lock(TSDN_NULL, &init_lock);
      .           		} while (!malloc_initialized());
      .           		return false;
      .           	}
      .           #endif
      .           	return true;
      .           }
      .           
      .           static bool
     13 ( 0.00%)  malloc_init_hard_a0_locked() {
      2 ( 0.00%)  	malloc_initializer = INITIALIZER;
      4 ( 0.00%)  => ???:0x00000000001152c0 (1x)
      .           
      .           	JEMALLOC_DIAGNOSTIC_PUSH
      .           	JEMALLOC_DIAGNOSTIC_IGNORE_MISSING_STRUCT_FIELD_INITIALIZERS
    827 ( 0.00%)  	sc_data_t sc_data = {0};
      .           	JEMALLOC_DIAGNOSTIC_POP
      .           
      .           	/*
      .           	 * Ordering here is somewhat tricky; we need sc_boot() first, since that
      .           	 * determines what the size classes will be, and then
      .           	 * malloc_conf_init(), since any slab size tweaking will need to be done
      .           	 * before sz_boot and bin_info_boot, which assume that the values they
      .           	 * read out of sc_data_global are final.
      .           	 */
      2 ( 0.00%)  	sc_boot(&sc_data);
 10,321 ( 0.00%)  => ???:_rjem_je_sc_boot (1x)
      .           	unsigned bin_shard_sizes[SC_NBINS];
      3 ( 0.00%)  	bin_shard_sizes_boot(bin_shard_sizes);
     12 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/bin.c:_rjem_je_bin_shard_sizes_boot (1x)
      .           	/*
      .           	 * prof_boot0 only initializes opt_prof_prefix.  We need to do it before
      .           	 * we parse malloc_conf options, in case malloc_conf parsing overwrites
      .           	 * it.
      .           	 */
      .           	if (config_prof) {
      .           		prof_boot0();
      .           	}
      .           	malloc_conf_init(&sc_data, bin_shard_sizes);
      3 ( 0.00%)  	san_init(opt_lg_san_uaf_align);
      6 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/san.c:_rjem_je_san_init (1x)
      3 ( 0.00%)  	sz_boot(&sc_data, opt_cache_oblivious);
  5,780 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/sz.c:_rjem_je_sz_boot (1x)
      3 ( 0.00%)  	bin_info_boot(&sc_data, bin_shard_sizes);
  1,003 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/bin_info.c:_rjem_je_bin_info_boot (1x)
      .           
      3 ( 0.00%)  	if (opt_stats_print) {
      .           		/* Print statistics at exit. */
      .           		if (atexit(stats_print_atexit) != 0) {
      .           			malloc_write("<jemalloc>: Error in atexit()\n");
      .           			if (opt_abort) {
      .           				abort();
      .           			}
      .           		}
      .           	}
      .           
      3 ( 0.00%)  	if (stats_boot()) {
     15 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/stats.c:_rjem_je_stats_boot (1x)
      .           		return true;
      .           	}
      3 ( 0.00%)  	if (pages_boot()) {
  1,349 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/pages.c:_rjem_je_pages_boot (1x)
      .           		return true;
      .           	}
      4 ( 0.00%)  	if (base_boot(TSDN_NULL)) {
  2,123 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_base_boot (1x)
      .           		return true;
      .           	}
      .           	/* emap_global is static, hence zeroed. */
      7 ( 0.00%)  	if (emap_init(&arena_emap_global, b0get(), /* zeroed */ true)) {
    142 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_init (1x)
      3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_b0get (1x)
      .           		return true;
      .           	}
      3 ( 0.00%)  	if (extent_boot()) {
     55 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_boot (1x)
      .           		return true;
      .           	}
      3 ( 0.00%)  	if (ctl_boot()) {
    145 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/ctl.c:_rjem_je_ctl_boot (1x)
      .           		return true;
      .           	}
      .           	if (config_prof) {
      .           		prof_boot1();
      .           	}
      2 ( 0.00%)  	if (opt_hpa && !hpa_supported()) {
      .           		malloc_printf("<jemalloc>: HPA not supported in the current "
      .           		    "configuration; %s.",
      .           		    opt_abort_conf ? "aborting" : "disabling");
      .           		if (opt_abort_conf) {
      .           			malloc_abort_invalid_conf();
      .           		} else {
      .           			opt_hpa = false;
      .           		}
      .           	}
      7 ( 0.00%)  	if (arena_boot(&sc_data, b0get(), opt_hpa)) {
    903 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_boot (1x)
      3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_b0get (1x)
      .           		return true;
      .           	}
      6 ( 0.00%)  	if (tcache_boot(TSDN_NULL, b0get())) {
  3,120 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tcache.c:_rjem_je_tcache_boot (1x)
      3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_b0get (1x)
      .           		return true;
      .           	}
      8 ( 0.00%)  	if (malloc_mutex_init(&arenas_lock, "arenas", WITNESS_RANK_ARENAS,
    132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
      .           	    malloc_mutex_rank_exclusive)) {
      .           		return true;
      .           	}
      2 ( 0.00%)  	hook_boot();
    138 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c:_rjem_je_hook_boot (1x)
      .           	/*
      .           	 * Create enough scaffolding to allow recursive allocation in
      .           	 * malloc_ncpus().
      .           	 */
      1 ( 0.00%)  	narenas_auto = 1;
      1 ( 0.00%)  	manual_arena_base = narenas_auto + 1;
      .           	memset(arenas, 0, sizeof(arena_t *) * narenas_auto);
      .           	/*
      .           	 * Initialize one arena here.  The rest are lazily created in
      .           	 * arena_choose_hard().
      .           	 */
      6 ( 0.00%)  	if (arena_init(TSDN_NULL, 0, &arena_config_default) == NULL) {
 19,747 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_arena_init (1x)
      .           		return true;
      .           	}
      1 ( 0.00%)  	a0 = arena_get(TSDN_NULL, 0, false);
      .           
      2 ( 0.00%)  	if (opt_hpa && !hpa_supported()) {
      .           		malloc_printf("<jemalloc>: HPA not supported in the current "
      .           		    "configuration; %s.",
      .           		    opt_abort_conf ? "aborting" : "disabling");
      .           		if (opt_abort_conf) {
      .           			malloc_abort_invalid_conf();
      .           		} else {
      .           			opt_hpa = false;
      .           		}
-- line 1898 ----------------------------------------
-- line 1900 ----------------------------------------
      .           		hpa_shard_opts_t hpa_shard_opts = opt_hpa_opts;
      .           		hpa_shard_opts.deferral_allowed = background_thread_enabled();
      .           		if (pa_shard_enable_hpa(TSDN_NULL, &a0->pa_shard,
      .           		    &hpa_shard_opts, &opt_hpa_sec_opts)) {
      .           			return true;
      .           		}
      .           	}
      .           
      1 ( 0.00%)  	malloc_init_state = malloc_init_a0_initialized;
      .           
      1 ( 0.00%)  	return false;
     10 ( 0.00%)  }
      .           
      .           static bool
      .           malloc_init_hard_a0(void) {
      .           	bool ret;
      .           
      .           	malloc_mutex_lock(TSDN_NULL, &init_lock);
      .           	ret = malloc_init_hard_a0_locked();
      .           	malloc_mutex_unlock(TSDN_NULL, &init_lock);
      .           	return ret;
      .           }
      .           
      .           /* Initialize data structures which may trigger recursive allocation. */
      .           static bool
      .           malloc_init_hard_recursible(void) {
      1 ( 0.00%)  	malloc_init_state = malloc_init_recursible;
      .           
      1 ( 0.00%)  	ncpus = malloc_ncpus();
      3 ( 0.00%)  	if (opt_percpu_arena != percpu_arena_disabled) {
      .           		bool cpu_count_is_deterministic =
      .           		    malloc_cpu_count_is_deterministic();
      .           		if (!cpu_count_is_deterministic) {
      .           			/*
      .           			 * If # of CPU is not deterministic, and narenas not
      .           			 * specified, disables per cpu arena since it may not
      .           			 * detect CPU IDs properly.
      .           			 */
-- line 1937 ----------------------------------------
-- line 1949 ----------------------------------------
      .           			}
      .           		}
      .           	}
      .           
      .           #if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) && !defined(JEMALLOC_MUTEX_INIT_CB) \
      .               && !defined(JEMALLOC_ZONE) && !defined(_WIN32) && \
      .               !defined(__native_client__))
      .           	/* LinuxThreads' pthread_atfork() allocates. */
      6 ( 0.00%)  	if (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,
     48 ( 0.00%)  => ???:pthread_atfork (1x)
      .           	    jemalloc_postfork_child) != 0) {
      .           		malloc_write("<jemalloc>: Error in pthread_atfork()\n");
      .           		if (opt_abort) {
      .           			abort();
      .           		}
      .           		return true;
      .           	}
      .           #endif
      .           
      3 ( 0.00%)  	if (background_thread_boot0()) {
      5 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/background_thread.c:_rjem_je_background_thread_boot0 (1x)
      .           		return true;
      .           	}
      .           
      .           	return false;
      .           }
      .           
      .           static unsigned
      .           malloc_narenas_default(void) {
      .           	assert(ncpus > 0);
      .           	/*
      .           	 * For SMP systems, create more than one arena per CPU by
      .           	 * default.
      .           	 */
      3 ( 0.00%)  	if (ncpus > 1) {
      2 ( 0.00%)  		fxp_t fxp_ncpus = FXP_INIT_INT(ncpus);
      .           		fxp_t goal = fxp_mul(fxp_ncpus, opt_narenas_ratio);
      .           		uint32_t int_goal = fxp_round_nearest(goal);
      .           		if (int_goal == 0) {
      .           			return 1;
      .           		}
      .           		return int_goal;
      .           	} else {
      .           		return 1;
-- line 1990 ----------------------------------------
-- line 1992 ----------------------------------------
      .           }
      .           
      .           static percpu_arena_mode_t
      .           percpu_arena_as_initialized(percpu_arena_mode_t mode) {
      .           	assert(!malloc_initialized());
      .           	assert(mode <= percpu_arena_disabled);
      .           
      .           	if (mode != percpu_arena_disabled) {
      3 ( 0.00%)  		mode += percpu_arena_mode_enabled_base;
      .           	}
      .           
      .           	return mode;
      .           }
      .           
      .           static bool
      .           malloc_init_narenas(void) {
      .           	assert(ncpus > 0);
      .           
      2 ( 0.00%)  	if (opt_percpu_arena != percpu_arena_disabled) {
      .           		if (!have_percpu_arena || malloc_getcpu() < 0) {
      .           			opt_percpu_arena = percpu_arena_disabled;
      .           			malloc_printf("<jemalloc>: perCPU arena getcpu() not "
      .           			    "available. Setting narenas to %u.\n", opt_narenas ?
      .           			    opt_narenas : malloc_narenas_default());
      .           			if (opt_abort) {
      .           				abort();
      .           			}
-- line 2018 ----------------------------------------
-- line 2051 ----------------------------------------
      .           				 * of affinity setting from numactl), reserving
      .           				 * narenas this way provides a workaround for
      .           				 * percpu_arena.
      .           				 */
      .           				opt_narenas = n;
      .           			}
      .           		}
      .           	}
      3 ( 0.00%)  	if (opt_narenas == 0) {
      1 ( 0.00%)  		opt_narenas = malloc_narenas_default();
      .           	}
      .           	assert(opt_narenas > 0);
      .           
      1 ( 0.00%)  	narenas_auto = opt_narenas;
      .           	/*
      .           	 * Limit the number of arenas to the indexing range of MALLOCX_ARENA().
      .           	 */
      2 ( 0.00%)  	if (narenas_auto >= MALLOCX_ARENA_LIMIT) {
      .           		narenas_auto = MALLOCX_ARENA_LIMIT - 1;
      .           		malloc_printf("<jemalloc>: Reducing narenas to limit (%d)\n",
      .           		    narenas_auto);
      .           	}
      .           	narenas_total_set(narenas_auto);
      3 ( 0.00%)  	if (arena_init_huge()) {
     18 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_init_huge (1x)
      .           		narenas_total_inc();
      .           	}
      1 ( 0.00%)  	manual_arena_base = narenas_total_get();
      .           
      .           	return false;
      .           }
      .           
      .           static void
      .           malloc_init_percpu(void) {
      2 ( 0.00%)  	opt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);
      .           }
      .           
      .           static bool
      .           malloc_init_hard_finish(void) {
      4 ( 0.00%)  	if (malloc_mutex_boot()) {
      3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_boot (1x)
      .           		return true;
      .           	}
      .           
      1 ( 0.00%)  	malloc_init_state = malloc_init_initialized;
      .           	malloc_slow_flag_init();
      .           
      .           	return false;
      .           }
      .           
      .           static void
      .           malloc_init_hard_cleanup(tsdn_t *tsdn, bool reentrancy_set) {
      .           	malloc_mutex_assert_owner(tsdn, &init_lock);
-- line 2101 ----------------------------------------
-- line 2104 ----------------------------------------
      .           		assert(!tsdn_null(tsdn));
      .           		tsd_t *tsd = tsdn_tsd(tsdn);
      .           		assert(tsd_reentrancy_level_get(tsd) > 0);
      .           		post_reentrancy(tsd);
      .           	}
      .           }
      .           
      .           static bool
     11 ( 0.00%)  malloc_init_hard(void) {
      .           	tsd_t *tsd;
      .           
      .           #if defined(_WIN32) && _WIN32_WINNT < 0x0600
      .           	_init_init_lock();
      .           #endif
      .           	malloc_mutex_lock(TSDN_NULL, &init_lock);
      .           
      .           #define UNLOCK_RETURN(tsdn, ret, reentrancy)		\
      .           	malloc_init_hard_cleanup(tsdn, reentrancy);	\
      .           	return ret;
      .           
      .           	if (!malloc_init_hard_needed()) {
      .           		UNLOCK_RETURN(TSDN_NULL, false, false)
      .           	}
      .           
      4 ( 0.00%)  	if (malloc_init_state != malloc_init_a0_initialized &&
      3 ( 0.00%)  	    malloc_init_hard_a0_locked()) {
 46,867 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:malloc_init_hard_a0_locked (1x)
      .           		UNLOCK_RETURN(TSDN_NULL, true, false)
      .           	}
      .           
      .           	malloc_mutex_unlock(TSDN_NULL, &init_lock);
      .           	/* Recursive allocation relies on functional tsd. */
      2 ( 0.00%)  	tsd = malloc_tsd_boot0();
  9,599 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tsd.c:_rjem_je_malloc_tsd_boot0 (1x)
      2 ( 0.00%)  	if (tsd == NULL) {
      .           		return true;
      .           	}
      .           	if (malloc_init_hard_recursible()) {
      .           		return true;
      .           	}
      .           
      .           	malloc_mutex_lock(tsd_tsdn(tsd), &init_lock);
      .           	/* Set reentrancy level to 1 during init. */
      .           	pre_reentrancy(tsd, NULL);
      .           	/* Initialize narenas before prof_boot2 (for allocation). */
      .           	if (malloc_init_narenas()
      6 ( 0.00%)  	    || background_thread_boot1(tsd_tsdn(tsd), b0get())) {
  1,981 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/background_thread.c:_rjem_je_background_thread_boot1 (1x)
      3 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/base.c:_rjem_je_b0get (1x)
      .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
      .           	}
      .           	if (config_prof && prof_boot2(tsd, b0get())) {
      .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
      .           	}
      .           
      .           	malloc_init_percpu();
      .           
-- line 2156 ----------------------------------------
-- line 2157 ----------------------------------------
      .           	if (malloc_init_hard_finish()) {
      .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
      .           	}
      .           	post_reentrancy(tsd);
      .           	malloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);
      .           
      .           	witness_assert_lockless(witness_tsd_tsdn(
      .           	    tsd_witness_tsdp_get_unsafe(tsd)));
      1 ( 0.00%)  	malloc_tsd_boot1();
     46 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tsd.c:_rjem_je_malloc_tsd_boot1 (1x)
      .           	/* Update TSD after tsd_boot1. */
      .           	tsd = tsd_fetch();
      4 ( 0.00%)  	if (opt_background_thread) {
      .           		assert(have_background_thread);
      .           		/*
      .           		 * Need to finish init & unlock first before creating background
      .           		 * threads (pthread_create depends on malloc).  ctl_init (which
      .           		 * sets isthreaded) needs to be called without holding any lock.
      .           		 */
      .           		background_thread_ctl_init(tsd_tsdn(tsd));
      .           		if (background_thread_create(tsd, 0)) {
      .           			return true;
      .           		}
      .           	}
      .           #undef UNLOCK_RETURN
      .           	return false;
     12 ( 0.00%)  }
      .           
      .           /*
      .            * End initialization functions.
      .            */
      .           /******************************************************************************/
      .           /*
      .            * Begin allocation-path internal functions and data structures.
      .            */
-- line 2190 ----------------------------------------
-- line 2290 ----------------------------------------
      .           /*
      .            * ind parameter is optional and is only checked and filled if alignment == 0;
      .            * return true if result is out of range.
      .            */
      .           JEMALLOC_ALWAYS_INLINE bool
      .           aligned_usize_get(size_t size, size_t alignment, size_t *usize, szind_t *ind,
      .               bool bump_empty_aligned_alloc) {
      .           	assert(usize != NULL);
 10,910 ( 0.00%)  	if (alignment == 0) {
      .           		if (ind != NULL) {
      .           			*ind = sz_size2index(size);
  3,792 ( 0.00%)  			if (unlikely(*ind >= SC_NSIZES)) {
      .           				return true;
      .           			}
      .           			*usize = sz_index2size(*ind);
      .           			assert(*usize > 0 && *usize <= SC_LARGE_MAXCLASS);
      .           			return false;
      .           		}
      .           		*usize = sz_s2u(size);
      .           	} else {
      .           		if (bump_empty_aligned_alloc && unlikely(size == 0)) {
      .           			size = 1;
      .           		}
      .           		*usize = sz_sa2u(size, alignment);
      .           	}
 15,080 ( 0.00%)  	if (unlikely(*usize == 0 || *usize > SC_LARGE_MAXCLASS)) {
      .           		return true;
      .           	}
      .           	return false;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE bool
      .           zero_get(bool guarantee, bool slow) {
  3,770 ( 0.00%)  	if (config_fill && slow && unlikely(opt_zero)) {
 15,080 ( 0.00%)  		return true;
      .           	} else {
      .           		return guarantee;
      .           	}
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE tcache_t *
      .           tcache_get_from_ind(tsd_t *tsd, unsigned tcache_ind, bool slow, bool is_alloc) {
      .           	tcache_t *tcache;
      .           	if (tcache_ind == TCACHE_IND_AUTOMATIC) {
  3,370 ( 0.00%)  		if (likely(!slow)) {
      .           			/* Getting tcache ptr unconditionally. */
      .           			tcache = tsd_tcachep_get(tsd);
      .           			assert(tcache == tcache_get(tsd));
      .           		} else if (is_alloc ||
      .           		    likely(tsd_reentrancy_level_get(tsd) == 0)) {
      .           			tcache = tcache_get(tsd);
      .           		} else {
      .           			tcache = NULL;
-- line 2342 ----------------------------------------
-- line 2567 ----------------------------------------
      .           			prof_alloc_rollback(tsd, tctx);
      .           			goto label_oom;
      .           		}
      .           		prof_malloc(tsd, allocation, size, usize, &alloc_ctx, tctx);
      .           	} else {
      .           		assert(!opt_prof);
      .           		allocation = imalloc_no_sample(sopts, dopts, tsd, size, usize,
      .           		    ind);
  3,654 ( 0.00%)  		if (unlikely(allocation == NULL)) {
      .           			goto label_oom;
      .           		}
      .           	}
      .           
      .           	/*
      .           	 * Allocation has been done at this point.  We still have some
      .           	 * post-allocation work to do though.
      .           	 */
-- line 2583 ----------------------------------------
-- line 2596 ----------------------------------------
      .           
      .           	if (sopts->slow) {
      .           		UTRACE(0, size, allocation);
      .           	}
      .           
      .           	/* Success! */
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
      .           	*dopts->result = allocation;
  1,320 ( 0.00%)  	return 0;
      .           
      .           label_oom:
      .           	if (unlikely(sopts->slow) && config_xmalloc && unlikely(opt_xmalloc)) {
      .           		malloc_write(sopts->oom_string);
      .           		abort();
      .           	}
      .           
      .           	if (sopts->slow) {
-- line 2612 ----------------------------------------
-- line 2692 ----------------------------------------
      .           
      .           		sopts->slow = true;
      .           		return imalloc_body(sopts, dopts, tsd);
      .           	}
      .           }
      .           
      .           JEMALLOC_NOINLINE
      .           void *
 18,960 ( 0.01%)  malloc_default(size_t size) {
      .           	void *ret;
      .           	static_opts_t sopts;
      .           	dynamic_opts_t dopts;
      .           
      .           	/*
      .           	 * This variant has logging hook on exit but not on entry.  It's callled
      .           	 * only by je_malloc, below, which emits the entry one for us (and, if
      .           	 * it calls us, does so only via tail call).
-- line 2708 ----------------------------------------
-- line 2727 ----------------------------------------
      .           	if (sopts.slow) {
      .           		uintptr_t args[3] = {size};
      .           		hook_invoke_alloc(hook_alloc_malloc, ret, (uintptr_t)ret, args);
      .           	}
      .           
      .           	LOG("core.malloc.exit", "result: %p", ret);
      .           
      .           	return ret;
 22,752 ( 0.01%)  }
      .           
      .           /******************************************************************************/
      .           /*
      .            * Begin malloc(3)-compatible functions.
      .            */
      .           
      .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
      .           void JEMALLOC_NOTHROW *
      .           JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
249,019 ( 0.07%)  je_malloc(size_t size) {
  1,043 ( 0.00%)  	return imalloc_fastpath(size, &malloc_default);
494,246 ( 0.14%)  }
      .           
      .           JEMALLOC_EXPORT int JEMALLOC_NOTHROW
      .           JEMALLOC_ATTR(nonnull(1))
      .           je_posix_memalign(void **memptr, size_t alignment, size_t size) {
      .           	int ret;
      .           	static_opts_t sopts;
      .           	dynamic_opts_t dopts;
      .           
-- line 2755 ----------------------------------------
-- line 3089 ----------------------------------------
      .           		}
      .           		assert(alloc_ctx.szind != SC_NSIZES);
      .           	} else {
      .           		/*
      .           		 * Check for both sizes that are too large, and for sampled /
      .           		 * special aligned objects.  The alignment check will also check
      .           		 * for null ptr.
      .           		 */
993,050 ( 0.27%)  		if (unlikely(size > SC_LOOKUP_MAXCLASS ||
      .           		    free_fastpath_nonfast_aligned(ptr,
      .           		    /* check_prof */ true))) {
      .           			return false;
      .           		}
      .           		alloc_ctx.szind = sz_size2index_lookup(size);
      .           		/* Max lookup class must be small. */
      .           		assert(alloc_ctx.szind < SC_NBINS);
      .           		/* This is a dead store, except when opt size checking is on. */
-- line 3105 ----------------------------------------
-- line 3111 ----------------------------------------
      .           	 * tcache szind upper limit (i.e. tcache_maxclass) as well.
      .           	 */
      .           	assert(alloc_ctx.slab);
      .           
      .           	uint64_t deallocated, threshold;
      .           	te_free_fastpath_ctx(tsd, &deallocated, &threshold);
      .           
      .           	size_t usize = sz_index2size(alloc_ctx.szind);
742,518 ( 0.20%)  	uint64_t deallocated_after = deallocated + usize;
      .           	/*
      .           	 * Check for events and tsd non-nominal (fast_threshold will be set to
      .           	 * 0) in a single branch.  Note that this handles the uninitialized case
      .           	 * as well (TSD init will be triggered on the non-fastpath).  Therefore
      .           	 * anything depends on a functional TSD (e.g. the alloc_ctx sanity check
      .           	 * below) needs to be after this branch.
      .           	 */
495,012 ( 0.14%)  	if (unlikely(deallocated_after >= threshold)) {
      .           		return false;
      .           	}
      .           	assert(tsd_fast(tsd));
      .           	bool fail = maybe_check_alloc_ctx(tsd, ptr, &alloc_ctx);
      .           	if (fail) {
      .           		/* See the comment in isfree. */
      .           		return true;
      .           	}
-- line 3135 ----------------------------------------
-- line 3144 ----------------------------------------
      .           	 * that to double-check.
      .           	 */
      .           	assert(!opt_junk_free);
      .           
      .           	if (!cache_bin_dalloc_easy(bin, ptr)) {
      .           		return false;
      .           	}
      .           
247,334 ( 0.07%)  	*tsd_thread_deallocatedp_get(tsd) = deallocated_after;
      .           
      .           	return true;
      .           }
      .           
      .           JEMALLOC_EXPORT void JEMALLOC_NOTHROW
      .           je_free(void *ptr) {
      .           	LOG("core.free.entry", "ptr: %p", ptr);
      .           
-- line 3160 ----------------------------------------
-- line 3310 ----------------------------------------
      .            */
      .           /******************************************************************************/
      .           /*
      .            * Begin non-standard functions.
      .            */
      .           
      .           JEMALLOC_ALWAYS_INLINE unsigned
      .           mallocx_tcache_get(int flags) {
 16,365 ( 0.00%)  	if (likely((flags & MALLOCX_TCACHE_MASK) == 0)) {
      .           		return TCACHE_IND_AUTOMATIC;
      .           	} else if ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
      .           		return TCACHE_IND_NONE;
      .           	} else {
      .           		return MALLOCX_TCACHE_GET(flags);
      .           	}
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE unsigned
      .           mallocx_arena_get(int flags) {
  7,540 ( 0.00%)  	if (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {
      .           		return MALLOCX_ARENA_GET(flags);
      .           	} else {
      .           		return ARENA_IND_AUTOMATIC;
      .           	}
      .           }
      .           
      .           #ifdef JEMALLOC_EXPERIMENTAL_SMALLOCX_API
      .           
-- line 3337 ----------------------------------------
-- line 3485 ----------------------------------------
      .           	assert(usize == isalloc(tsd_tsdn(tsd), p));
      .           	prof_realloc(tsd, p, size, usize, tctx, prof_active, old_ptr,
      .           	    old_usize, &old_prof_info, sample_event);
      .           
      .           	return p;
      .           }
      .           
      .           static void *
 56,550 ( 0.02%)  do_rallocx(void *ptr, size_t size, int flags, bool is_realloc) {
      .           	void *p;
      .           	tsd_t *tsd;
      .           	size_t usize;
      .           	size_t old_usize;
 15,080 ( 0.00%)  	size_t alignment = MALLOCX_ALIGN_GET(flags);
      .           	arena_t *arena;
      .           
      .           	assert(ptr != NULL);
      .           	assert(size != 0);
      .           	assert(malloc_initialized() || IS_INITIALIZER);
      .           	tsd = tsd_fetch();
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
      .           
  7,540 ( 0.00%)  	bool zero = zero_get(MALLOCX_ZERO_GET(flags), /* slow */ true);
      .           
      .           	unsigned arena_ind = mallocx_arena_get(flags);
      .           	if (arena_get_from_ind(tsd, arena_ind, &arena)) {
      .           		goto label_oom;
      .           	}
      .           
      .           	unsigned tcache_ind = mallocx_tcache_get(flags);
      .           	tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind,
-- line 3515 ----------------------------------------
-- line 3520 ----------------------------------------
      .           	    &alloc_ctx);
      .           	assert(alloc_ctx.szind != SC_NSIZES);
      .           	old_usize = sz_index2size(alloc_ctx.szind);
      .           	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
      .           	if (aligned_usize_get(size, alignment, &usize, NULL, false)) {
      .           		goto label_oom;
      .           	}
      .           
 18,850 ( 0.01%)  	hook_ralloc_args_t hook_args = {is_realloc, {(uintptr_t)ptr, size,
      .           		flags, 0}};
      .           	if (config_prof && opt_prof) {
      .           		p = irallocx_prof(tsd, ptr, old_usize, size, alignment, usize,
      .           		    zero, tcache, arena, &alloc_ctx, &hook_args);
      .           		if (unlikely(p == NULL)) {
      .           			goto label_oom;
      .           		}
      .           	} else {
  3,770 ( 0.00%)  		p = iralloct(tsd_tsdn(tsd), ptr, old_usize, size, alignment,
      .           		    zero, tcache, arena, &hook_args);
 18,850 ( 0.01%)  		if (unlikely(p == NULL)) {
      .           			goto label_oom;
      .           		}
      .           		assert(usize == isalloc(tsd_tsdn(tsd), p));
      .           	}
      .           	assert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));
      .           	thread_alloc_event(tsd, usize);
      .           	thread_dalloc_event(tsd, old_usize);
      .           
      .           	UTRACE(ptr, size, p);
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
      .           
 11,310 ( 0.00%)  	if (config_fill && unlikely(opt_junk_alloc) && usize > old_usize
  3,770 ( 0.00%)  	    && !zero) {
      .           		size_t excess_len = usize - old_usize;
      .           		void *excess_start = (void *)((uintptr_t)p + old_usize);
      .           		junk_alloc_callback(excess_start, excess_len);
      .           	}
      .           
      .           	return p;
      .           label_oom:
      .           	if (config_xmalloc && unlikely(opt_xmalloc)) {
      .           		malloc_write("<jemalloc>: Error in rallocx(): out of memory\n");
      .           		abort();
      .           	}
      .           	UTRACE(ptr, size, 0);
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
      .           
      .           	return NULL;
 45,240 ( 0.01%)  }
      .           
      .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
      .           void JEMALLOC_NOTHROW *
      .           JEMALLOC_ALLOC_SIZE(2)
      .           je_rallocx(void *ptr, size_t size, int flags) {
      .           	LOG("core.rallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
      .           	    size, flags);
      .           	void *ret = do_rallocx(ptr, size, flags, false);
-- line 3576 ----------------------------------------
-- line 3585 ----------------------------------------
      .           	}
      .           	if (opt_zero_realloc_action == zero_realloc_action_alloc) {
      .           		/*
      .           		 * The user might have gotten an alloc setting while expecting a
      .           		 * free setting.  If that's the case, we at least try to
      .           		 * reduce the harm, and turn off the tcache while allocating, so
      .           		 * that we'll get a true first fit.
      .           		 */
  7,540 ( 0.00%)  		return do_rallocx(ptr, 1, MALLOCX_TCACHE_NONE, true);
26,630,256 ( 7.34%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:do_rallocx (3,770x)
      .           	} else if (opt_zero_realloc_action == zero_realloc_action_free) {
      .           		UTRACE(ptr, 0, 0);
      .           		tsd_t *tsd = tsd_fetch();
      .           		check_entry_exit_locking(tsd_tsdn(tsd));
      .           
      .           		tcache_t *tcache = tcache_get_from_ind(tsd,
      .           		    TCACHE_IND_AUTOMATIC, /* slow */ true,
      .           		    /* is_alloc */ false);
-- line 3601 ----------------------------------------
-- line 3615 ----------------------------------------
      .           		 */
      .           		return NULL;
      .           	}
      .           }
      .           
      .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
      .           void JEMALLOC_NOTHROW *
      .           JEMALLOC_ALLOC_SIZE(2)
 52,780 ( 0.01%)  je_realloc(void *ptr, size_t size) {
      .           	LOG("core.realloc.entry", "ptr: %p, size: %zu\n", ptr, size);
      .           
 22,620 ( 0.01%)  	if (likely(ptr != NULL && size != 0)) {
 18,850 ( 0.01%)  		void *ret = do_rallocx(ptr, size, 0, true);
      .           		LOG("core.realloc.exit", "result: %p", ret);
      .           		return ret;
      .           	} else if (ptr != NULL && size == 0) {
      .           		void *ret = do_realloc_nonnull_zero(ptr);
      .           		LOG("core.realloc.exit", "result: %p", ret);
      .           		return ret;
      .           	} else {
      .           		/* realloc(NULL, size) is equivalent to malloc(size). */
-- line 3635 ----------------------------------------
-- line 3654 ----------------------------------------
      .           		if (sopts.slow) {
      .           			uintptr_t args[3] = {(uintptr_t)ptr, size};
      .           			hook_invoke_alloc(hook_alloc_realloc, ret,
      .           			    (uintptr_t)ret, args);
      .           		}
      .           		LOG("core.realloc.exit", "result: %p", ret);
      .           		return ret;
      .           	}
 26,390 ( 0.01%)  }
      .           
      .           JEMALLOC_ALWAYS_INLINE size_t
      .           ixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
      .               size_t extra, size_t alignment, bool zero) {
      .           	size_t newsize;
      .           
      .           	if (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero,
      .           	    &newsize)) {
-- line 3670 ----------------------------------------
-- line 3894 ----------------------------------------
      .           	LOG("core.dallocx.exit", "");
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE size_t
      .           inallocx(tsdn_t *tsdn, size_t size, int flags) {
      .           	check_entry_exit_locking(tsdn);
      .           	size_t usize;
      .           	/* In case of out of range, let the user see it rather than fail. */
  6,740 ( 0.00%)  	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
      .           	check_entry_exit_locking(tsdn);
      .           	return usize;
      .           }
      .           
      .           JEMALLOC_NOINLINE void
 25,275 ( 0.01%)  sdallocx_default(void *ptr, size_t size, int flags) {
      .           	assert(ptr != NULL);
      .           	assert(malloc_initialized() || IS_INITIALIZER);
      .           
      .           	tsd_t *tsd = tsd_fetch_min();
      .           	bool fast = tsd_fast(tsd);
      .           	size_t usize = inallocx(tsd_tsdn(tsd), size, flags);
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
      .           
-- line 3916 ----------------------------------------
-- line 3923 ----------------------------------------
      .           		tsd_assert_fast(tsd);
      .           		isfree(tsd, ptr, usize, tcache, false);
      .           	} else {
      .           		uintptr_t args_raw[3] = {(uintptr_t)ptr, size, flags};
      .           		hook_invoke_dalloc(hook_dalloc_sdallocx, ptr, args_raw);
      .           		isfree(tsd, ptr, usize, tcache, true);
      .           	}
      .           	check_entry_exit_locking(tsd_tsdn(tsd));
 19,334 ( 0.01%)  }
      .           
      .           JEMALLOC_EXPORT void JEMALLOC_NOTHROW
249,019 ( 0.07%)  je_sdallocx(void *ptr, size_t size, int flags) {
      .           	LOG("core.sdallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
      .           		size, flags);
      .           
      .           	if (flags != 0 || !free_fastpath(ptr, size, true)) {
  1,685 ( 0.00%)  		sdallocx_default(ptr, size, flags);
1,597,035 ( 0.44%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_sdallocx_default (1,685x)
      .           	}
      .           
      .           	LOG("core.sdallocx.exit", "");
247,334 ( 0.07%)  }
      .           
      .           void JEMALLOC_NOTHROW
      .           je_sdallocx_noflags(void *ptr, size_t size) {
      .           	LOG("core.sdallocx.entry", "ptr: %p, size: %zu, flags: 0", ptr,
      .           		size);
      .           
      .           	if (!free_fastpath(ptr, size, true)) {
      .           		sdallocx_default(ptr, size, 0);
-- line 3951 ----------------------------------------
-- line 4313 ----------------------------------------
      .            * the allocator isn't fully initialized at fork time.  The following library
      .            * constructor is a partial solution to this problem.  It may still be possible
      .            * to trigger the deadlock described above, but doing so would involve forking
      .            * via a library constructor that runs before jemalloc's runs.
      .            */
      .           #ifndef JEMALLOC_JET
      .           JEMALLOC_ATTR(constructor)
      .           static void
      1 ( 0.00%)  jemalloc_constructor(void) {
      .           	malloc_init();
      .           }
      .           #endif
      .           
      .           #ifndef JEMALLOC_MUTEX_INIT_CB
      .           void
      .           jemalloc_prefork(void)
      .           #else
-- line 4329 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/tsd.h
--------------------------------------------------------------------------------
Ir              

-- line 261 ----------------------------------------
     .           JEMALLOC_ALWAYS_INLINE uint8_t
     .           tsd_state_get(tsd_t *tsd) {
     .           	/*
     .           	 * This should be atomic.  Unfortunately, compilers right now can't tell
     .           	 * that this can be done as a memory comparison, and forces a load into
     .           	 * a register that hurts fast-path performance.
     .           	 */
     .           	/* return atomic_load_u8(&tsd->state, ATOMIC_RELAXED); */
12,836 ( 0.00%)  	return *(uint8_t *)&tsd->state;
     .           }
     .           
     .           /*
     .            * Wrapper around tsd_t that makes it possible to avoid implicit conversion
     .            * between tsd_t and tsdn_t, where tsdn_t is "nullable" and has to be
     .            * explicitly converted to tsd_t, which is non-nullable.
     .            */
     .           struct tsdn_s {
-- line 277 ----------------------------------------
-- line 315 ----------------------------------------
     .            * foo.  This omits some safety checks, and so can be used during tsd
     .            * initialization and cleanup.
     .            */
     .           #define O(n, t, nt)							\
     .           JEMALLOC_ALWAYS_INLINE t *						\
     .           tsd_##n##p_get_unsafe(tsd_t *tsd) {					\
     .           	return &tsd->TSD_MANGLE(n);					\
     .           }
41,778 ( 0.01%)  TSD_DATA_SLOW
56,893 ( 0.02%)  TSD_DATA_FAST
     .           TSD_DATA_SLOWER
     .           #undef O
     .           
     .           /* tsd_foop_get(tsd) returns a pointer to the thread-local instance of foo. */
     .           #define O(n, t, nt)							\
     .           JEMALLOC_ALWAYS_INLINE t *						\
     .           tsd_##n##p_get(tsd_t *tsd) {						\
     .           	/*								\
-- line 332 ----------------------------------------
-- line 365 ----------------------------------------
     .           #undef O
     .           
     .           /* tsd_foo_get(tsd) returns the value of the thread-local instance of foo. */
     .           #define O(n, t, nt)							\
     .           JEMALLOC_ALWAYS_INLINE t						\
     .           tsd_##n##_get(tsd_t *tsd) {						\
     .           	return *tsd_##n##p_get(tsd);					\
     .           }
 6,033 ( 0.00%)  TSD_DATA_SLOW
 2,196 ( 0.00%)  TSD_DATA_FAST
     .           TSD_DATA_SLOWER
     .           #undef O
     .           
     .           /* tsd_foo_set(tsd, val) updates the thread-local instance of foo to be val. */
     .           #define O(n, t, nt)							\
     .           JEMALLOC_ALWAYS_INLINE void						\
     .           tsd_##n##_set(tsd_t *tsd, t val) {					\
     .           	assert(tsd_state_get(tsd) != tsd_state_reincarnated &&		\
     .           	    tsd_state_get(tsd) != tsd_state_minimal_initialized);	\
     .           	*tsd_##n##p_get(tsd) = val;					\
     .           }
     5 ( 0.00%)  TSD_DATA_SLOW
     .           TSD_DATA_FAST
     .           TSD_DATA_SLOWER
     .           #undef O
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           tsd_assert_fast(tsd_t *tsd) {
     .           	/*
     .           	 * Note that our fastness assertion does *not* include global slowness
-- line 394 ----------------------------------------
-- line 413 ----------------------------------------
     .           tsd_fetch_impl(bool init, bool minimal) {
     .           	tsd_t *tsd = tsd_get(init);
     .           
     .           	if (!init && tsd_get_allocates() && tsd == NULL) {
     .           		return NULL;
     .           	}
     .           	assert(tsd != NULL);
     .           
14,708 ( 0.00%)  	if (unlikely(tsd_state_get(tsd) != tsd_state_nominal)) {
     3 ( 0.00%)  		return tsd_fetch_slow(tsd, minimal);
 9,423 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tsd.c:_rjem_je_tsd_fetch_slow (1x)
     .           	}
     .           	assert(tsd_fast(tsd));
     .           	tsd_assert_fast(tsd);
     .           
16,792 ( 0.00%)  	return tsd;
     .           }
     .           
     .           /* Get a minimal TSD that requires no cleanup.  See comments in free(). */
     .           JEMALLOC_ALWAYS_INLINE tsd_t *
     .           tsd_fetch_min(void) {
     .           	return tsd_fetch_impl(true, true);
     .           }
     .           
-- line 435 ----------------------------------------
-- line 471 ----------------------------------------
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE rtree_ctx_t *
     .           tsdn_rtree_ctx(tsdn_t *tsdn, rtree_ctx_t *fallback) {
     .           	/*
     .           	 * If tsd cannot be accessed, initialize the fallback rtree_ctx and
     .           	 * return a pointer to it.
     .           	 */
35,544 ( 0.01%)  	if (unlikely(tsdn_null(tsdn))) {
     .           		rtree_ctx_data_init(fallback);
     .           		return fallback;
     .           	}
     .           	return tsd_rtree_ctx(tsdn_tsd(tsdn));
     .           }
     .           
     .           static inline bool
     .           tsd_state_nocleanup(tsd_t *tsd) {
-- line 487 ----------------------------------------
-- line 493 ----------------------------------------
     .            * These "raw" tsd reentrancy functions don't have any debug checking to make
     .            * sure that we're not touching arena 0.  Better is to call pre_reentrancy and
     .            * post_reentrancy if this is possible.
     .            */
     .           static inline void
     .           tsd_pre_reentrancy_raw(tsd_t *tsd) {
     .           	bool fast = tsd_fast(tsd);
     .           	assert(tsd_reentrancy_level_get(tsd) < INT8_MAX);
     1 ( 0.00%)  	++*tsd_reentrancy_levelp_get(tsd);
     2 ( 0.00%)  	if (fast) {
     .           		/* Prepare slow path for reentrancy. */
     .           		tsd_slow_update(tsd);
     .           		assert(tsd_state_get(tsd) == tsd_state_nominal_slow);
     .           	}
     .           }
     .           
     .           static inline void
     .           tsd_post_reentrancy_raw(tsd_t *tsd) {
     .           	int8_t *reentrancy_level = tsd_reentrancy_levelp_get(tsd);
     .           	assert(*reentrancy_level > 0);
     2 ( 0.00%)  	if (--*reentrancy_level == 0) {
     3 ( 0.00%)  		tsd_slow_update(tsd);
    39 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/tsd.c:_rjem_je_tsd_slow_update (1x)
     .           	}
     .           }
     .           
     .           #endif /* JEMALLOC_INTERNAL_TSD_H */

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c
--------------------------------------------------------------------------------
Ir              

-- line 55 ----------------------------------------
     .           extent_may_force_decay(pac_t *pac) {
     .           	return !(pac_decay_ms_get(pac, extent_state_dirty) == -1
     .           	    || pac_decay_ms_get(pac, extent_state_muzzy) == -1);
     .           }
     .           
     .           static bool
     .           extent_try_delayed_coalesce(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               ecache_t *ecache, edata_t *edata) {
   120 ( 0.00%)  	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
 1,578 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (24x)
     .           
     .           	bool coalesced;
     .           	edata = extent_try_coalesce(tsdn, pac, ehooks, ecache,
     .           	    edata, &coalesced);
   120 ( 0.00%)  	emap_update_edata_state(tsdn, pac->emap, edata, ecache->state);
 1,848 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (24x)
     .           
    48 ( 0.00%)  	if (!coalesced) {
     .           		return true;
     .           	}
    63 ( 0.00%)  	eset_insert(&ecache->eset, edata);
 4,095 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_insert (21x)
    21 ( 0.00%)  	return false;
     .           }
     .           
     .           edata_t *
     .           ecache_alloc(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
     .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
12,957 ( 0.00%)      bool guarded) {
     .           	assert(size != 0);
     .           	assert(alignment != 0);
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
 1,851 ( 0.00%)  	bool commit = true;
16,659 ( 0.00%)  	edata_t *edata = extent_recycle(tsdn, pac, ehooks, ecache, expand_edata,
1,929,110 ( 0.53%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_recycle (1,851x)
     .           	    size, alignment, zero, &commit, false, guarded);
     .           	assert(edata == NULL || edata_pai_get(edata) == EXTENT_PAI_PAC);
     .           	assert(edata == NULL || edata_guarded_get(edata) == guarded);
     .           	return edata;
 9,255 ( 0.00%)  }
     .           
     .           edata_t *
     .           ecache_alloc_grow(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
     .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
 8,120 ( 0.00%)      bool guarded) {
     .           	assert(size != 0);
     .           	assert(alignment != 0);
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
   406 ( 0.00%)  	bool commit = true;
 1,218 ( 0.00%)  	edata_t *edata = extent_alloc_retained(tsdn, pac, ehooks, expand_edata,
     .           	    size, alignment, zero, &commit, guarded);
    27 ( 0.00%)  	if (edata == NULL) {
 1,290 ( 0.00%)  		if (opt_retain && expand_edata != NULL) {
     .           			/*
     .           			 * When retain is enabled and trying to expand, we do
     .           			 * not attempt extent_alloc_wrapper which does mmap that
     .           			 * is very unlikely to succeed (unless it happens to be
     .           			 * at the end).
     .           			 */
     .           			return NULL;
     .           		}
-- line 115 ----------------------------------------
-- line 125 ----------------------------------------
     .           		    edata_past_get(expand_edata);
     .           		edata = extent_alloc_wrapper(tsdn, pac, ehooks, new_addr,
     .           		    size, alignment, zero, &commit,
     .           		    /* growing_retained */ false);
     .           	}
     .           
     .           	assert(edata == NULL || edata_pai_get(edata) == EXTENT_PAI_PAC);
     .           	return edata;
 4,872 ( 0.00%)  }
     .           
     .           void
     .           ecache_dalloc(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
 1,110 ( 0.00%)      edata_t *edata) {
     .           	assert(edata_base_get(edata) != NULL);
     .           	assert(edata_size_get(edata) != 0);
     .           	assert(edata_pai_get(edata) == EXTENT_PAI_PAC);
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
     .           	edata_addr_set(edata, edata_base_get(edata));
     .           	edata_zeroed_set(edata, false);
     .           
 1,110 ( 0.00%)  	extent_record(tsdn, pac, ehooks, ecache, edata);
1,339,438 ( 0.37%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_record (1,110x)
     .           }
     .           
     .           edata_t *
     .           ecache_evict(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    51 ( 0.00%)      ecache_t *ecache, size_t npages_min) {
     .           	malloc_mutex_lock(tsdn, &ecache->mtx);
     .           
     .           	/*
     .           	 * Get the LRU coalesced extent, if any.  If coalescing was delayed,
     .           	 * the loop will iterate until the LRU extent is fully coalesced.
     .           	 */
     .           	edata_t *edata;
     .           	while (true) {
     .           		/* Get the LRU extent, if any. */
    30 ( 0.00%)  		eset_t *eset = &ecache->eset;
     .           		edata = edata_list_inactive_first(&eset->lru);
    48 ( 0.00%)  		if (edata == NULL) {
     .           			/*
     .           			 * Next check if there are guarded extents.  They are
     .           			 * more expensive to purge (since they are not
     .           			 * mergeable), thus in favor of caching them longer.
     .           			 */
     9 ( 0.00%)  			eset = &ecache->guarded_eset;
     .           			edata = edata_list_inactive_first(&eset->lru);
     .           			if (edata == NULL) {
     .           				goto label_return;
     .           			}
     .           		}
     .           		/* Check the eviction limit. */
     .           		size_t extents_npages = ecache_npages_get(ecache);
    48 ( 0.00%)  		if (extents_npages <= npages_min) {
     .           			edata = NULL;
     .           			goto label_return;
     .           		}
    72 ( 0.00%)  		eset_remove(eset, edata);
 4,943 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_remove (24x)
    96 ( 0.00%)  		if (!ecache->delay_coalesce || edata_guarded_get(edata)) {
     .           			break;
     .           		}
     .           		/* Try to coalesce. */
     .           		if (extent_try_delayed_coalesce(tsdn, pac, ehooks, ecache,
     .           		    edata)) {
     .           			break;
     .           		}
     .           		/*
-- line 191 ----------------------------------------
-- line 193 ----------------------------------------
     .           		 * the LRU at its neighbor's position.  Start over.
     .           		 */
     .           	}
     .           
     .           	/*
     .           	 * Either mark the extent active or deregister it to protect against
     .           	 * concurrent operations.
     .           	 */
     6 ( 0.00%)  	switch (ecache->state) {
     .           	case extent_state_active:
     .           		not_reached();
     .           	case extent_state_dirty:
     .           	case extent_state_muzzy:
    15 ( 0.00%)  		emap_update_edata_state(tsdn, pac->emap, edata,
   231 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (3x)
     .           		    extent_state_active);
     3 ( 0.00%)  		break;
     .           	case extent_state_retained:
     .           		extent_deregister(tsdn, pac, edata);
     .           		break;
     .           	default:
     .           		not_reached();
     .           	}
     .           
     .           label_return:
     .           	malloc_mutex_unlock(tsdn, &ecache->mtx);
     .           	return edata;
    36 ( 0.00%)  }
     .           
     .           /*
     .            * This can only happen when we fail to allocate a new extent struct (which
     .            * indicates OOM), e.g. when trying to split an existing extent.
     .            */
     .           static void
     .           extents_abandon_vm(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
     .               edata_t *edata, bool growing_retained) {
-- line 227 ----------------------------------------
-- line 245 ----------------------------------------
     .           }
     .           
     .           static void
     .           extent_deactivate_locked_impl(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
     .               edata_t *edata) {
     .           	malloc_mutex_assert_owner(tsdn, &ecache->mtx);
     .           	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
     .           
11,110 ( 0.00%)  	emap_update_edata_state(tsdn, pac->emap, edata, ecache->state);
83,689 ( 0.02%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (1,103x)
 8,888 ( 0.00%)  	eset_t *eset = edata_guarded_get(edata) ? &ecache->guarded_eset :
     .           	    &ecache->eset;
 4,444 ( 0.00%)  	eset_insert(eset, edata);
192,926 ( 0.05%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_insert (1,103x)
     .           }
     .           
     .           static void
     .           extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
     .               edata_t *edata) {
     .           	assert(edata_state_get(edata) == extent_state_active);
     .           	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
     .           }
-- line 264 ----------------------------------------
-- line 272 ----------------------------------------
     .           
     .           static void
     .           extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
     .               edata_t *edata) {
     .           	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
     .           	assert(edata_state_get(edata) == ecache->state ||
     .           	    edata_state_get(edata) == extent_state_merging);
     .           
 2,634 ( 0.00%)  	eset_remove(eset, edata);
224,386 ( 0.06%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_remove (1,317x)
 6,585 ( 0.00%)  	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
100,365 ( 0.03%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_update_edata_state (1,317x)
     .           }
     .           
     .           void
     .           extent_gdump_add(tsdn_t *tsdn, const edata_t *edata) {
     .           	cassert(config_prof);
     .           	/* prof_gdump() requirement. */
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
-- line 289 ----------------------------------------
-- line 320 ----------------------------------------
     .           
     .           static bool
     .           extent_register_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata, bool gdump_add) {
     .           	assert(edata_state_get(edata) == extent_state_active);
     .           	/*
     .           	 * No locking needed, as the edata must be in active state, which
     .           	 * prevents other threads from accessing the edata.
     .           	 */
    60 ( 0.00%)  	if (emap_register_boundary(tsdn, pac->emap, edata, SC_NSIZES,
 3,032 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_register_boundary (6x)
     .           	    /* slab */ false)) {
     .           		return true;
     .           	}
     .           
     .           	if (config_prof && gdump_add) {
     .           		extent_gdump_add(tsdn, edata);
     .           	}
     .           
-- line 336 ----------------------------------------
-- line 395 ----------------------------------------
     .           		 * interior lookups (which of course cannot be recycled).
     .           		 */
     .           		void *new_addr = edata_past_get(expand_edata);
     .           		assert(PAGE_ADDR2BASE(new_addr) == new_addr);
     .           		assert(alignment <= PAGE);
     .           	}
     .           
     .           	edata_t *edata;
 9,028 ( 0.00%)  	eset_t *eset = guarded ? &ecache->guarded_eset : &ecache->eset;
 4,514 ( 0.00%)  	if (expand_edata != NULL) {
 6,671 ( 0.00%)  		edata = emap_try_acquire_edata_neighbor_expand(tsdn, pac->emap,
112,973 ( 0.03%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_try_acquire_edata_neighbor_expand (953x)
     .           		    expand_edata, EXTENT_PAI_PAC, ecache->state);
 1,906 ( 0.00%)  		if (edata != NULL) {
     .           			extent_assert_can_expand(expand_edata, edata);
 1,089 ( 0.00%)  			if (edata_size_get(edata) < size) {
 1,040 ( 0.00%)  				emap_release_edata(tsdn, pac->emap, edata,
15,730 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_release_edata (208x)
     .           				    ecache->state);
     .           				edata = NULL;
     .           			}
     .           		}
     .           	} else {
     .           		/*
     .           		 * A large extent might be broken up from its original size to
     .           		 * some small size to satisfy a small request.  When that small
     .           		 * request is freed, though, it won't merge back with the larger
     .           		 * extent if delayed coalescing is on.  The large extent can
     .           		 * then no longer satify a request for its original size.  To
     .           		 * limit this effect, when delayed coalescing is enabled, we
     .           		 * put a cap on how big an extent we can split for a request.
     .           		 */
 5,080 ( 0.00%)  		unsigned lg_max_fit = ecache->delay_coalesce
     .           		    ? (unsigned)opt_lg_extent_max_active_fit : SC_PTR_BITS;
     .           
     .           		/*
     .           		 * If split and merge are not allowed (Windows w/o retain), try
     .           		 * exact fit only.
     .           		 *
     .           		 * For simplicity purposes, splitting guarded extents is not
     .           		 * supported.  Hence, we do only exact fit for guarded
     .           		 * allocations.
     .           		 */
     .           		bool exact_only = (!maps_coalesce && !opt_retain) || guarded;
 7,824 ( 0.00%)  		edata = eset_fit(eset, size, alignment, exact_only,
373,306 ( 0.10%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_fit (1,304x)
     .           		    lg_max_fit);
     .           	}
 4,054 ( 0.00%)  	if (edata == NULL) {
     .           		return NULL;
     .           	}
     .           	assert(!guarded || edata_guarded_get(edata));
     .           	extent_activate_locked(tsdn, pac, ecache, eset, edata);
     .           
     .           	return edata;
     .           }
     .           
-- line 448 ----------------------------------------
-- line 468 ----------------------------------------
     .           	 * In a potentially invalid state.  Must leak (if *to_leak is non-NULL),
     .           	 * and salvage what's still salvageable (if *to_salvage is non-NULL).
     .           	 * None of lead, edata, or trail are valid.
     .           	 */
     .           	extent_split_interior_error
     .           } extent_split_interior_result_t;
     .           
     .           static extent_split_interior_result_t
15,876 ( 0.00%)  extent_split_interior(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               /* The result of splitting, in case of success. */
     .               edata_t **edata, edata_t **lead, edata_t **trail,
     .               /* The mess to clean up, in case of error. */
     .               edata_t **to_leak, edata_t **to_salvage,
     .               edata_t *expand_edata, size_t size, size_t alignment) {
11,907 ( 0.00%)  	size_t leadsize = ALIGNMENT_CEILING((uintptr_t)edata_base_get(*edata),
     .           	    PAGE_CEILING(alignment)) - (uintptr_t)edata_base_get(*edata);
     .           	assert(expand_edata == NULL || leadsize == 0);
 5,292 ( 0.00%)  	if (edata_size_get(*edata) < leadsize + size) {
 1,323 ( 0.00%)  		return extent_split_interior_cant_alloc;
     .           	}
 5,292 ( 0.00%)  	size_t trailsize = edata_size_get(*edata) - leadsize - size;
     .           
 1,323 ( 0.00%)  	*lead = NULL;
 1,323 ( 0.00%)  	*trail = NULL;
 2,646 ( 0.00%)  	*to_leak = NULL;
 2,646 ( 0.00%)  	*to_salvage = NULL;
     .           
     .           	/* Split the lead. */
 2,646 ( 0.00%)  	if (leadsize != 0) {
     .           		assert(!edata_guarded_get(*edata));
     .           		*lead = *edata;
     .           		*edata = extent_split_impl(tsdn, pac, ehooks, *lead, leadsize,
     .           		    size + trailsize, /* holding_core_locks*/ true);
     .           		if (*edata == NULL) {
     .           			*to_leak = *lead;
     .           			*lead = NULL;
     .           			return extent_split_interior_error;
     .           		}
     .           	}
     .           
     .           	/* Split the trail. */
 2,646 ( 0.00%)  	if (trailsize != 0) {
     .           		assert(!edata_guarded_get(*edata));
 6,654 ( 0.00%)  		*trail = extent_split_impl(tsdn, pac, ehooks, *edata, size,
660,269 ( 0.18%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_split_impl.constprop.0 (1,109x)
     .           		    trailsize, /* holding_core_locks */ true);
 2,218 ( 0.00%)  		if (*trail == NULL) {
     .           			*to_leak = *edata;
     .           			*to_salvage = *lead;
     .           			*lead = NULL;
     .           			*edata = NULL;
     .           			return extent_split_interior_error;
     .           		}
     .           	}
     .           
 1,323 ( 0.00%)  	return extent_split_interior_ok;
11,907 ( 0.00%)  }
     .           
     .           /*
     .            * This fulfills the indicated allocation request out of the given extent (which
     .            * the caller should have ensured was big enough).  If there's any unused space
     .            * before or after the resulting allocation, that space is given its own extent
     .            * and put back into ecache.
     .            */
     .           static edata_t *
-- line 531 ----------------------------------------
-- line 532 ----------------------------------------
     .           extent_recycle_split(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               ecache_t *ecache, edata_t *expand_edata, size_t size, size_t alignment,
     .               edata_t *edata, bool growing_retained) {
     .           	assert(!edata_guarded_get(edata) || size == edata_size_get(edata));
     .           	malloc_mutex_assert_owner(tsdn, &ecache->mtx);
     .           
     .           	edata_t *lead;
     .           	edata_t *trail;
 1,317 ( 0.00%)  	edata_t *to_leak JEMALLOC_CC_SILENCE_INIT(NULL);
 1,317 ( 0.00%)  	edata_t *to_salvage JEMALLOC_CC_SILENCE_INIT(NULL);
     .           
18,438 ( 0.01%)  	extent_split_interior_result_t result = extent_split_interior(
735,133 ( 0.20%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_split_interior.constprop.0 (1,317x)
     .           	    tsdn, pac, ehooks, &edata, &lead, &trail, &to_leak, &to_salvage,
     .           	    expand_edata, size, alignment);
     .           
     .           	if (!maps_coalesce && result != extent_split_interior_ok
     .           	    && !opt_retain) {
     .           		/*
     .           		 * Split isn't supported (implies Windows w/o retain).  Avoid
     .           		 * leaking the extent.
     .           		 */
     .           		assert(to_leak != NULL && lead == NULL && trail == NULL);
     .           		extent_deactivate_locked(tsdn, pac, ecache, to_leak);
     .           		return NULL;
     .           	}
     .           
 3,951 ( 0.00%)  	if (result == extent_split_interior_ok) {
 3,951 ( 0.00%)  		if (lead != NULL) {
     .           			extent_deactivate_locked(tsdn, pac, ecache, lead);
     .           		}
 3,951 ( 0.00%)  		if (trail != NULL) {
     .           			extent_deactivate_locked(tsdn, pac, ecache, trail);
     .           		}
 1,317 ( 0.00%)  		return edata;
     .           	} else {
     .           		/*
     .           		 * We should have picked an extent that was large enough to
     .           		 * fulfill our allocation request.
     .           		 */
     .           		assert(result == extent_split_interior_error);
     .           		if (to_salvage != NULL) {
     .           			extent_deregister(tsdn, pac, to_salvage);
-- line 573 ----------------------------------------
-- line 590 ----------------------------------------
     .           
     .           /*
     .            * Tries to satisfy the given allocation request by reusing one of the extents
     .            * in the given ecache_t.
     .            */
     .           static edata_t *
     .           extent_recycle(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
     .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
54,168 ( 0.01%)      bool *commit, bool growing_retained, bool guarded) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
     .           	assert(!guarded || expand_edata == NULL);
     .           	assert(!guarded || alignment <= PAGE);
     .           
     .           	malloc_mutex_lock(tsdn, &ecache->mtx);
     .           
 1,304 ( 0.00%)  	edata_t *edata = extent_recycle_extract(tsdn, pac, ehooks, ecache,
     .           	    expand_edata, size, alignment, guarded);
   208 ( 0.00%)  	if (edata == NULL) {
     .           		malloc_mutex_unlock(tsdn, &ecache->mtx);
     .           		return NULL;
     .           	}
     .           
 2,634 ( 0.00%)  	edata = extent_recycle_split(tsdn, pac, ehooks, ecache, expand_edata,
     .           	    size, alignment, edata, growing_retained);
     .           	malloc_mutex_unlock(tsdn, &ecache->mtx);
 3,574 ( 0.00%)  	if (edata == NULL) {
   940 ( 0.00%)  		return NULL;
     .           	}
     .           
     .           	assert(edata_state_get(edata) == extent_state_active);
13,170 ( 0.00%)  	if (extent_commit_zero(tsdn, ehooks, edata, *commit, zero,
26,340 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_commit_zero (1,317x)
     .           	    growing_retained)) {
     .           		extent_record(tsdn, pac, ehooks, ecache, edata);
     .           		return NULL;
     .           	}
 2,634 ( 0.00%)  	if (edata_committed_get(edata)) {
     .           		/*
     .           		 * This reverses the purpose of this variable - previously it
     .           		 * was treated as an input parameter, now it turns into an
     .           		 * output parameter, reporting if the edata has actually been
     .           		 * committed.
     .           		 */
 2,634 ( 0.00%)  		*commit = true;
     .           	}
     .           	return edata;
27,084 ( 0.01%)  }
     .           
     .           /*
     .            * If virtual memory is retained, create increasingly larger extents from which
     .            * to split requested extents in order to limit the total number of disjoint
     .            * virtual memory ranges retained by each shard.
     .            */
     .           static edata_t *
     .           extent_grow_retained(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               size_t size, size_t alignment, bool zero, bool *commit) {
     .           	malloc_mutex_assert_owner(tsdn, &pac->grow_mtx);
     .           
    30 ( 0.00%)  	size_t alloc_size_min = size + PAGE_CEILING(alignment) - PAGE;
     .           	/* Beware size_t wrap-around. */
    12 ( 0.00%)  	if (alloc_size_min < size) {
     .           		goto label_err;
     .           	}
     .           	/*
     .           	 * Find the next extent size in the series that would be large enough to
     .           	 * satisfy this request.
     .           	 */
     .           	size_t alloc_size;
     .           	pszind_t exp_grow_skip;
     .           	bool err = exp_grow_size_prepare(&pac->exp_grow, alloc_size_min,
     .           	    &alloc_size, &exp_grow_skip);
     .           	if (err) {
     .           		goto label_err;
     .           	}
     .           
    30 ( 0.00%)  	edata_t *edata = edata_cache_get(tsdn, pac->edata_cache);
 1,914 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata_cache.c:_rjem_je_edata_cache_get (6x)
    12 ( 0.00%)  	if (edata == NULL) {
     .           		goto label_err;
     .           	}
     6 ( 0.00%)  	bool zeroed = false;
     6 ( 0.00%)  	bool committed = false;
     .           
     .           	void *ptr = ehooks_alloc(tsdn, ehooks, NULL, alloc_size, PAGE, &zeroed,
     .           	    &committed);
     .           
    12 ( 0.00%)  	if (ptr == NULL) {
     .           		edata_cache_put(tsdn, pac->edata_cache, edata);
     .           		goto label_err;
     .           	}
     .           
    18 ( 0.00%)  	edata_init(edata, ecache_ind_get(&pac->ecache_retained), ptr,
     .           	    alloc_size, false, SC_NSIZES, extent_sn_next(pac),
     .           	    extent_state_active, zeroed, committed, EXTENT_PAI_PAC,
     .           	    EXTENT_IS_HEAD);
     .           
     .           	if (extent_register_no_gdump_add(tsdn, pac, edata)) {
     .           		edata_cache_put(tsdn, pac->edata_cache, edata);
     .           		goto label_err;
     .           	}
     .           
    12 ( 0.00%)  	if (edata_committed_get(edata)) {
     6 ( 0.00%)  		*commit = true;
     .           	}
     .           
     .           	edata_t *lead;
     .           	edata_t *trail;
     6 ( 0.00%)  	edata_t *to_leak JEMALLOC_CC_SILENCE_INIT(NULL);
     6 ( 0.00%)  	edata_t *to_salvage JEMALLOC_CC_SILENCE_INIT(NULL);
     .           
    84 ( 0.00%)  	extent_split_interior_result_t result = extent_split_interior(tsdn,
 5,450 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_split_interior.constprop.0 (6x)
     .           	    pac, ehooks, &edata, &lead, &trail, &to_leak, &to_salvage, NULL,
     .           	    size, alignment);
     .           
    18 ( 0.00%)  	if (result == extent_split_interior_ok) {
    18 ( 0.00%)  		if (lead != NULL) {
     .           			extent_record(tsdn, pac, ehooks, &pac->ecache_retained,
     .           			    lead);
     .           		}
    18 ( 0.00%)  		if (trail != NULL) {
    30 ( 0.00%)  			extent_record(tsdn, pac, ehooks, &pac->ecache_retained,
 3,416 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_record (6x)
     .           			    trail);
     .           		}
     .           	} else {
     .           		/*
     .           		 * We should have allocated a sufficiently large extent; the
     .           		 * cant_alloc case should not occur.
     .           		 */
     .           		assert(result == extent_split_interior_error);
-- line 717 ----------------------------------------
-- line 725 ----------------------------------------
     .           		if (to_leak != NULL) {
     .           			extent_deregister_no_gdump_sub(tsdn, pac, to_leak);
     .           			extents_abandon_vm(tsdn, pac, ehooks,
     .           			    &pac->ecache_retained, to_leak, true);
     .           		}
     .           		goto label_err;
     .           	}
     .           
    30 ( 0.00%)  	if (*commit && !edata_committed_get(edata)) {
     .           		if (extent_commit_impl(tsdn, ehooks, edata, 0,
     .           		    edata_size_get(edata), true)) {
     .           			extent_record(tsdn, pac, ehooks,
     .           			    &pac->ecache_retained, edata);
     .           			goto label_err;
     .           		}
     .           		/* A successful commit should return zeroed memory. */
     .           		if (config_debug) {
-- line 741 ----------------------------------------
-- line 755 ----------------------------------------
     .           	/* All opportunities for failure are past. */
     .           	exp_grow_size_commit(&pac->exp_grow, exp_grow_skip);
     .           	malloc_mutex_unlock(tsdn, &pac->grow_mtx);
     .           
     .           	if (config_prof) {
     .           		/* Adjust gdump stats now that extent is final size. */
     .           		extent_gdump_add(tsdn, edata);
     .           	}
    15 ( 0.00%)  	if (zero && !edata_zeroed_get(edata)) {
     .           		ehooks_zero(tsdn, ehooks, edata_base_get(edata),
     .           		    edata_size_get(edata));
     .           	}
     5 ( 0.00%)  	return edata;
     .           label_err:
     .           	malloc_mutex_unlock(tsdn, &pac->grow_mtx);
     .           	return NULL;
     .           }
     .           
     .           static edata_t *
     .           extent_alloc_retained(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
     .               bool *commit, bool guarded) {
     .           	assert(size != 0);
     .           	assert(alignment != 0);
     .           
     .           	malloc_mutex_lock(tsdn, &pac->grow_mtx);
     .           
 7,308 ( 0.00%)  	edata_t *edata = extent_recycle(tsdn, pac, ehooks,
310,895 ( 0.09%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_recycle (406x)
     .           	    &pac->ecache_retained, expand_edata, size, alignment, zero, commit,
     .           	    /* growing_retained */ true, guarded);
 1,218 ( 0.00%)  	if (edata != NULL) {
     .           		malloc_mutex_unlock(tsdn, &pac->grow_mtx);
     .           		if (config_prof) {
     .           			extent_gdump_add(tsdn, edata);
     .           		}
 1,332 ( 0.00%)  	} else if (opt_retain && expand_edata == NULL && !guarded) {
     .           		edata = extent_grow_retained(tsdn, pac, ehooks, size,
     .           		    alignment, zero, commit);
     .           		/* extent_grow_retained() always releases pac->grow_mtx. */
     .           	} else {
     .           		malloc_mutex_unlock(tsdn, &pac->grow_mtx);
     .           	}
     .           	malloc_mutex_assert_not_owner(tsdn, &pac->grow_mtx);
     .           
     .           	return edata;
     .           }
     .           
     .           static bool
     .           extent_coalesce(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
     .               edata_t *inner, edata_t *outer, bool forward) {
     .           	extent_assert_can_coalesce(inner, outer);
 2,604 ( 0.00%)  	eset_remove(&ecache->eset, outer);
142,041 ( 0.04%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/eset.c:_rjem_je_eset_remove (868x)
     .           
 5,208 ( 0.00%)  	bool err = extent_merge_impl(tsdn, pac, ehooks,
349,965 ( 0.10%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_merge_impl.constprop.0 (868x)
     .           	    forward ? inner : outer, forward ? outer : inner,
     .           	    /* holding_core_locks */ true);
 1,736 ( 0.00%)  	if (err) {
     .           		extent_deactivate_check_state_locked(tsdn, pac, ecache, outer,
     .           		    extent_state_merging);
     .           	}
     .           
     .           	return err;
     .           }
     .           
     .           static edata_t *
     .           extent_try_coalesce_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
30,719 ( 0.01%)      ecache_t *ecache, edata_t *edata, bool *coalesced) {
     .           	assert(!edata_guarded_get(edata));
     .           	/*
     .           	 * We avoid checking / locking inactive neighbors for large size
     .           	 * classes, since they are eagerly coalesced on deallocation which can
     .           	 * cause lock contention.
     .           	 */
     .           	/*
     .           	 * Continue attempting to coalesce until failure, to protect against
     .           	 * races with other threads that are thwarted by this one.
     .           	 */
     .           	bool again;
     .           	do {
     .           		again = false;
     .           
     .           		/* Try to coalesce forward. */
14,464 ( 0.00%)  		edata_t *next = emap_try_acquire_edata_neighbor(tsdn, pac->emap,
213,699 ( 0.06%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_try_acquire_edata_neighbor (1,808x)
     .           		    edata, EXTENT_PAI_PAC, ecache->state, /* forward */ true);
 3,616 ( 0.00%)  		if (next != NULL) {
     .           			if (!extent_coalesce(tsdn, pac, ehooks, ecache, edata,
     .           			    next, true)) {
 1,244 ( 0.00%)  				if (ecache->delay_coalesce) {
     .           					/* Do minimal coalescing. */
 1,242 ( 0.00%)  					*coalesced = true;
   621 ( 0.00%)  					return edata;
     .           				}
     .           				again = true;
     .           			}
     .           		}
     .           
     .           		/* Try to coalesce backward. */
 9,496 ( 0.00%)  		edata_t *prev = emap_try_acquire_edata_neighbor(tsdn, pac->emap,
120,922 ( 0.03%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_try_acquire_edata_neighbor (1,187x)
     .           		    edata, EXTENT_PAI_PAC, ecache->state, /* forward */ false);
 2,620 ( 0.00%)  		if (prev != NULL) {
     .           			if (!extent_coalesce(tsdn, pac, ehooks, ecache, edata,
     .           			    prev, false)) {
     .           				edata = prev;
   492 ( 0.00%)  				if (ecache->delay_coalesce) {
     .           					/* Do minimal coalescing. */
   492 ( 0.00%)  					*coalesced = true;
   492 ( 0.00%)  					return edata;
     .           				}
     .           				again = true;
     .           			}
     .           		}
     .           	} while (again);
     .           
 1,880 ( 0.00%)  	if (ecache->delay_coalesce) {
 1,862 ( 0.00%)  		*coalesced = false;
     .           	}
     .           	return edata;
16,263 ( 0.00%)  }
     .           
     .           static edata_t *
     .           extent_try_coalesce(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               ecache_t *ecache, edata_t *edata, bool *coalesced) {
   273 ( 0.00%)  	return extent_try_coalesce_impl(tsdn, pac, ehooks, ecache, edata,
19,106 ( 0.01%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_try_coalesce_impl (24x)
     .           	    coalesced);
     .           }
     .           
     .           static edata_t *
     .           extent_try_coalesce_large(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               ecache_t *ecache, edata_t *edata, bool *coalesced) {
14,192 ( 0.00%)  	return extent_try_coalesce_impl(tsdn, pac, ehooks, ecache, edata,
899,887 ( 0.25%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:extent_try_coalesce_impl (1,774x)
     .           	    coalesced);
     .           }
     .           
     .           /* Purge a single extent to retained / unmapped directly. */
     .           static void
     .           extent_maximally_purge(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               edata_t *edata) {
     .           	size_t extent_size = edata_size_get(edata);
-- line 892 ----------------------------------------
-- line 908 ----------------------------------------
     .           }
     .           
     .           /*
     .            * Does the metadata management portions of putting an unused extent into the
     .            * given ecache_t (coalesces and inserts into the eset).
     .            */
     .           void
     .           extent_record(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
19,023 ( 0.01%)      edata_t *edata) {
     .           	assert((ecache->state != extent_state_dirty &&
     .           	    ecache->state != extent_state_muzzy) ||
     .           	    !edata_zeroed_get(edata));
     .           
     .           	malloc_mutex_lock(tsdn, &ecache->mtx);
     .           
     .           	emap_assert_mapped(tsdn, pac->emap, edata);
     .           
 2,238 ( 0.00%)  	if (edata_guarded_get(edata)) {
     .           		goto label_skip_coalesce;
     .           	}
 2,238 ( 0.00%)  	if (!ecache->delay_coalesce) {
     .           		edata = extent_try_coalesce(tsdn, pac, ehooks, ecache, edata,
     .           		    NULL);
 4,076 ( 0.00%)  	} else if (edata_size_get(edata) >= SC_LARGE_MINCLASS) {
     .           		assert(ecache == &pac->ecache_dirty);
     .           		/* Always coalesce large extents eagerly. */
     .           		bool coalesced;
     .           		do {
     .           			assert(edata_state_get(edata) == extent_state_active);
     .           			edata = extent_try_coalesce_large(tsdn, pac, ehooks,
     .           			    ecache, edata, &coalesced);
 5,322 ( 0.00%)  		} while (coalesced);
 1,856 ( 0.00%)  		if (edata_size_get(edata) >=
     .           		    atomic_load_zu(&pac->oversize_threshold, ATOMIC_RELAXED)
     .           		    && extent_may_force_decay(pac)) {
     .           			/* Shortcut to purge the oversize extent eagerly. */
     .           			malloc_mutex_unlock(tsdn, &ecache->mtx);
     .           			extent_maximally_purge(tsdn, pac, ehooks, edata);
     .           			return;
     .           		}
     .           	}
     .           label_skip_coalesce:
     .           	extent_deactivate_locked(tsdn, pac, ecache, edata);
     .           
     .           	malloc_mutex_unlock(tsdn, &ecache->mtx);
 7,833 ( 0.00%)  }
     .           
     .           void
     .           extent_dalloc_gap(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               edata_t *edata) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
     .           	if (extent_register(tsdn, pac, edata)) {
-- line 961 ----------------------------------------
-- line 1021 ----------------------------------------
     .           		return NULL;
     .           	}
     .           
     .           	return edata;
     .           }
     .           
     .           void
     .           extent_dalloc_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    39 ( 0.00%)      edata_t *edata) {
     .           	assert(edata_pai_get(edata) == EXTENT_PAI_PAC);
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           
     .           	/* Avoid calling the default extent_dalloc unless have to. */
     6 ( 0.00%)  	if (!ehooks_dalloc_will_fail(ehooks)) {
     .           		/* Remove guard pages for dalloc / unmap. */
     .           		if (edata_guarded_get(edata)) {
     .           			assert(ehooks_are_default(ehooks));
     .           			san_unguard_pages_two_sided(tsdn, ehooks, edata,
     .           			    pac->emap);
     .           		}
     .           		/*
     .           		 * Deregister first to avoid a race with other allocating
-- line 1043 ----------------------------------------
-- line 1047 ----------------------------------------
     .           		if (!extent_dalloc_wrapper_try(tsdn, pac, ehooks, edata)) {
     .           			return;
     .           		}
     .           		extent_reregister(tsdn, pac, edata);
     .           	}
     .           
     .           	/* Try to decommit; purge if that fails. */
     .           	bool zeroed;
     6 ( 0.00%)  	if (!edata_committed_get(edata)) {
     .           		zeroed = true;
    27 ( 0.00%)  	} else if (!extent_decommit_wrapper(tsdn, ehooks, edata, 0,
   213 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_decommit_wrapper (3x)
     .           	    edata_size_get(edata))) {
     .           		zeroed = true;
    12 ( 0.00%)  	} else if (!ehooks_purge_forced(tsdn, ehooks, edata_base_get(edata),
     .           	    edata_size_get(edata), 0, edata_size_get(edata))) {
     .           		zeroed = true;
     .           	} else if (edata_state_get(edata) == extent_state_muzzy ||
     .           	    !ehooks_purge_lazy(tsdn, ehooks, edata_base_get(edata),
     .           	    edata_size_get(edata), 0, edata_size_get(edata))) {
     .           		zeroed = false;
     .           	} else {
     .           		zeroed = false;
     .           	}
     .           	edata_zeroed_set(edata, zeroed);
     .           
     .           	if (config_prof) {
     .           		extent_gdump_sub(tsdn, edata);
     .           	}
     .           
    18 ( 0.00%)  	extent_record(tsdn, pac, ehooks, &pac->ecache_retained, edata);
 2,599 ( 0.00%)  => target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent.c:_rjem_je_extent_record (3x)
    21 ( 0.00%)  }
     .           
     .           void
     .           extent_destroy_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               edata_t *edata) {
     .           	assert(edata_base_get(edata) != NULL);
     .           	assert(edata_size_get(edata) != 0);
     .           	extent_state_t state = edata_state_get(edata);
     .           	assert(state == extent_state_retained || state == extent_state_active);
-- line 1085 ----------------------------------------
-- line 1115 ----------------------------------------
     .           extent_commit_wrapper(tsdn_t *tsdn, ehooks_t *ehooks, edata_t *edata,
     .               size_t offset, size_t length) {
     .           	return extent_commit_impl(tsdn, ehooks, edata, offset, length,
     .           	    /* growing_retained */ false);
     .           }
     .           
     .           bool
     .           extent_decommit_wrapper(tsdn_t *tsdn, ehooks_t *ehooks, edata_t *edata,
    30 ( 0.00%)      size_t offset, size_t length) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, 0);
     .           	bool err = ehooks_decommit(tsdn, ehooks, edata_base_get(edata),
     .           	    edata_size_get(edata), offset, length);
    21 ( 0.00%)  	edata_committed_set(edata, edata_committed_get(edata) && err);
     .           	return err;
    27 ( 0.00%)  }
     .           
     .           static bool
     .           extent_purge_lazy_impl(tsdn_t *tsdn, ehooks_t *ehooks, edata_t *edata,
     .               size_t offset, size_t length, bool growing_retained) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
     .           	bool err = ehooks_purge_lazy(tsdn, ehooks, edata_base_get(edata),
     .           	    edata_size_get(edata), offset, length);
-- line 1138 ----------------------------------------
-- line 1166 ----------------------------------------
     .           /*
     .            * Accepts the extent to split, and the characteristics of each side of the
     .            * split.  The 'a' parameters go with the 'lead' of the resulting pair of
     .            * extents (the lower addressed portion of the split), and the 'b' parameters go
     .            * with the trail (the higher addressed portion).  This makes 'extent' the lead,
     .            * and returns the trail (except in case of error).
     .            */
     .           static edata_t *
11,090 ( 0.00%)  extent_split_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
     .               edata_t *edata, size_t size_a, size_t size_b, bool holding_core_locks) {
     .           	assert(edata_size_get(edata) == size_a + size_b);
     .           	/* Only the shrink path may split w/o holding core locks. */
     .           	if (holding_core_locks) {
     .           		witness_assert_positive_depth_to_rank(
     .           		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
     .           	} else {
     .           		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           		    WITNESS_RANK_CORE, 0);
     .           	}
     .           
 3,327 ( 0.00%)  	if (ehooks_split_will_fail(ehooks)) {
     .           		return NULL;
     .           	}
     .           
 6,654 ( 0.00%)  	edata_t *trail = edata_cache_get(tsdn, pac->edata_cache);
370,820 ( 0.10%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata_cache.c:_rjem_je_edata_cache_get (1,109x)
 2,218 ( 0.00%)  	if (trail == NULL) {
     .           		goto label_error_a;
     .           	}
     .           
     .           	edata_init(trail, edata_arena_ind_get(edata),
 2,218 ( 0.00%)  	    (void *)((uintptr_t)edata_base_get(edata) + size_a), size_b,
     .           	    /* slab */ false, SC_NSIZES, edata_sn_get(edata),
     .           	    edata_state_get(edata), edata_zeroed_get(edata),
     .           	    edata_committed_get(edata), EXTENT_PAI_PAC, EXTENT_NOT_HEAD);
     .           	emap_prepare_t prepare;
11,090 ( 0.00%)  	bool err = emap_split_prepare(tsdn, pac->emap, &prepare, edata,
123,099 ( 0.03%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_split_prepare (1,109x)
     .           	    size_a, trail, size_b);
 4,436 ( 0.00%)  	if (err) {
     .           		goto label_error_b;
     .           	}
     .           
     .           	/*
     .           	 * No need to acquire trail or edata, because: 1) trail was new (just
     .           	 * allocated); and 2) edata is either an active allocation (the shrink
     .           	 * path), or in an acquired state (extracted from the ecache on the
     .           	 * extent_recycle_split path).
     .           	 */
     .           	assert(emap_edata_is_acquired(tsdn, pac->emap, edata));
     .           	assert(emap_edata_is_acquired(tsdn, pac->emap, trail));
     .           
     .           	err = ehooks_split(tsdn, ehooks, edata_base_get(edata), size_a + size_b,
     .           	    size_a, size_b, edata_committed_get(edata));
     .           
 2,218 ( 0.00%)  	if (err) {
     .           		goto label_error_b;
     .           	}
     .           
     .           	edata_size_set(edata, size_a);
 9,981 ( 0.00%)  	emap_split_commit(tsdn, pac->emap, &prepare, edata, size_a, trail,
49,905 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_split_commit (1,109x)
     .           	    size_b);
     .           
 3,327 ( 0.00%)  	return trail;
     .           label_error_b:
     .           	edata_cache_put(tsdn, pac->edata_cache, trail);
     .           label_error_a:
     .           	return NULL;
13,308 ( 0.00%)  }
     .           
     .           edata_t *
     .           extent_split_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *edata,
     .               size_t size_a, size_t size_b, bool holding_core_locks) {
     .           	return extent_split_impl(tsdn, pac, ehooks, edata, size_a, size_b,
     .           	    holding_core_locks);
     .           }
     .           
     .           static bool
13,888 ( 0.00%)  extent_merge_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *a,
     .               edata_t *b, bool holding_core_locks) {
     .           	/* Only the expanding path may merge w/o holding ecache locks. */
     .           	if (holding_core_locks) {
     .           		witness_assert_positive_depth_to_rank(
     .           		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
     .           	} else {
     .           		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           		    WITNESS_RANK_CORE, 0);
-- line 1250 ----------------------------------------
-- line 1255 ----------------------------------------
     .           	assert(edata_arena_ind_get(a) == ehooks_ind_get(ehooks));
     .           	emap_assert_mapped(tsdn, pac->emap, a);
     .           	emap_assert_mapped(tsdn, pac->emap, b);
     .           
     .           	bool err = ehooks_merge(tsdn, ehooks, edata_base_get(a),
     .           	    edata_size_get(a), edata_base_get(b), edata_size_get(b),
     .           	    edata_committed_get(a));
     .           
 2,046 ( 0.00%)  	if (err) {
     .           		return true;
     .           	}
     .           
     .           	/*
     .           	 * The rtree writes must happen while all the relevant elements are
     .           	 * owned, so the following code uses decomposed helper functions rather
     .           	 * than extent_{,de}register() to do things in the right order.
     .           	 */
     .           	emap_prepare_t prepare;
 8,184 ( 0.00%)  	emap_merge_prepare(tsdn, pac->emap, &prepare, a, b);
15,655 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_merge_prepare (155x)
     .           
     .           	assert(edata_state_get(a) == extent_state_active ||
     .           	    edata_state_get(a) == extent_state_merging);
     .           	edata_state_set(a, extent_state_active);
 1,023 ( 0.00%)  	edata_size_set(a, edata_size_get(a) + edata_size_get(b));
     .           	edata_sn_set(a, (edata_sn_get(a) < edata_sn_get(b)) ?
     .           	    edata_sn_get(a) : edata_sn_get(b));
 4,092 ( 0.00%)  	edata_zeroed_set(a, edata_zeroed_get(a) && edata_zeroed_get(b));
     .           
 6,138 ( 0.00%)  	emap_merge_commit(tsdn, pac->emap, &prepare, a, b);
 5,115 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/emap.c:_rjem_je_emap_merge_commit (155x)
     .           
 4,092 ( 0.00%)  	edata_cache_put(tsdn, pac->edata_cache, b);
21,328 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/edata_cache.c:_rjem_je_edata_cache_put (155x)
     .           
   868 ( 0.00%)  	return false;
10,416 ( 0.00%)  }
     .           
     .           bool
     .           extent_merge_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
 2,635 ( 0.00%)      edata_t *a, edata_t *b) {
   155 ( 0.00%)  	return extent_merge_impl(tsdn, pac, ehooks, a, b,
     .           	    /* holding_core_locks */ false);
 1,860 ( 0.00%)  }
     .           
     .           bool
     .           extent_commit_zero(tsdn_t *tsdn, ehooks_t *ehooks, edata_t *edata,
 9,219 ( 0.00%)      bool commit, bool zero, bool growing_retained) {
     .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
     .           	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
     .           
 5,268 ( 0.00%)  	if (commit && !edata_committed_get(edata)) {
     .           		if (extent_commit_impl(tsdn, ehooks, edata, 0,
     .           		    edata_size_get(edata), growing_retained)) {
     .           			return true;
     .           		}
     .           	}
 2,634 ( 0.00%)  	if (zero && !edata_zeroed_get(edata)) {
     .           		void *addr = edata_base_get(edata);
     .           		size_t size = edata_size_get(edata);
     .           		ehooks_zero(tsdn, ehooks, addr, size);
     .           	}
 1,317 ( 0.00%)  	return false;
 7,902 ( 0.00%)  }
     .           
     .           bool
     3 ( 0.00%)  extent_boot(void) {
     .           	assert(sizeof(slab_data_t) >= sizeof(e_prof_info_t));
     .           
     .           	if (have_dss) {
     1 ( 0.00%)  		extent_dss_boot();
    48 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/extent_dss.c:_rjem_je_extent_dss_boot (1x)
     .           	}
     .           
     .           	return false;
     3 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/hook.c
--------------------------------------------------------------------------------
Ir              

-- line 14 ----------------------------------------
     .           
     .           seq_define(hooks_internal_t, hooks)
     .           
     .           static atomic_u_t nhooks = ATOMIC_INIT(0);
     .           static seq_hooks_t hooks[HOOK_MAX];
     .           static malloc_mutex_t hooks_mu;
     .           
     .           bool
     1 ( 0.00%)  hook_boot() {
     5 ( 0.00%)  	return malloc_mutex_init(&hooks_mu, "hooks", WITNESS_RANK_HOOK,
   132 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
     .           	    malloc_mutex_rank_exclusive);
     .           }
     .           
     .           static void *
     .           hook_install_locked(hooks_t *to_install) {
     .           	hooks_internal_t hooks_internal;
     .           	for (int i = 0; i < HOOK_MAX; i++) {
     .           		bool success = seq_try_load_hooks(&hooks_internal, &hooks[i]);
-- line 31 ----------------------------------------
-- line 147 ----------------------------------------
     .           	}								\
     .           	*in_hook = true;
     .           
     .           #define HOOK_EPILOGUE							\
     .           	*in_hook = false;
     .           
     .           void
     .           hook_invoke_alloc(hook_alloc_t type, void *result, uintptr_t result_raw,
32,535 ( 0.01%)      uintptr_t args_raw[3]) {
 7,230 ( 0.00%)  	HOOK_PROLOGUE
     .           
     .           	hooks_internal_t hook;
     .           	FOR_EACH_HOOK_BEGIN(&hook)
     .           		hook_alloc h = hook.hooks.alloc_hook;
     .           		if (h != NULL) {
     .           			h(hook.hooks.extra, type, result, result_raw, args_raw);
     .           		}
     .           	FOR_EACH_HOOK_END
     .           
     .           	HOOK_EPILOGUE
39,765 ( 0.01%)  }
     .           
     .           void
32,535 ( 0.01%)  hook_invoke_dalloc(hook_dalloc_t type, void *address, uintptr_t args_raw[3]) {
 7,230 ( 0.00%)  	HOOK_PROLOGUE
     .           	hooks_internal_t hook;
     .           	FOR_EACH_HOOK_BEGIN(&hook)
     .           		hook_dalloc h = hook.hooks.dalloc_hook;
     .           		if (h != NULL) {
     .           			h(hook.hooks.extra, type, address, args_raw);
     .           		}
     .           	FOR_EACH_HOOK_END
     .           	HOOK_EPILOGUE
39,765 ( 0.01%)  }
     .           
     .           void
     .           hook_invoke_expand(hook_expand_t type, void *address, size_t old_usize,
 1,395 ( 0.00%)      size_t new_usize, uintptr_t result_raw, uintptr_t args_raw[4]) {
   310 ( 0.00%)  	HOOK_PROLOGUE
     .           	hooks_internal_t hook;
     .           	FOR_EACH_HOOK_BEGIN(&hook)
     .           		hook_expand h = hook.hooks.expand_hook;
     .           		if (h != NULL) {
     .           			h(hook.hooks.extra, type, address, old_usize, new_usize,
     .           			    result_raw, args_raw);
     .           		}
     .           	FOR_EACH_HOOK_END
     .           	HOOK_EPILOGUE
 1,705 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/arena_inlines_b.h
--------------------------------------------------------------------------------
Ir              

-- line 14 ----------------------------------------
     .           static inline arena_t *
     .           arena_get_from_edata(edata_t *edata) {
     .           	return (arena_t *)atomic_load_p(&arenas[edata_arena_ind_get(edata)],
     .           	    ATOMIC_RELAXED);
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE arena_t *
     .           arena_choose_maybe_huge(tsd_t *tsd, arena_t *arena, size_t size) {
 2,510 ( 0.00%)  	if (arena != NULL) {
     .           		return arena;
     .           	}
     .           
     .           	/*
     .           	 * For huge allocations, use the dedicated huge arena if both are true:
     .           	 * 1) is using auto arena selection (i.e. arena == NULL), and 2) the
     .           	 * thread is not assigned to a manual arena.
     .           	 */
 1,646 ( 0.00%)  	if (unlikely(size >= oversize_threshold)) {
     .           		arena_t *tsd_arena = tsd_arena_get(tsd);
     .           		if (tsd_arena == NULL || arena_is_auto(tsd_arena)) {
     .           			return arena_choose_huge(tsd);
     .           		}
     .           	}
     .           
     .           	return arena_choose(tsd, NULL);
     .           }
-- line 39 ----------------------------------------
-- line 111 ----------------------------------------
     .           	cassert(config_prof);
     .           
     .           	assert(!edata_slab_get(edata));
     .           	large_prof_info_set(edata, tctx, size);
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_decay_ticks(tsdn_t *tsdn, arena_t *arena, unsigned nticks) {
 7,387 ( 0.00%)  	if (unlikely(tsdn_null(tsdn))) {
     .           		return;
     .           	}
     .           	tsd_t *tsd = tsdn_tsd(tsdn);
     .           	/*
     .           	 * We use the ticker_geom_t to avoid having per-arena state in the tsd.
     .           	 * Instead of having a countdown-until-decay timer running for every
     .           	 * arena in every thread, we flip a coin once per tick, whose
     .           	 * probability of coming up heads is 1/nticks; this is effectively the
     .           	 * operation of the ticker_geom_t.  Each arena has the same chance of a
     .           	 * coinflip coming up heads (1/ARENA_DECAY_NTICKS_PER_UPDATE), so we can
     .           	 * use a single ticker for all of them.
     .           	 */
     .           	ticker_geom_t *decay_ticker = tsd_arena_decay_tickerp_get(tsd);
     .           	uint64_t *prng_state = tsd_prng_statep_get(tsd);
    16 ( 0.00%)  	if (unlikely(ticker_geom_ticks(decay_ticker, prng_state, nticks))) {
    50 ( 0.00%)  		arena_decay(tsdn, arena, false, false);
44,288 ( 0.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_decay (7x)
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_decay_tick(tsdn_t *tsdn, arena_t *arena) {
     .           	arena_decay_ticks(tsdn, arena, 1);
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           arena_malloc(tsdn_t *tsdn, arena_t *arena, size_t size, szind_t ind, bool zero,
     .               tcache_t *tcache, bool slow_path) {
     .           	assert(!tsdn_null(tsdn) || tcache == NULL);
     .           
 6,714 ( 0.00%)  	if (likely(tcache != NULL)) {
 3,644 ( 0.00%)  		if (likely(size <= SC_SMALL_MAXCLASS)) {
     .           			return tcache_alloc_small(tsdn_tsd(tsdn), arena,
     .           			    tcache, size, ind, zero, slow_path);
     .           		}
 2,580 ( 0.00%)  		if (likely(size <= tcache_maxclass)) {
     .           			return tcache_alloc_large(tsdn_tsd(tsdn), arena,
     .           			    tcache, size, ind, zero, slow_path);
     .           		}
     .           		/* (size > tcache_maxclass) case falls through. */
     .           		assert(size > tcache_maxclass);
     .           	}
     .           
 3,488 ( 0.00%)  	return arena_malloc_hard(tsdn, arena, size, ind, zero);
909,608 ( 0.25%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_malloc_hard (436x)
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE arena_t *
     .           arena_aalloc(tsdn_t *tsdn, const void *ptr) {
     .           	edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global, ptr);
     .           	unsigned arena_ind = edata_arena_ind_get(edata);
     .           	return (arena_t *)atomic_load_p(&arenas[arena_ind], ATOMIC_RELAXED);
     .           }
-- line 170 ----------------------------------------
-- line 275 ----------------------------------------
     .           	} else {
     .           		arena_dalloc_large_no_tcache(tsdn, ptr, alloc_ctx.szind);
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_dalloc_large(tsdn_t *tsdn, void *ptr, tcache_t *tcache, szind_t szind,
     .               bool slow_path) {
 3,354 ( 0.00%)  	if (szind < nhbins) {
     .           		if (config_prof && unlikely(szind < SC_NBINS)) {
     .           			arena_dalloc_promoted(tsdn, ptr, tcache, slow_path);
     .           		} else {
     .           			tcache_dalloc_large(tsdn_tsd(tsdn), tcache, ptr, szind,
     .           			    slow_path);
     .           		}
     .           	} else {
     .           		edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global,
     .           		    ptr);
     .           		if (large_dalloc_safety_checks(edata, ptr, szind)) {
     .           			/* See the comment in isfree. */
     .           			return;
     .           		}
 2,167 ( 0.00%)  		large_dalloc(tsdn, edata);
837,426 ( 0.23%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/large.c:_rjem_je_large_dalloc (561x)
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_dalloc(tsdn_t *tsdn, void *ptr, tcache_t *tcache,
     .               emap_alloc_ctx_t *caller_alloc_ctx, bool slow_path) {
     .           	assert(!tsdn_null(tsdn) || tcache == NULL);
     .           	assert(ptr != NULL);
-- line 305 ----------------------------------------
-- line 377 ----------------------------------------
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_sdalloc(tsdn_t *tsdn, void *ptr, size_t size, tcache_t *tcache,
     .               emap_alloc_ctx_t *caller_alloc_ctx, bool slow_path) {
     .           	assert(!tsdn_null(tsdn) || tcache == NULL);
     .           	assert(ptr != NULL);
     .           	assert(size <= SC_LARGE_MAXCLASS);
     .           
 7,230 ( 0.00%)  	if (unlikely(tcache == NULL)) {
     .           		arena_sdalloc_no_tcache(tsdn, ptr, size);
     .           		return;
     .           	}
     .           
     .           	emap_alloc_ctx_t alloc_ctx;
     .           	if (config_prof && opt_prof) {
     .           		if (caller_alloc_ctx == NULL) {
     .           			/* Uncommon case and should be a static check. */
-- line 393 ----------------------------------------
-- line 408 ----------------------------------------
     .           
     .           	if (config_debug) {
     .           		edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global,
     .           		    ptr);
     .           		assert(alloc_ctx.szind == edata_szind_get(edata));
     .           		assert(alloc_ctx.slab == edata_slab_get(edata));
     .           	}
     .           
10,600 ( 0.00%)  	if (likely(alloc_ctx.slab)) {
     .           		/* Small allocation. */
     .           		tcache_dalloc_small(tsdn_tsd(tsdn), tcache, ptr,
     .           		    alloc_ctx.szind, slow_path);
     .           	} else {
     .           		arena_dalloc_large(tsdn, ptr, tcache, alloc_ctx.szind,
     .           		    slow_path);
     .           	}
     .           }
     .           
     .           static inline void
     .           arena_cache_oblivious_randomize(tsdn_t *tsdn, arena_t *arena, edata_t *edata,
     .               size_t alignment) {
     .           	assert(edata_base_get(edata) == edata_addr_get(edata));
     .           
 4,095 ( 0.00%)  	if (alignment < PAGE) {
     .           		unsigned lg_range = LG_PAGE -
 1,636 ( 0.00%)  		    lg_floor(CACHELINE_CEILING(alignment));
     .           		size_t r;
 1,636 ( 0.00%)  		if (!tsdn_null(tsdn)) {
     .           			tsd_t *tsd = tsdn_tsd(tsdn);
     .           			r = (size_t)prng_lg_range_u64(
     .           			    tsd_prng_statep_get(tsd), lg_range);
     .           		} else {
     .           			uint64_t stack_value = (uint64_t)(uintptr_t)&r;
     .           			r = (size_t)prng_lg_range_u64(&stack_value, lg_range);
     .           		}
 1,636 ( 0.00%)  		uintptr_t random_offset = ((uintptr_t)r) << (LG_PAGE -
     .           		    lg_range);
   818 ( 0.00%)  		edata->e_addr = (void *)((uintptr_t)edata->e_addr +
     .           		    random_offset);
     .           		assert(ALIGNMENT_ADDR2BASE(edata->e_addr, alignment) ==
     .           		    edata->e_addr);
     .           	}
     .           }
     .           
     .           /*
     .            * The dalloc bin info contains just the information that the common paths need
-- line 453 ----------------------------------------
-- line 469 ----------------------------------------
     .           
     .           	/* Freeing a pointer outside the slab can cause assertion failure. */
     .           	assert((uintptr_t)ptr >= (uintptr_t)edata_addr_get(slab));
     .           	assert((uintptr_t)ptr < (uintptr_t)edata_past_get(slab));
     .           	/* Freeing an interior pointer can cause assertion failure. */
     .           	assert(((uintptr_t)ptr - (uintptr_t)edata_addr_get(slab)) %
     .           	    (uintptr_t)bin_infos[binind].reg_size == 0);
     .           
 2,865 ( 0.00%)  	diff = (size_t)((uintptr_t)ptr - (uintptr_t)edata_addr_get(slab));
     .           
     .           	/* Avoid doing division with a variable divisor. */
     .           	regind = div_compute(&info->div_info, diff);
     .           
     .           	assert(regind < bin_infos[binind].nregs);
     .           
     .           	return regind;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_dalloc_bin_locked_begin(arena_dalloc_bin_locked_info_t *info,
     .               szind_t binind) {
 2,488 ( 0.00%)  	info->div_info = arena_binind_div_info[binind];
     .           	info->nregs = bin_infos[binind].nregs;
     .           	info->ndalloc = 0;
     .           }
     .           
     .           /*
     .            * Does the deallocation work associated with freeing a single pointer (a
     .            * "step") in between a arena_dalloc_bin_locked begin and end call.
     .            *
-- line 498 ----------------------------------------
-- line 511 ----------------------------------------
     .           	assert(edata_nfree_get(slab) < bin_info->nregs);
     .           	/* Freeing an unallocated pointer can cause assertion failure. */
     .           	assert(bitmap_get(slab_data->bitmap, &bin_info->bitmap_info, regind));
     .           
     .           	bitmap_unset(slab_data->bitmap, &bin_info->bitmap_info, regind);
     .           	edata_nfree_inc(slab);
     .           
     .           	if (config_stats) {
 2,865 ( 0.00%)  		info->ndalloc++;
     .           	}
     .           
     .           	unsigned nfree = edata_nfree_get(slab);
16,436 ( 0.00%)  	if (nfree == bin_info->nregs) {
 2,764 ( 0.00%)  		arena_dalloc_bin_locked_handle_newly_empty(tsdn, arena, slab,
 7,325 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_dalloc_bin_locked_handle_newly_empty (302x)
     .           		    bin);
     .           		return true;
 5,380 ( 0.00%)  	} else if (nfree == 1 && slab != bin->slabcur) {
 1,415 ( 0.00%)  		arena_dalloc_bin_locked_handle_newly_nonempty(tsdn, arena, slab,
 6,069 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_dalloc_bin_locked_handle_newly_nonempty (99x)
     .           		    bin);
     .           	}
     .           	return false;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           arena_dalloc_bin_locked_finish(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
     .               arena_dalloc_bin_locked_info_t *info) {
     .           	if (config_stats) {
   622 ( 0.00%)  		bin->stats.ndalloc += info->ndalloc;
     .           		assert(bin->stats.curregs >= (size_t)info->ndalloc);
   622 ( 0.00%)  		bin->stats.curregs -= (size_t)info->ndalloc;
     .           	}
     .           }
     .           
     .           static inline bin_t *
     .           arena_get_bin(arena_t *arena, szind_t binind, unsigned binshard) {
 4,549 ( 0.00%)  	bin_t *shard0 = (bin_t *)((uintptr_t)arena + arena_bin_offsets[binind]);
 6,545 ( 0.00%)  	return shard0 + binshard;
     .           }
     .           
     .           #endif /* JEMALLOC_INTERNAL_ARENA_INLINES_B_H */

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/thread_event.h
--------------------------------------------------------------------------------
Ir               

-- line 89 ----------------------------------------
      .            * allocation call.
      .            */
      .           #define C(counter)							\
      .           JEMALLOC_ALWAYS_INLINE void						\
      .           counter##_set(tsd_t *tsd, uint64_t v) {					\
      .           	*tsd_##counter##p_get(tsd) = v;					\
      .           }
      .           
254,771 ( 0.07%)  ITERATE_OVER_ALL_COUNTERS
      .           #undef C
      .           
      .           /*
      .            * For generating _event_wait getter / setter functions for each individual
      .            * event.
      .            */
      .           #undef E
      .           
-- line 105 ----------------------------------------
-- line 107 ----------------------------------------
      .            * The malloc and free fastpath getters -- use the unsafe getters since tsd may
      .            * be non-nominal, in which case the fast_threshold will be set to 0.  This
      .            * allows checking for events and tsd non-nominal in a single branch.
      .            *
      .            * Note that these can only be used on the fastpath.
      .            */
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_malloc_fastpath_ctx(tsd_t *tsd, uint64_t *allocated, uint64_t *threshold) {
247,781 ( 0.07%)  	*allocated = *tsd_thread_allocatedp_get_unsafe(tsd);
      .           	*threshold = *tsd_thread_allocated_next_event_fastp_get_unsafe(tsd);
      .           	assert(*threshold <= TE_NEXT_EVENT_FAST_MAX);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_free_fastpath_ctx(tsd_t *tsd, uint64_t *deallocated, uint64_t *threshold) {
      .           	/* Unsafe getters since this may happen before tsd_init. */
247,506 ( 0.07%)  	*deallocated = *tsd_thread_deallocatedp_get_unsafe(tsd);
      .           	*threshold = *tsd_thread_deallocated_next_event_fastp_get_unsafe(tsd);
      .           	assert(*threshold <= TE_NEXT_EVENT_FAST_MAX);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE bool
      .           te_ctx_is_alloc(te_ctx_t *ctx) {
      .           	return ctx->is_alloc;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE uint64_t
      .           te_ctx_current_bytes_get(te_ctx_t *ctx) {
 15,485 ( 0.00%)  	return *ctx->current;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_ctx_current_bytes_set(te_ctx_t *ctx, uint64_t v) {
      .           	*ctx->current = v;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE uint64_t
      .           te_ctx_last_event_get(te_ctx_t *ctx) {
  6,546 ( 0.00%)  	return *ctx->last_event;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_ctx_last_event_set(te_ctx_t *ctx, uint64_t v) {
  2,186 ( 0.00%)  	*ctx->last_event = v;
      .           }
      .           
      .           /* Below 3 for next_event_fast. */
      .           JEMALLOC_ALWAYS_INLINE uint64_t
      .           te_ctx_next_event_fast_get(te_ctx_t *ctx) {
      .           	uint64_t v = *ctx->next_event_fast;
      .           	assert(v <= TE_NEXT_EVENT_FAST_MAX);
      .           	return v;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_ctx_next_event_fast_set(te_ctx_t *ctx, uint64_t v) {
      .           	assert(v <= TE_NEXT_EVENT_FAST_MAX);
  4,370 ( 0.00%)  	*ctx->next_event_fast = v;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_next_event_fast_set_non_nominal(tsd_t *tsd) {
      .           	/*
      .           	 * Set the fast thresholds to zero when tsd is non-nominal.  Use the
      .           	 * unsafe getter as this may get called during tsd init and clean up.
      .           	 */
      4 ( 0.00%)  	*tsd_thread_allocated_next_event_fastp_get_unsafe(tsd) = 0;
      4 ( 0.00%)  	*tsd_thread_deallocated_next_event_fastp_get_unsafe(tsd) = 0;
      .           }
      .           
      .           /* For next_event.  Setter also updates the fast threshold. */
      .           JEMALLOC_ALWAYS_INLINE uint64_t
      .           te_ctx_next_event_get(te_ctx_t *ctx) {
  4,370 ( 0.00%)  	return *ctx->next_event;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_ctx_next_event_set(tsd_t *tsd, te_ctx_t *ctx, uint64_t v) {
  4,366 ( 0.00%)  	*ctx->next_event = v;
      .           	te_recompute_fast_threshold(tsd);
      .           }
      .           
      .           /*
      .            * The function checks in debug mode whether the thread event counters are in
      .            * a consistent state, which forms the invariants before and after each round
      .            * of thread event handling that we can rely on and need to promise.
      .            * The invariants are only temporarily violated in the middle of
-- line 193 ----------------------------------------
-- line 198 ----------------------------------------
      .           te_assert_invariants(tsd_t *tsd) {
      .           	if (config_debug) {
      .           		te_assert_invariants_debug(tsd);
      .           	}
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_ctx_get(tsd_t *tsd, te_ctx_t *ctx, bool is_alloc) {
 11,121 ( 0.00%)  	ctx->is_alloc = is_alloc;
      .           	if (is_alloc) {
 33,996 ( 0.01%)  		ctx->current = tsd_thread_allocatedp_get(tsd);
      .           		ctx->last_event = tsd_thread_allocated_last_eventp_get(tsd);
      .           		ctx->next_event = tsd_thread_allocated_next_eventp_get(tsd);
      .           		ctx->next_event_fast =
      .           		    tsd_thread_allocated_next_event_fastp_get(tsd);
      .           	} else {
 32,730 ( 0.01%)  		ctx->current = tsd_thread_deallocatedp_get(tsd);
      .           		ctx->last_event = tsd_thread_deallocated_last_eventp_get(tsd);
      .           		ctx->next_event = tsd_thread_deallocated_next_eventp_get(tsd);
      .           		ctx->next_event_fast =
      .           		    tsd_thread_deallocated_next_event_fastp_get(tsd);
      .           	}
      .           }
      .           
      .           /*
-- line 222 ----------------------------------------
-- line 273 ----------------------------------------
      .           JEMALLOC_ALWAYS_INLINE void
      .           te_event_advance(tsd_t *tsd, size_t usize, bool is_alloc) {
      .           	te_assert_invariants(tsd);
      .           
      .           	te_ctx_t ctx;
      .           	te_ctx_get(tsd, &ctx, is_alloc);
      .           
      .           	uint64_t bytes_before = te_ctx_current_bytes_get(&ctx);
 22,242 ( 0.01%)  	te_ctx_current_bytes_set(&ctx, bytes_before + usize);
      .           
      .           	/* The subtraction is intentionally susceptible to underflow. */
 44,484 ( 0.01%)  	if (likely(usize < te_ctx_next_event_get(&ctx) - bytes_before)) {
      .           		te_assert_invariants(tsd);
      .           	} else {
 10,821 ( 0.00%)  		te_event_trigger(tsd, &ctx);
586,918 ( 0.16%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/thread_event.c:_rjem_je_te_event_trigger (723x)
      .           	}
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           thread_dalloc_event(tsd_t *tsd, size_t usize) {
      .           	te_event_advance(tsd, usize, false);
      .           }
      .           
-- line 295 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /home/adarsh/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rand_core-0.6.4/src/block.rs
--------------------------------------------------------------------------------
Ir               

-- line 168 ----------------------------------------
      .                   self.index = self.results.as_ref().len();
      .               }
      .           
      .               /// Generate a new set of results immediately, setting the index to the
      .               /// given value.
      .               #[inline]
      .               pub fn generate_and_set(&mut self, index: usize) {
      .                   assert!(index < self.results.as_ref().len());
  3,125 ( 0.00%)          self.core.generate(&mut self.results);
      .                   self.index = index;
      .               }
      .           }
      .           
      .           impl<R: BlockRngCore<Item = u32>> RngCore for BlockRng<R>
      .           where
      .               <R as BlockRngCore>::Results: AsRef<[u32]> + AsMut<[u32]>,
      .           {
      .               #[inline]
      .               fn next_u32(&mut self) -> u32 {
600,000 ( 0.17%)          if self.index >= self.results.as_ref().len() {
      .                       self.generate_and_set(0);
      .                   }
      .           
200,000 ( 0.06%)          let value = self.results.as_ref()[self.index];
400,000 ( 0.11%)          self.index += 1;
      .                   value
      .               }
      .           
      .               #[inline]
      .               fn next_u64(&mut self) -> u64 {
      .                   let read_u64 = |results: &[u32], index| {
      .                       let data = &results[index..=index + 1];
      .                       u64::from(data[1]) << 32 | u64::from(data[0])
-- line 200 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/extent.h
--------------------------------------------------------------------------------
Ir              

-- line 63 ----------------------------------------
     .           JEMALLOC_ALWAYS_INLINE bool
     .           extent_neighbor_head_state_mergeable(bool edata_is_head,
     .               bool neighbor_is_head, bool forward) {
     .           	/*
     .           	 * Head states checking: disallow merging if the higher addr extent is a
     .           	 * head extent.  This helps preserve first-fit, and more importantly
     .           	 * makes sure no merge across arenas.
     .           	 */
 7,774 ( 0.00%)  	if (forward) {
 5,404 ( 0.00%)  		if (neighbor_is_head) {
     .           			return false;
     .           		}
     .           	} else {
 2,370 ( 0.00%)  		if (edata_is_head) {
     .           			return false;
     .           		}
     .           	}
     .           	return true;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE bool
     .           extent_can_acquire_neighbor(edata_t *edata, rtree_contents_t contents,
     .               extent_pai_t pai, extent_state_t expected_state, bool forward,
     .               bool expanding) {
     .           	edata_t *neighbor = contents.edata;
 7,896 ( 0.00%)  	if (neighbor == NULL) {
     .           		return false;
     .           	}
     .           	/* It's not safe to access *neighbor yet; must verify states first. */
     .           	bool neighbor_is_head = contents.metadata.is_head;
     .           	if (!extent_neighbor_head_state_mergeable(edata_is_head_get(edata),
     .           	    neighbor_is_head, forward)) {
     .           		return false;
     .           	}
     .           	extent_state_t neighbor_state = contents.metadata.state;
 7,670 ( 0.00%)  	if (pai == EXTENT_PAI_PAC) {
10,274 ( 0.00%)  		if (neighbor_state != expected_state) {
     .           			return false;
     .           		}
     .           		/* From this point, it's safe to access *neighbor. */
 5,066 ( 0.00%)  		if (!expanding && (edata_committed_get(edata) !=
     .           		    edata_committed_get(neighbor))) {
     .           			/*
     .           			 * Some platforms (e.g. Windows) require an explicit
     .           			 * commit step (and writing to uncommitted memory is not
     .           			 * allowed).
     .           			 */
     .           			return false;
     .           		}
-- line 111 ----------------------------------------
-- line 112 ----------------------------------------
     .           	} else {
     .           		if (neighbor_state == extent_state_active) {
     .           			return false;
     .           		}
     .           		/* From this point, it's safe to access *neighbor. */
     .           	}
     .           
     .           	assert(edata_pai_get(edata) == pai);
 2,462 ( 0.00%)  	if (edata_pai_get(neighbor) != pai) {
     .           		return false;
     .           	}
 3,693 ( 0.00%)  	if (opt_retain) {
     .           		assert(edata_arena_ind_get(edata) ==
     .           		    edata_arena_ind_get(neighbor));
     .           	} else {
     .           		if (edata_arena_ind_get(edata) !=
     .           		    edata_arena_ind_get(neighbor)) {
     .           			return false;
     .           		}
     .           	}
-- line 131 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/ph.h
--------------------------------------------------------------------------------
Ir              

-- line 68 ----------------------------------------
     .           	 * happened since, and we don't track whether or not those removals are
     .           	 * from the aux list.
     .           	 */
     .           	size_t auxcount;
     .           };
     .           
     .           JEMALLOC_ALWAYS_INLINE phn_link_t *
     .           phn_link_get(void *phn, size_t offset) {
14,446 ( 0.00%)  	return (phn_link_t *)(((uintptr_t)phn) + offset);
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           phn_link_init(void *phn, size_t offset) {
 7,002 ( 0.00%)  	phn_link_get(phn, offset)->prev = NULL;
     .           	phn_link_get(phn, offset)->next = NULL;
 3,501 ( 0.00%)  	phn_link_get(phn, offset)->lchild = NULL;
     .           }
     .           
     .           /* Internal utility helpers. */
     .           JEMALLOC_ALWAYS_INLINE void *
     .           phn_lchild_get(void *phn, size_t offset) {
 5,725 ( 0.00%)  	return phn_link_get(phn, offset)->lchild;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           phn_lchild_set(void *phn, void *lchild, size_t offset) {
 4,460 ( 0.00%)  	phn_link_get(phn, offset)->lchild = lchild;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           phn_next_get(void *phn, size_t offset) {
16,705 ( 0.00%)  	return phn_link_get(phn, offset)->next;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           phn_next_set(void *phn, void *next, size_t offset) {
 4,881 ( 0.00%)  	phn_link_get(phn, offset)->next = next;
     8 ( 0.00%)  }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           phn_prev_get(void *phn, size_t offset) {
   303 ( 0.00%)  	return phn_link_get(phn, offset)->prev;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           phn_prev_set(void *phn, void *prev, size_t offset) {
19,848 ( 0.01%)  	phn_link_get(phn, offset)->prev = prev;
    17 ( 0.00%)  }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           phn_merge_ordered(void *phn0, void *phn1, size_t offset,
     .               ph_cmp_t cmp) {
     .           	void *phn0child;
     .           
     .           	assert(phn0 != NULL);
     .           	assert(phn1 != NULL);
     .           	assert(cmp(phn0, phn1) <= 0);
     .           
     .           	phn_prev_set(phn1, phn0, offset);
     .           	phn0child = phn_lchild_get(phn0, offset);
     .           	phn_next_set(phn1, phn0child, offset);
 4,629 ( 0.00%)  	if (phn0child != NULL) {
     .           		phn_prev_set(phn0child, phn1, offset);
     .           	}
     .           	phn_lchild_set(phn0, phn1, offset);
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           phn_merge(void *phn0, void *phn1, size_t offset, ph_cmp_t cmp) {
     .           	void *result;
    12 ( 0.00%)  	if (phn0 == NULL) {
     .           		result = phn1;
 1,396 ( 0.00%)  	} else if (phn1 == NULL) {
     .           		result = phn0;
 4,628 ( 0.00%)  	} else if (cmp(phn0, phn1) < 0) {
     .           		phn_merge_ordered(phn0, phn1, offset, cmp);
 1,897 ( 0.00%)  		result = phn0;
     .           	} else {
     .           		phn_merge_ordered(phn1, phn0, offset, cmp);
     .           		result = phn1;
     .           	}
     .           	return result;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
-- line 152 ----------------------------------------
-- line 159 ----------------------------------------
     .           	/*
     .           	 * Multipass merge, wherein the first two elements of a FIFO
     .           	 * are repeatedly merged, and each result is appended to the
     .           	 * singly linked FIFO, until the FIFO contains only a single
     .           	 * element.  We start with a sibling list but no reference to
     .           	 * its tail, so we do a single pass over the sibling list to
     .           	 * populate the FIFO.
     .           	 */
 3,758 ( 0.00%)  	if (phn1 != NULL) {
     .           		void *phnrest = phn_next_get(phn1, offset);
 1,438 ( 0.00%)  		if (phnrest != NULL) {
     .           			phn_prev_set(phnrest, NULL, offset);
     .           		}
     .           		phn_prev_set(phn0, NULL, offset);
     .           		phn_next_set(phn0, NULL, offset);
     .           		phn_prev_set(phn1, NULL, offset);
   139 ( 0.00%)  		phn_next_set(phn1, NULL, offset);
     .           		phn0 = phn_merge(phn0, phn1, offset, cmp);
 1,035 ( 0.00%)  		head = tail = phn0;
     .           		phn0 = phnrest;
 2,330 ( 0.00%)  		while (phn0 != NULL) {
     .           			phn1 = phn_next_get(phn0, offset);
 1,396 ( 0.00%)  			if (phn1 != NULL) {
     .           				phnrest = phn_next_get(phn1, offset);
   890 ( 0.00%)  				if (phnrest != NULL) {
     .           					phn_prev_set(phnrest, NULL, offset);
     .           				}
     .           				phn_prev_set(phn0, NULL, offset);
     .           				phn_next_set(phn0, NULL, offset);
     .           				phn_prev_set(phn1, NULL, offset);
     .           				phn_next_set(phn1, NULL, offset);
     .           				phn0 = phn_merge(phn0, phn1, offset, cmp);
     .           				phn_next_set(tail, phn0, offset);
   590 ( 0.00%)  				tail = phn0;
     .           				phn0 = phnrest;
     .           			} else {
     .           				phn_next_set(tail, phn0, offset);
     .           				tail = phn0;
     .           				phn0 = NULL;
     .           			}
     .           		}
     .           		phn0 = head;
     .           		phn1 = phn_next_get(phn0, offset);
 1,754 ( 0.00%)  		if (phn1 != NULL) {
     .           			while (true) {
   295 ( 0.00%)  				head = phn_next_get(phn1, offset);
     .           				assert(phn_prev_get(phn0, offset) == NULL);
     .           				phn_next_set(phn0, NULL, offset);
     .           				assert(phn_prev_get(phn1, offset) == NULL);
     .           				phn_next_set(phn1, NULL, offset);
     .           				phn0 = phn_merge(phn0, phn1, offset, cmp);
 1,404 ( 0.00%)  				if (head == NULL) {
     .           					break;
     .           				}
     .           				phn_next_set(tail, phn0, offset);
   295 ( 0.00%)  				tail = phn0;
     .           				phn0 = head;
     .           				phn1 = phn_next_get(phn0, offset);
     .           			}
     .           		}
     .           	}
     .           	return phn0;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           ph_merge_aux(ph_t *ph, size_t offset, ph_cmp_t cmp) {
 7,381 ( 0.00%)  	ph->auxcount = 0;
     .           	void *phn = phn_next_get(ph->root, offset);
 8,908 ( 0.00%)  	if (phn != NULL) {
     .           		phn_prev_set(ph->root, NULL, offset);
     .           		phn_next_set(ph->root, NULL, offset);
     .           		phn_prev_set(phn, NULL, offset);
     .           		phn = phn_merge_siblings(phn, offset, cmp);
     .           		assert(phn_next_get(phn, offset) == NULL);
   237 ( 0.00%)  		ph->root = phn_merge(ph->root, phn, offset, cmp);
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           ph_merge_children(void *phn, size_t offset, ph_cmp_t cmp) {
     .           	void *result;
     .           	void *lchild = phn_lchild_get(phn, offset);
 1,545 ( 0.00%)  	if (lchild == NULL) {
   172 ( 0.00%)  		result = NULL;
     .           	} else {
     .           		result = phn_merge_siblings(lchild, offset, cmp);
     .           	}
     .           	return result;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           ph_new(ph_t *ph) {
     1 ( 0.00%)  	ph->root = NULL;
     1 ( 0.00%)  	ph->auxcount = 0;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE bool
     .           ph_empty(ph_t *ph) {
     .           	return ph->root == NULL;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           ph_first(ph_t *ph, size_t offset, ph_cmp_t cmp) {
 9,237 ( 0.00%)  	if (ph->root == NULL) {
     .           		return NULL;
     .           	}
     .           	ph_merge_aux(ph, offset, cmp);
     .           	return ph->root;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           ph_any(ph_t *ph, size_t offset) {
-- line 270 ----------------------------------------
-- line 278 ----------------------------------------
     .           	return ph->root;
     .           }
     .           
     .           /* Returns true if we should stop trying to merge. */
     .           JEMALLOC_ALWAYS_INLINE bool
     .           ph_try_aux_merge_pair(ph_t *ph, size_t offset, ph_cmp_t cmp) {
     .           	assert(ph->root != NULL);
     .           	void *phn0 = phn_next_get(ph->root, offset);
   456 ( 0.00%)  	if (phn0 == NULL) {
     .           		return true;
     .           	}
     .           	void *phn1 = phn_next_get(phn0, offset);
   448 ( 0.00%)  	if (phn1 == NULL) {
     .           		return true;
     .           	}
     .           	void *next_phn1 = phn_next_get(phn1, offset);
     .           	phn_next_set(phn0, NULL, offset);
     .           	phn_prev_set(phn0, NULL, offset);
     .           	phn_next_set(phn1, NULL, offset);
     .           	phn_prev_set(phn1, NULL, offset);
     .           	phn0 = phn_merge(phn0, phn1, offset, cmp);
     .           	phn_next_set(phn0, next_phn1, offset);
   430 ( 0.00%)  	if (next_phn1 != NULL) {
     .           		phn_prev_set(next_phn1, phn0, offset);
     .           	}
     .           	phn_next_set(ph->root, phn0, offset);
     .           	phn_prev_set(phn0, ph->root, offset);
     .           	return next_phn1 == NULL;
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
-- line 308 ----------------------------------------
-- line 311 ----------------------------------------
     .           
     .           	/*
     .           	 * Treat the root as an aux list during insertion, and lazily merge
     .           	 * during a_prefix##remove_first().  For elements that are inserted,
     .           	 * then removed via a_prefix##remove() before the aux list is ever
     .           	 * processed, this makes insert/remove constant-time, whereas eager
     .           	 * merging would make insert O(log n).
     .           	 */
10,503 ( 0.00%)  	if (ph->root == NULL) {
 3,242 ( 0.00%)  		ph->root = phn;
     .           	} else {
     .           		/*
     .           		 * As a special case, check to see if we can replace the root.
     .           		 * This is practically common in some important cases, and lets
     .           		 * us defer some insertions (hopefully, until the point where
     .           		 * some of the items in the aux list have been removed, savings
     .           		 * us from linking them at all).
     .           		 */
 3,760 ( 0.00%)  		if (cmp(phn, ph->root) < 0) {
     .           			phn_lchild_set(phn, ph->root, offset);
     .           			phn_prev_set(ph->root, phn, offset);
 1,071 ( 0.00%)  			ph->root = phn;
 1,071 ( 0.00%)  			ph->auxcount = 0;
 1,071 ( 0.00%)  			return;
     .           		}
 8,911 ( 0.00%)  		ph->auxcount++;
     .           		phn_next_set(phn, phn_next_get(ph->root, offset), offset);
 1,618 ( 0.00%)  		if (phn_next_get(ph->root, offset) != NULL) {
     .           			phn_prev_set(phn_next_get(ph->root, offset), phn,
     .           			    offset);
     .           		}
     .           		phn_prev_set(phn, ph->root, offset);
     .           		phn_next_set(ph->root, phn, offset);
     .           	}
 4,860 ( 0.00%)  	if (ph->auxcount > 1) {
   437 ( 0.00%)  		unsigned nmerges = ffs_zu(ph->auxcount - 1);
     .           		bool done = false;
 1,726 ( 0.00%)  		for (unsigned i = 0; i < nmerges && !done; i++) {
     .           			done = ph_try_aux_merge_pair(ph, offset, cmp);
     .           		}
     .           	}
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void *
     .           ph_remove_first(ph_t *ph, size_t offset, ph_cmp_t cmp) {
     .           	void *ret;
     .           
26,712 ( 0.01%)  	if (ph->root == NULL) {
     .           		return NULL;
     .           	}
     .           	ph_merge_aux(ph, offset, cmp);
     .           	ret = ph->root;
   173 ( 0.00%)  	ph->root = ph_merge_children(ph->root, offset, cmp);
     .           
     .           	return ret;
     .           
     .           }
     .           
     .           JEMALLOC_ALWAYS_INLINE void
     .           ph_remove(ph_t *ph, void *phn, size_t offset, ph_cmp_t cmp) {
     .           	void *replace;
     .           	void *parent;
     .           
 9,696 ( 0.00%)  	if (ph->root == phn) {
     .           		/*
     .           		 * We can delete from aux list without merging it, but we need
     .           		 * to merge if we are dealing with the root node and it has
     .           		 * children.
     .           		 */
 5,900 ( 0.00%)  		if (phn_lchild_get(phn, offset) == NULL) {
 1,596 ( 0.00%)  			ph->root = phn_next_get(phn, offset);
 3,192 ( 0.00%)  			if (ph->root != NULL) {
     .           				phn_prev_set(ph->root, NULL, offset);
     .           			}
     .           			return;
     .           		}
     .           		ph_merge_aux(ph, offset, cmp);
     .           		if (ph->root == phn) {
 1,354 ( 0.00%)  			ph->root = ph_merge_children(ph->root, offset, cmp);
 1,354 ( 0.00%)  			return;
     .           		}
     .           	}
     .           
     .           	/* Get parent (if phn is leftmost child) before mutating. */
   564 ( 0.00%)  	if ((parent = phn_prev_get(phn, offset)) != NULL) {
   564 ( 0.00%)  		if (phn_lchild_get(parent, offset) != phn) {
     .           			parent = NULL;
     .           		}
     .           	}
     .           	/* Find a possible replacement node, and link to parent. */
     .           	replace = ph_merge_children(phn, offset, cmp);
     .           	/* Set next/prev for sibling linked list. */
     .           	if (replace != NULL) {
    30 ( 0.00%)  		if (parent != NULL) {
     .           			phn_prev_set(replace, parent, offset);
     .           			phn_lchild_set(parent, replace, offset);
     .           		} else {
     .           			phn_prev_set(replace, phn_prev_get(phn, offset),
     .           			    offset);
     2 ( 0.00%)  			if (phn_prev_get(phn, offset) != NULL) {
     .           				phn_next_set(phn_prev_get(phn, offset), replace,
     .           				    offset);
     .           			}
     .           		}
     .           		phn_next_set(replace, phn_next_get(phn, offset), offset);
    94 ( 0.00%)  		if (phn_next_get(phn, offset) != NULL) {
     .           			phn_prev_set(phn_next_get(phn, offset), replace,
     .           			    offset);
     .           		}
     .           	} else {
     .           		if (parent != NULL) {
     .           			void *next = phn_next_get(phn, offset);
     .           			phn_lchild_set(parent, next, offset);
   236 ( 0.00%)  			if (next != NULL) {
     .           				phn_prev_set(next, parent, offset);
     .           			}
     .           		} else {
     .           			assert(phn_prev_get(phn, offset) != NULL);
     .           			phn_next_set(
     .           			    phn_prev_get(phn, offset),
     .           			    phn_next_get(phn, offset), offset);
     .           		}
   234 ( 0.00%)  		if (phn_next_get(phn, offset) != NULL) {
     .           			phn_prev_set(
     .           			    phn_next_get(phn, offset),
     .           			    phn_prev_get(phn, offset), offset);
     .           		}
     .           	}
     .           }
     .           
     .           #define ph_structs(a_prefix, a_type)					\
-- line 441 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h
--------------------------------------------------------------------------------
Ir               

-- line 72 ----------------------------------------
      .           
      .           	assert(usize != 0);
      .           	assert(usize == sz_sa2u(usize, alignment));
      .           	assert(!is_internal || tcache == NULL);
      .           	assert(!is_internal || arena == NULL || arena_is_auto(arena));
      .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
      .           	    WITNESS_RANK_CORE, 0);
      .           
      7 ( 0.00%)  	ret = arena_palloc(tsdn, arena, usize, alignment, zero, tcache);
  6,865 ( 0.00%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_palloc (1x)
      .           	assert(ALIGNMENT_ADDR2BASE(ret, alignment) == ret);
      2 ( 0.00%)  	if (config_stats && is_internal && likely(ret != NULL)) {
      .           		arena_internal_add(iaalloc(tsdn, ret), isalloc(tsdn, ret));
      .           	}
      .           	return ret;
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void *
      .           ipalloct(tsdn_t *tsdn, size_t usize, size_t alignment, bool zero,
      .               tcache_t *tcache, arena_t *arena) {
-- line 90 ----------------------------------------
-- line 186 ----------------------------------------
      .           		/*
      .           		 * Existing object alignment is inadequate; allocate new space
      .           		 * and copy.
      .           		 */
      .           		return iralloct_realign(tsdn, ptr, oldsize, size, alignment,
      .           		    zero, tcache, arena, hook_args);
      .           	}
      .           
 56,550 ( 0.02%)  	return arena_ralloc(tsdn, arena, ptr, oldsize, size, alignment, zero,
25,423,387 ( 7.01%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/arena.c:_rjem_je_arena_ralloc (3,770x)
      .           	    tcache, hook_args);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void *
      .           iralloc(tsd_t *tsd, void *ptr, size_t oldsize, size_t size, size_t alignment,
      .               bool zero, hook_ralloc_args_t *hook_args) {
      .           	return iralloct(tsd_tsdn(tsd), ptr, oldsize, size, alignment, zero,
      .           	    tcache_get(tsd), NULL, hook_args);
-- line 202 ----------------------------------------
-- line 221 ----------------------------------------
      .           	    newsize);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE void
      .           fastpath_success_finish(tsd_t *tsd, uint64_t allocated_after,
      .               cache_bin_t *bin, void *ret) {
      .           	thread_allocated_set(tsd, allocated_after);
      .           	if (config_stats) {
247,123 ( 0.07%)  		bin->tstats.nrequests++;
      .           	}
      .           
      .           	LOG("core.malloc.exit", "result: %p", ret);
      .           }
      .           
      .           JEMALLOC_ALWAYS_INLINE bool
      .           malloc_initialized(void) {
      2 ( 0.00%)  	return (malloc_init_state == malloc_init_initialized);
      .           }
      .           
      .           /*
      .            * malloc() fastpath.  Included here so that we can inline it into operator new;
      .            * function call overhead there is non-negligible as a fraction of total CPU in
      .            * allocation-heavy C++ programs.  We take the fallback alloc to allow malloc
      .            * (which can return NULL) to differ in its behavior from operator new (which
      .            * can't).  It matches the signature of malloc / operator new so that we can
-- line 245 ----------------------------------------
-- line 257 ----------------------------------------
      .           JEMALLOC_ALWAYS_INLINE void *
      .           imalloc_fastpath(size_t size, void *(fallback_alloc)(size_t)) {
      .           	LOG("core.malloc.entry", "size: %zu", size);
      .           	if (tsd_get_allocates() && unlikely(!malloc_initialized())) {
      .           		return fallback_alloc(size);
      .           	}
      .           
      .           	tsd_t *tsd = tsd_get(false);
498,038 ( 0.14%)  	if (unlikely((size > SC_LOOKUP_MAXCLASS) || tsd == NULL)) {
  1,896 ( 0.00%)  		return fallback_alloc(size);
2,382,291 ( 0.66%)  => /home/adarsh/Adarsh_Data/Adarsh_Coding/convex_hull_simulation/profiling/target/release/build/tikv-jemalloc-sys-fd81f27dfdc0db83/out/build/src/jemalloc.c:_rjem_je_malloc_default (1,896x)
      .           	}
      .           	/*
      .           	 * The code below till the branch checking the next_event threshold may
      .           	 * execute before malloc_init(), in which case the threshold is 0 to
      .           	 * trigger slow path and initialization.
      .           	 *
      .           	 * Note that when uninitialized, only the fast-path variants of the sz /
      .           	 * tsd facilities may be called.
-- line 274 ----------------------------------------
-- line 284 ----------------------------------------
      .           	sz_size2index_usize_fastpath(size, &ind, &usize);
      .           	/* Fast path relies on size being a bin. */
      .           	assert(ind < SC_NBINS);
      .           	assert((SC_LOOKUP_MAXCLASS < SC_SMALL_MAXCLASS) &&
      .           	    (size <= SC_SMALL_MAXCLASS));
      .           
      .           	uint64_t allocated, threshold;
      .           	te_malloc_fastpath_ctx(tsd, &allocated, &threshold);
743,343 ( 0.20%)  	uint64_t allocated_after = allocated + usize;
      .           	/*
      .           	 * The ind and usize might be uninitialized (or partially) before
      .           	 * malloc_init().  The assertions check for: 1) full correctness (usize
      .           	 * & ind) when initialized; and 2) guaranteed slow-path (threshold == 0)
      .           	 * when !initialized.
      .           	 */
      .           	if (!malloc_initialized()) {
      .           		assert(threshold == 0);
-- line 300 ----------------------------------------
-- line 301 ----------------------------------------
      .           	} else {
      .           		assert(ind == sz_size2index(size));
      .           		assert(usize > 0 && usize == sz_index2size(ind));
      .           	}
      .           	/*
      .           	 * Check for events and tsd non-nominal (fast_threshold will be set to
      .           	 * 0) in a single branch.
      .           	 */
495,562 ( 0.14%)  	if (unlikely(allocated_after >= threshold)) {
      .           		return fallback_alloc(size);
      .           	}
      .           	assert(tsd_fast(tsd));
      .           
      .           	tcache_t *tcache = tsd_tcachep_get(tsd);
      .           	assert(tcache == tcache_get(tsd));
      .           	cache_bin_t *bin = &tcache->bins[ind];
      .           	bool tcache_success;
-- line 317 ----------------------------------------

--------------------------------------------------------------------------------
The following files chosen for auto-annotation could not be found:
--------------------------------------------------------------------------------
  ./elf/./elf/dl-lookup.c
  ./elf/./elf/dl-tunables.c
  ./nptl/./nptl/pthread_mutex_trylock.c
  ./nptl/./nptl/pthread_mutex_unlock.c
  ./stdio-common/./stdio-common/vfscanf-internal.c
  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
  /rust/deps/hashbrown-0.14.3/src/raw/bitmask.rs
  /rust/deps/hashbrown-0.14.3/src/raw/mod.rs
  /rust/deps/hashbrown-0.14.3/src/set.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/raw_vec.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/into_iter.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/mod.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/alloc/src/vec/set_len_on_drop.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/avx2.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/../../stdarch/crates/core_arch/src/x86/sse2.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/alloc/layout.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/cmp.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/convert/num.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/hash/mod.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/intrinsics.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/adapters/map.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/range.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/iter/traits/iterator.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/f32.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/mod.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/num/nonzero.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/arith.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ops/bit.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/option.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/const_ptr.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mod.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/mut_ptr.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/ptr/non_null.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/result.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/index.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/iter/macros.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/core/src/slice/sort.rs
  /rustc/85e449a3237e82c9ade8936a82bd4fc64cfe1057/library/std/src/collections/hash/set.rs

--------------------------------------------------------------------------------
Ir                  
--------------------------------------------------------------------------------
61,471,623 (16.95%)  events annotated

